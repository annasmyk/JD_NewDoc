[["jdemetra-software.html", "JDemetra+ online documentation Chapter 1 JDemetra+ Software 1.1 Structure of this book 1.2 Audience 1.3 How Jdemetra+ came to be", " JDemetra+ online documentation Anna Smyk 2022-05-15 Chapter 1 JDemetra+ Software 1.1 Structure of this book 1.1.1 Available algorithms what can be done 1.1.2 Tools to access the functions how it can be done 1.1.3 Underlying Statistical Methods why (roughly) is it done this way 1.2 Audience 1.3 How Jdemetra+ came to be history of the project "],["quick-start-with.html", "Chapter 2 Quick start with… 2.1 Seasonal Adjustment 2.2 Seasonal Adjustment of High-Frequency Data 2.3 Use of JD+ algorithms in R 2.4 Use of JD+ graphical interface", " Chapter 2 Quick start with… 2.1 Seasonal Adjustment 2.2 Seasonal Adjustment of High-Frequency Data 2.3 Use of JD+ algorithms in R 2.4 Use of JD+ graphical interface "],["main-functions-overview.html", "Chapter 3 Main functions overview 3.1 Seasonal adjustment algorithms 3.2 Trend-cycle estimation", " Chapter 3 Main functions overview link to key references * 2 handbooks * sets of guidelines (start with goof practice rules) 3.1 Seasonal adjustment algorithms 3.1.1 Data frequencies link to HF (edit) The seasonal adjustment methods available in JDemetra+ aim to decompose a time series into components and remove seasonal fluctuations from the observed time series. The X-11 method considers monthly and quarterly series while SEATS is able to decompose series with 2, 3, 4, 6 and 12 observations per year. 3.1.2 X-13 X-13ARIMA-SEATS is a seasonal adjustment program developed and supported by the U.S. Census Bureau. It is based on the U.S. Census Bureau's earlier X-11 program, the X-11-ARIMA program developed at Statistics Canada, the X-12-ARIMA program developed by the U.S. Census Bureau, and the SEATS program developed at the Banco de España. The program is now used by the U.S. Census Bureau for a seasonal adjustment of time series. Users can download the X-13ARIMA-SEATS application, which is a Windows interface for the X-13ARIMA-SEATS program. Detailed information on X-13ARIMA-SEATS can be found at a dedicated U.S. Census Bureau webpage. In contrast to the earlier product (X-12-ARIMA), X-13ARIMA-SEATS includes not only the enhanced X-11 seasonal adjustment procedure but also the capability to generate ARIMA model-based seasonal adjustment using a version of the SEATS procedure originally developed by Victor Gómez and Agustín Maravall at the Banco de España. The program also includes a variety of new tools to overcome adjustment problems and thereby enlarge the range of economic time series that can be adequately seasonally adjusted. In general, X-13ARIMA-SEATS can perform seasonal adjustment in two ways: either using ARIMA model-based seasonal adjustment as in SEATS or by means of an enhanced X-11 method. 3.1.3 STL 3.1.4 Tramo-Seats links : to pre adj, decomp, tools, methods TRAMO-SEATS is a model-based seasonal adjustment method developed by Victor Gómez (Ministerio de Hacienda), and Agustin Maravall (Banco de España). It consists of two linked programs: TRAMO and SEATS. TRAMO (Time Series Regression with ARIMA Noise, Missing Observations, and Outliers) performs estimation, forecasting, and interpolation of regression models with missing observations and ARIMA errors, in the presence of possibly several types of outlier. SEATS (Signal Extraction in ARIMA Time Series) performs an ARIMA-based decomposition of an observed time series into unobserved components. Information about the TRAMO-SEATS method available in this section derives directly from papers by Victor Gómez and Agustin Maravall; the most important ones are: GÓMEZ, V., and MARAVALL, A. (1996), GÓMEZ, V., and MARAVALL, A. (2001a, b) and MARAVALL, A. (2009). More information about the TRAMO-SEATS method, TRAMO-SEATS software (DOS version and TSW+ – Tramo Seats Windows software and several interfaces) and its documentation as well as papers on methodology and application of the programs, can be found in the dedicated section of the Banco de España website. 3.1.5 Basic Strcutural models 3.2 Trend-cycle estimation "],["seasonal-adjustment-1.html", "Chapter 4 Seasonal Adjustment 4.1 Motivation 4.2 Unobserved Components (UC) 4.3 Seasonality tests 4.4 Calendar correction 4.5 Outliers and intervention variables 4.6 Pre-adjustment 4.7 Decomposition", " Chapter 4 Seasonal Adjustment 4.1 Motivation The primary aim of the seasonal adjustment process is to remove seasonal fluctuations from the time series. To achieve this goal, seasonal adjustment methods decompose the original time series into components that capture specific movements. These components are: trend-cycle, seasonality and irregularity. The trend-cycle component includes long-term and medium-term movements in the data. For seasonal adjustment purposes there is no need to divide this component into two parts. JDemetra+ refers to the trend-cycle as trend and consequently this convention is used here. This section presents the options of the seasonal adjustment processes performed by the methods implemented in JDemetra+ (X-12-ARIMA/X-13ARIMA-SEATS and TRAMO/SEATS) and discusses the output displayed by JDemetra+. As these seasonal adjustment methods use different approach to the decomposition, the output produced for both of them has different structure and content. Therefore, the results for both methods are discussed separately. However, in contrast to the original programs, in JDemetra+ some quality indicators have been implemented for both methods, allowing for an easier compaison of the results. 4.2 Unobserved Components (UC) The main components, each representing the impact of certain types of phenomena on the time series (\\(X_{t}\\)), are: The trend (\\(T_{t}\\)) that captures long-term and medium-term behaviour; The seasonal component (\\(S_{t}\\)) representing intra-year fluctuations, monthly or quarterly, that are repeated more or less regularly year after year; The irregular component (\\(I_{t}\\)) combining all the other more or less erratic fluctuations not covered by the previous components. In general, the trend consists of 2 sub-components: The long-term evolution of the series; The cycle, that represents the smooth, almost periodic movement around the long-term evolution of the series. It reveals a succession of phases of growth and recession. For seasonal adjustment purposes both TRAMO-SEATS and X-13ARIMA-SEATS do not separate the long-term trend from the cycle as these two components are usually too short to perform their reliable estimation. Consequently, hereafter TRAMO-SEATS and X-13ARIMA-SEATS estimate the trend component. However, the original TRAMO-SEATS may separate the long-term trend from the cycle through the Hodrick-Precsott filter using the output of the standard decomposition. It should be remembered that JDemetra+ refers to the trend-cycle as trend (\\(T_{t}\\)), and consequently this convention is used in this document. TRAMO-SEATS considers two decomposition models: The additive model: \\(X_{t} = T_{t} + S_{t} + I_{t}\\); The log additive model: \\(log(X_{t}) = log(T_{t}) + log(S_{t}) + log(I_{t})\\). Apart from these two decomposition types X-13ARIMA-SEATS allows the user to apply also the multiplicative model: \\(X_{t} = T_{t} \\times S_{t} \\times I_{t}\\). A time series \\(x_{t}\\), which is a subject to a decomposition, is assumed to be a realisation of a discrete-time stochastic, covariance-stationary linear process, which is a collection of random variables \\(x_{t}\\), where \\(t\\) denotes time. It can be shown that any stochastic, covariance-stationary process can be presented in the form: \\(x_{t} = \\mu_{t} + {\\widetilde{x}}_{t}\\), \\[1\\] where \\(\\mu_{t}\\) is a linearly deterministic component and \\({\\widetilde{x}}_{t}\\) is a linearly interderministic component, such as: \\[ {\\widetilde{x}}_{t} = {\\sum_{j = 0}^{\\infty}\\psi_{j}a}_{t - j} \\], \\[2\\] where \\(\\sum_{j = 0}^{\\infty}\\psi_{i}^{2} &lt; \\infty\\) (coefficients \\(\\psi_{j}\\) are absolutely summable), \\(\\psi_{0} = 1\\) and \\(a_{t}\\) is the white noise error with zero mean and constant variance \\(V_{a}\\). The error term \\(a_{t}\\) represents the one-period ahead forecast error of \\(x_{t}\\), that is: \\[ a_{t} = {\\widetilde{x}}_{t} - {\\widehat{x}}_{t|t - 1} \\], \\[3\\] where \\[{\\widehat{x}}_{t|t - 1}\\] is the forecast of \\[{\\widetilde{x}}_{t}\\] made at period \\(t - 1\\). As \\(a_{t}\\) represents what is new in \\[{\\widetilde{x}}_{t}\\] in point \\(t\\), i.e., not contained in the past values of \\[{\\widetilde{x}}_{t}\\], it is also called innovation of the process. From \\[3\\] \\[{\\widetilde{x}}_{t}\\] can be viewed as a linear filter applied to the innovations. The equation 7.1 is called a Wold representation. It presents a process as a sum of linearly deterministic component \\(\\mu_{t}\\) and linearly interderministic component \\(\\sum_{j = 0}^{\\infty}\\psi_{j}a_{t - j}\\), the first one is perfectly predictable once the history of the process \\(x_{t - 1}\\) is known and the second one is impossible to predict perfectly. This explains why the stochastic process cannot be perfectly predicted. Under suitable conditions \\[{\\widetilde{x}}_{t}\\] can be presented as a weighted sum of its past values and \\(a_{t}\\), i.e.: \\[ { {\\widetilde{x}}_{t} = \\sum_{j = 0}^{\\infty}\\pi_{j}{\\widetilde{x}}_{t - j} + a}_{t} \\], \\[4\\] In general, for the observed time series, the assumptions concerning the nature of the process \\[1\\] do not hold for various reasons. Firstly, most observed time series display a mean that cannot be assumed to be constant due to the presence of a trend and the seasonal movements. Secondly, the variance of the time series may vary in time. Finally, the observed time series usually contain outliers, calendar effects and regression effects, which are treated as deterministic. Therefore, in practice a prior transformation and an adjustment need to be applied to the time series. The constant variance is usually achieved through taking a logarithmic transformation and the correction for the deterministic effects, while stationarity of the mean is achieved by applying regular and seasonal differencing. These processes, jointly referred to as preadjustment or linearization, can be performed with the TRAMO or RegARIMA models. Besides the linearisation, forecasts and backcasts of stochastic time series are estimated with the ARIMA model, allowing for later application of linear filters at both ends of time series. The estimation performed with these models delivers the stochastic part of the time series, called the linearised series, which is assumed to be an output of a linear stochastic process.1 The deterministic effects are removed from the time series and used to form the final components. In the next step the linearised series is decomposed into its components. There is a fundamental difference in how this process is performed in TRAMO-SEATS and X-13ARIMA-SEATS. In TRAMO-SEATS the decomposition is performed by the SEATS procedure, which follows a so called ARIMA model based approach. In principle, it aims to derive the components with statistical models. More information is given in the SEATS section. X-13ARIMA-SEATS offers two algorithms for decomposition: SEATS and X-11. The X-11 algorithm, which is described in the X-11 section section, decomposes a series by means of linear filters. Finally, in both methods the final components are derived by the assignment of the deterministic effects to the stochastic components. Consequently, the role of the ARIMA models is different in each method. TRAMO-SEATS applies the ARIMA models both in the preadjustment step and in the decomposition procedure. On the contrary, when the X-11 algorithm is used for decomposition, X-13ARIMA-SEATS uses the ARIMA model only in the preadjustment step. In summary, the decomposition procedure that results in an estimation of the seasonal component requires prior identification of the deterministic effects and their removal from the time series. This is achieved through the linearisation process performed by the TRAMO and the RegARIMA models, shortly discussed in the Linearisation with the TRAMO and RegARIMA models section.The linearised series is then decomposed into the stochastic components with SEATS or X-11 algorithms. 4.3 Seasonality tests 4.3.1 Overview 4.3.2 F-test on seasonal dummies The F-test on seasonal dummies checks for the presence of deterministic seasonality. The model used here uses seasonal dummies (mean effect and 11 seasonal dummies for monthly data, mean effect and 3 for quarterly data) to describe the (possibly transformed) time series behaviour. The test statistic checks if the seasonal dummies are jointly statistically not significant. When this hypothesis is rejected, it is assumed that the deterministic seasonality is present and the test results are displayed in green. This test refers to Model-Based $^{2}$and F-tests for Fixed Seasonal Effects proposed by LYTRAS, D.P., FELDPAUSCH, R.M., and BELL, W.R. (2007) that is based on the estimates of the regression dummy variables and the corresponding t-statistics of the RegARIMA model, in which the ARIMA part of the model has a form (0,1,1)(0,0,0). The consequences of a misspecification of a model are discussed in LYTRAS, D.P., FELDPAUSCH, R.M., and BELL, W.R. (2007). For a monthly time series the RegARIMA model structure is as follows: \\[\\left( 1 - B \\right)\\left( y_{t} - \\beta_{1}M_{1,t} - \\ldots - \\beta_{11}M_{11,t} - \\gamma X_{t} \\right) = \\mu + (1 - B)a_{t} \\] where: \\[ M_{j,t} = \\begin{cases} 1 &amp; \\text{ in month } j = 1, \\ldots, 11 \\\\ - 1 &amp; \\text{ in December}\\\\ 0 &amp; \\text{ otherwise} \\end{cases} \\text{ - dummy variables;} \\] \\(y_{t}\\) – the original time series; \\(B\\) – a backshift operator; \\(X_{t}\\) – other regression variables used in the model (e.g. outliers, calendar effects, user-defined regression variables, intervention variables); \\(\\mu\\) – a mean effect; \\(a_{t}\\) – a white-noise variable with mean zero and a constant variance. In the case of a quarterly series the estimated model has a form: \\[\\left( 1 - B \\right)\\left( y_{t} - \\beta_{1}M_{1,t} - \\ldots - \\beta_{3}M_{3,t} - \\gamma X_{t} \\right) = \\mu + (1 - B)a_{t}\\], \\[2\\] where: \\[ M_{j,t} = \\begin{cases} 1 &amp; \\text{ in quarter} j = 1, \\ldots, 3 \\\\ - 1 &amp; \\text{ in the fourth quarter}\\\\ 0 &amp; \\text{ otherwise} \\end{cases} \\text{ - dummy variables;} \\] One can use the individual t-statistics to assess whether seasonality for a given month is significant, or a chi-squared test statistic if the null hypothesis is that the parameters are collectively all zero. The chi-squared test statistic is \\({\\widehat{\\chi}}^{2} = {\\widehat{\\beta}}^{&#39;}{\\lbrack Var(\\widehat{\\beta})}^{\\ })^{- 1}\\rbrack{\\widehat{\\beta}}^{\\ }\\) in this case compared to critical values from a \\(\\chi^{2}\\left( \\text{df} \\right)\\)-distribution, with degrees of freedom $df = 11$(monthly series) or \\(df = 3\\) (quarterly series). Since the \\({Var(\\widehat{\\beta})}^{\\ }\\) computed using the estimated variance of \\(\\alpha_{t}\\) may be very different from the actual variance in small samples, this test is corrected using the proposed \\(\\text{F}\\) statistic: \\[ F = \\frac{ {\\widehat{\\chi}}^{2}}{s - 1} \\times \\frac{n - d - k}{n - d} \\], \\[3\\] where \\(n\\) is the sample size, \\(d\\) is the degree of differencing, s is time series frequency (12 for a monthly series, 4 for a quarterly series) and \\(k\\) is the total number of regressors in the RegARIMA model (including the seasonal dummies \\(\\text{M}_{j,t}\\) and the intercept). This statistic follows a \\[F_{s - 1,n - d - k}\\] distribution under the null hypothesis. 4.3.3 QS Test on autocorrelation at seasonal lags The QS test is a variant of the Ljung-Box test computed on seasonal lags, where we only consider positive auto-correlations More exactly, \\[ QS=n \\left(n+2\\right)\\sum_{i=1}^k\\frac{\\left[ \\max \\left(0, \\hat\\gamma_{i \\cdot l}\\right)\\right]^2}{n-i \\cdot l}\\] where \\[k=2\\], so only the first and second seasonal lags are considered. Thus, the test would checks the correlation between the actual observation and the observations lagged by one and two years. Note that \\[l=12\\] when dealing with monthly observations, so we consider the autocovariances \\[\\hat\\gamma_{12}\\] and \\[\\hat\\gamma_{24}\\] alone. In turn, \\[k=4\\] in the case of quarterly data. Under H0, which states that the data are independently distributed, the statistics follows a \\[\\chi \\left(k\\right)\\] distribution. However, the elimination of negative correlations makes it a bad approximation. The p-values would be given by \\(P(\\chi^{2}\\left( k \\right) &gt; Q)\\) for \\(k = 2\\). As \\({P(\\chi}^{2}(2)) &gt; 0.05 = 5.99146\\) and \\({P(\\chi}^{2}(2)) &gt; 0.01 = 9.21034\\), \\(QS &gt; 5.99146\\) and \\(QS &gt; 9.21034\\) would suggest rejecting the null hypothesis at \\(95\\%\\) and \\(99\\%\\) significance levels, respecively. 4.3.3.0.1 Modification Maravall (2012) proposes approximate the correct distribution (p-values) of the QS statistic using simulation techniques. Using 1000K replications of sample size 240, the correct critical values would be 3.83 and 7.09 with confidence levels of \\(95\\%\\) and \\(99\\%\\), respectively (lower than the 5.99146 and 9.21034 shown above). For each of the simulated series, he obtains the distribution by assuming \\(QS=0\\) when \\[\\hat\\gamma_{12}\\], so in practice this test will detect seasonality only when any of these conditions hold: - Statistically significant positive autocorrelation at lag 12 - Nonnegative sample autocorrelation at lag 12 and statistically significant positive autocorrelation at lag 24 4.3.3.1 Implementation 4.3.3.1.1 In the graphical user interface (GUI) The test can be applied directly to any series by selecting the option Statistical Methods &gt;&gt; Seasonal Adjustment &gt;&gt; Tools &gt;&gt; Seasonality Tests. This is an example of how results are displayed for the case of a monthly series: qs The test can be applied to the input series before any seasonal adjustment method has been applied. It can also be applied to the seasonally adjusted series or to the irreguar component. 4.3.3.1.2 Via R package: RJD3toolkit (blank) 4.3.3.1.3 Java Library This test is implemented in the class ec.satoolkit.diagnostics.QsTest 4.3.3.1.4 References LJUNG G. M. and G. E. P. BOX (1978). “On a Measure of a Lack of Fit in Time Series Models”. Biometrika 65 (2): 297–303. doi:10.1093/biomet/65.2.297 MARAVALL, A. (2011). “Seasonality Tests and Automatic Model Identification in Tramo-Seats”. Manuscript MARAVALL, A. (2012). “Update of Seasonality Tests and Automatic Model Identification in TRAMO-SEATS”. Bank of Spain (November 2012) 4.3.4 QS Test for seasonality (BIS : solve this) More exactly, \\[ qs=n \\left(n+2\\right)\\sum_{i=1}^k\\frac{\\left[ \\max \\left(0, \\hat\\gamma_{i \\cdot l}\\right)\\right]^2}{n-i \\cdot l}\\] The current implementation still considers that the statistics is distributed as a \\[\\chi \\left(k\\right)\\] even if it is obvioulsly incorrect. 4.3.5 Kurskall-Wallis The Kruskal-Wallis test is a non-parametric test used for testing whether samples originate from the same distribution. The parametric equivalent of the Kruskal-Wallis test is the one-way analysis of variance (ANOVA). When rejecting the null hypothesis of the Kruskal-Wallis test, then at least one sample stochastically dominates at least one other sample. The test does not identify where this stochastic dominance occurs or for how many pairs of groups stochastic dominance obtains. The null hypothesis states that all months (or quarters, respectively) have the same mean. Under this hypothesis the test statistic follows a \\[ \\chi^2 \\] distribution. When this hypothesis is rejected, it is assumed that time series values differ significantly between periods and the test results are displayed in green The test is typically applied to \\[ k \\] groups of data \\[ \\left\\{x_{i}\\right\\}_{j} \\]. Each group \\[ j=1,…,k \\] is composed of \\[ n_j \\] observations, which are indexed by \\[ i=1,…,n_j \\]. Each month (or quarter) groups all the observations available for a certain number of years. As opposed to the notation used in the Friedman test, number of observations here is not necessarily equal for each group. The ranking of each data point, represented by variable \\[ r_{ij} \\]., is now defined different than in Friedman test, since it considers all observables \\[ N=n_1+ \\dots + n_g \\], thereby ignoring group membership. The test statistic is given by \\[ Q=\\frac{SS_t}{SS_e} \\] where \\[ SS_t=(N-1)\\sum_{j=1}^{g}n_i(\\bar{r}_{.j}-\\bar{r})^2 \\] and \\[ SS_e=\\sum_{j=1}^{g}\\sum_{i=1}^{n_j}(r_{ij}-\\bar{r})^2 \\] - \\[ n_j \\] is the number of observations in group \\[ j \\] - \\[ \\bar{r}_{.j} \\] is the average of the absolute ranks of the data in group \\[ j \\] - The average rank is \\[ \\bar{r} =\\frac{1}{2}(N+1) \\] Under the null hypothesis that all groups are generated from the same distribution, the test statistic Q is approximated by a chi-squared distribution. Thus, the p-value is given by \\[ P( \\chi^2_{g-1}&gt;Q) \\]. This approximation can be misleading if some of the groups are very small (i.e. less than five elements). If the statistic is not significant, then there is no evidence of stochastic dominance between the samples. However, if the test is significant then at least one sample stochastically dominates another sample. 4.3.5.1 Use The test can be applied directly to any series by selecting the option Statistical Methods &gt;&gt; Seasonal Adjustment &gt;&gt; Tools &gt;&gt; Seasonality Tests. This is an example of how results are displayed for the case of a monthly series: kwResults The test can be applied to the input series before any seasonal adjustment method has been applied. It can also be applied to the seasonally adjusted series or to the irreguar component. 4.3.5.2 Implementation This test is implemented in the class ec.satoolkit.diagnostics.KruskallWallisTest 4.3.5.3 References Kruskal; Wallis (1952). “Use of ranks in one-criterion variance analysis”. Journal of the American Statistical Association 47 (260): 583–621. doi:10.1080/01621459.1952.10483441. 4.3.6 Friedman test (stable seasonality test) The Friedman test is a non-parametric method for testing that samples are drawn from the same population or from populations with equal medians. The significance of the month (or quarter) effect is tested. The Friedman test requires no distributional assumptions. It uses the rankings of the observations. If the null hypothesis of no stable seasonality is rejected at the 0.10% significance level then the series is considered to be seasonal and the test’s outcome is displayed in green. The test statistic is constructed as follows. Consider first the matrix of data \\[ \\left\\{x_{ij}\\right\\}_{n \\times k} \\] with \\[ n \\] rows (the blocks, i.e. number of years in the sample), \\[ k \\] columns (the treatments, i.e. either 12 months or 4 quarters, depending on the frequency of the data). The data matrix needs to be replaced by a new matrix \\[ \\left\\{r_{ij}\\right\\}_{n \\times k} \\], where the entry \\[ r_{ij} \\] is the rank of \\[ x_{ij} \\] within block \\[ i \\] . The test statistic is given by \\[ Q=\\frac{SS_t}{SS_e} \\] where \\[ SS_t=n \\sum_{j=1}^{k}(\\bar{r}_{.j}-\\bar{r})^2 \\] and \\[ SS_e=\\frac{1}{n(k-1)} \\sum_{i=1}^{n}\\sum_{j=1}^{k}(r_{ij}-\\bar{r})^2 \\] It represents the variance of the average ranking across treatments j relative to the total. Under the hypothesis of no seasonality, all months can be equally treated. For the sake of completeness: - \\[ \\bar{r}_{.j} \\] is the average ranks of each treatment (month) j within each block (year) - The average rank is given by \\[ \\bar{r}= \\frac{1}{nk}\\sum_{i=1}^{n}\\sum_{j=1}^{k}(r_{ij})\\] For large \\[ n \\] or \\[ k \\] , i.e. n &gt; 15 or k &gt; 4, the probability distribution of \\[ Q \\] can be approximated by that of a chi-squared distribution. Thus, the p-value is given by \\[ P( \\chi^2_{k-1}&gt;Q) \\] . 4.3.6.1 Use The test can be applied directly to any series by selecting the option Statistical Methods &gt;&gt; Seasonal Adjustment &gt;&gt; Tools &gt;&gt; Seasonality Tests. This is an example of how results are displayed for the case of a monthly series: friedman If the null hypothesis of no stable seasonality is rejected at the 1% significance level, then the series is considered to be seasonal and the outcome of the test is displayed in green. The test can be applied to the input series before any seasonal adjustment method has been applied. It can also be applied to the seasonally adjusted series or to the irreguar component. In the case of X-13ARIMA-SEATS, the test is applied to the preliminary estimate of the unmodified Seasonal-Irregular component2 (time series shown in Table B3). In this estimate, the number of observations is lower than in the final estimate of the unmodified Seasonal-Irregular component. Thus, the number of degrees of freedom in the stable seasonality test is lower than the number of degrees of freedom in the test for the presence of seasonality assuming stability. For example, X-13ARIMA-SEATS uses a centred moving average of order 12 to calculate the preliminary estimation of trend. Consequently, the first six and last six points in the series are not computed at this stage of calculation. The preliminary estimation of the trend is then used for the calculation of the preliminary estimation of the unmodified Seasonal-Irregular. 4.3.6.2 Related tests When using this kind of design for a binary response, one instead uses the Cochran’s Q test. Kendall’s W is a normalization of the Friedman statistic between 0 and 1. The Wilcoxon signed-rank test is a nonparametric test of non-independent data from only two groups. 4.3.6.3 Implementation This test is implemented in the class ec.satoolkit.diagnostics.FriedmanTest 4.3.6.4 References Friedman, Milton (December 1937). “The use of ranks to avoid the assumption of normality implicit in the analysis of variance”. Journal of the American Statistical Association (American Statistical Association) 32 (200): 675–701. doi:10.2307/2279372. JSTOR 2279372. Friedman, Milton (March 1939). “A correction: The use of ranks to avoid the assumption of normality implicit in the analysis of variance”. Journal of the American Statistical Association (American Statistical Association) 34 (205): 109. doi:10.2307/2279169. JSTOR Friedman, Milton (March 1940). “A comparison of alternative tests of significance for the problem of m rankings”. The Annals of Mathematical Statistics 11 (1): 86–92. doi:10.1214/aoms/1177731944. JSTOR 2235971. 4.3.7 Stable seasonality test (missing) 4.3.8 Moving seasonality test The evolutive seasonality test is based on a two-way analysis of variance model. The model uses the values from complete years only. Depending on the decomposition type for the Seasonal – Irregular component it uses \\[1\\] (in the case of a multiplicative model) or \\[2\\] (in the case of an additive model): \\[ \\left|\\text{SI}_{\\text{ij}} - 1 \\right| = X_{\\text{ij}} = b_{i} + m_{j} + e_{\\text{ij}} \\], \\[1\\] \\[ \\left| \\text{SI}_{\\text{ij}} \\right| = X_{\\text{ij}} = b_{i} + m_{j} + e_{\\text{ij}} \\], \\[2\\] where: \\(m_{j}\\) – the monthly or quarterly effect for \\(j\\)-th period, \\(j = (1,\\ldots,k)\\), where \\(k = 12\\) for a monthly series and \\(k = 4\\) for a quarterly series; \\(b_{j}\\) – the annual effect \\(i\\), \\((i = 1,\\ldots,N)\\) where \\(N\\) is the number of complete years; \\(e_{\\text{ij}}\\) – the residual effect. The test is based on the following decomposition: \\[S^{2} = S_{A}^{2} + S_{B}^{2} + S_{R}^{2},\\] \\[3\\] where: \\[ S^{2} = \\sum_{j = 1}^{k}{\\sum_{i = 1}^{N}\\left( {\\overline{X}}_{\\text{ij}} - {\\overline{X}}_{\\bullet \\bullet} \\right)^{2}}\\ \\] –the total sum of squares; \\[ S_{A}^{2} = N\\sum_{j = 1}^{k}\\left( {\\overline{X}}_{\\bullet j} - {\\overline{X}}_{\\bullet \\bullet} \\right)^{2} \\] – the inter-month (inter-quarter, respectively) sum of squares, which mainly measures the magnitude of the seasonality; \\[ S_{B}^{2} = k\\sum_{i = 1}^{N}\\left( {\\overline{X}}_{i \\bullet} - {\\overline{X}}_{\\bullet \\bullet} \\right)^{2} \\] – the inter-year sum of squares, which mainly measures the year-to-year movement of seasonality; \\[ S_{R}^{2} = \\sum_{i = 1}^{N}{\\sum_{j = 1}^{k}\\left( {\\overline{X}}_{\\text{ij}} - {\\overline{X}}_{i \\bullet} - {\\overline{X}}_{\\bullet j} - {\\overline{X}}_{\\bullet \\bullet} \\right)^{2}} \\] – the residual sum of squares. The null hypothesis $H_{0}$is that \\(b_{1} = b_{2} = ... = b_{N}\\) which means that there is no change in seasonality over the years. This hypothesis is verified by the following test statistic: \\[ F_{M} = \\frac{\\frac{S_{B}^{2}}{(n - 1)}}{\\frac{S_{R}^{2}}{(n - 1)(k - 1)}} \\], \\[4\\] which follows an \\(F\\)-distribution with \\(k - 1\\) and \\(n - k\\) degrees of freedom. 4.3.9 Identifiable seasonality This test combines the values of the \\(F\\)-statistic of the parametric test for stable seasonality and the values of the moving seasonality test, which was described above. The test statistic is: \\[ T = \\left( \\frac{\\frac{7}{F_{S}} + \\frac{3F_{M}}{F_{S}}}{2} \\right)^{\\frac{1}{2}} \\], \\[1\\] where \\(F_{S}\\) is a stable seasonality test statistic and \\(F_{M}\\) is moving seasonality test statistic. The test checks if the stable seasonality is not dominated by moving seasonality. In such a case the seasonality is regarded as identifiable. This test statistic is used in the combined seasonality tests (see section Combined seasonality test. The detailed description of the test is available in LOTHIAN, J., and MORRY, M. (1978). 4.3.10 Combined seasonality test This test combines the Kruskal-Wallis test along with test for the presence of seasonality assuming stability (\\(F_{S}\\)), and evaluative seasonality test for detecting the presence of identifiable seasonality (\\(F_{M}\\)). Those three tests are calculated using the final unmodified SI component. The main purpose of the combined seasonality test is to check whether the seasonality of the series is identifiable. For example, the identification of the seasonal pattern is problematic if the process is dominated by highly moving seasonality3. The testing procedure is shown in the figure below. Text Combined seasonality test, source: LADIRAY, D., QUENNEVILLE, B. (2001) 4.3.11 Spectral analysis In order to decide whether a series has a seasonal component that is predictable (stable) enough, these tests use visual criteria and formal tests for the periodogram. The periodogram is calculated using complete years, so that the set of Fourier frequencies contains exactly all seasonal frequencies4. The tests rely on two basic principles: The peaks associated with seasonal frequencies should be larger than &gt; the median spectrum for all frequencies and; The peaks should exceed the spectrum of the two adjacent values by &gt; more than a critical value. JDemetra+ performs this test on the original series. If these two requirements are met, the test results are displayed in green. The statistical significance of each of the seasonal peaks (i.e. frequencies $, , ,  $corresponding to 1, 2, 3, 4 and 5 cycles per year) is also displayed. The seasonal and trading days frequencies depends on the frequency of time series. They are shown in the table below. The symbol \\(d\\) denotes a default frequency and is described below the table. The seasonal and trading day frequencies by time series frequency Number of months per full period Seasonal frequency Trading day frequency (radians) 12 \\(\\frac{\\pi}{6},\\frac{\\pi}{3},\\ \\frac{\\pi}{2},\\frac{2\\pi}{3},\\ \\frac{5\\pi}{6},\\ \\pi\\) \\(d\\), 2.714 6 \\(\\frac{\\pi}{3},\\frac{2\\pi}{3}\\), \\(\\pi\\) \\[d\\] 4 \\(\\frac{\\pi}{2}\\), \\(\\pi\\) \\(d\\), 1.292, 1.850, 2.128 3 \\[\\pi\\] \\[d\\] 2 \\[\\pi\\] \\[d\\] The calendar (trading day or working day) effects, related to the variation in the number of different days of the week per period, can induce periodic patterns in the data that can be similar to those resulting from pure seasonal effects. From the theoretical point of view, trading day variability is mainly due to the fact that the average number of days in the months or quarters is not equal to a multiple of \\(7\\) (the average number of days of a month in the year of $365.25$days is equal to \\(\\frac{365.25}{12} = 30.4375\\) days). This effect occurs \\(\\frac{365.25}{12} \\times \\frac{1}{7} = 4.3482\\) times per month: one time for each one of the four complete weeks of each month, and a residual of \\(0.3482\\) cycles per month, i.e. \\(0.3482 \\times 2\\pi = 2.1878\\ radians\\). This turns out to be a fundamental frequency for the effects associated with monthly data. In JDemetra+ the fundamental frequency corresponding to \\(0.3482\\) cycles per month is used in place of the closest frequency\\(\\ \\frac{\\text{πk}}{60}\\). Thus, the quantity \\(\\frac{\\pi \\times 42}{60}\\) is replaced by \\[\\omega_{42} = 0.3482 \\times 2\\pi = 2.1878\\]. The frequencies neighbouring \\(\\omega_{42}\\), i.e. \\[\\omega_{41}\\] and \\[\\omega_{43}\\] are set to, respectively, \\[2.1865 - \\frac{1}{60}\\] and \\[2.1865 + \\frac{1}{60}\\]. The default frequencies ($d)$for calendar effect are: 2.188 (monthly series) and 0.280 (quarterly series). They are computed as: \\(\\omega_{\\text{ce}} = \\frac{2\\pi}{7}\\left( n - 7 \\times \\left\\lbrack \\frac{n}{7} \\right\\rbrack \\right)\\), \\[1\\] where: \\(n = \\frac{365.25}{s}\\), \\(s = 4\\) for quarterly series and \\(s = 12\\) for monthly series. Other frequencies that correspond to trading day frequencies are: 2.714 (monthly series) and 1.292, 1.850, 2.128 (quarterly series). In particular, the calendar frequency in monthly data (marked in red on the figure below) is very close to the seasonal frequency corresponding to 4 cycles per year \\(\\text{ω}_{40} = \\frac{2}{3}\\pi = 2.0944\\). Text Periodogram with seasonal (grey) and calendar (red) frequencies highlighted This implies that it may be hard to disentangle both effects using the frequency domain techniques. 4.3.11.1 Defining a F-test link to the definition of the periodogram in the methods part Brockwell and Davis (1991, section 10.2) exploit the fact that the periodogram can be expressed as the projection on the orthonormal basis defined above to derive a test. Thus, under the null hypothesis: \\[ 2I(\\omega_{k})= \\| P_{\\bar{sp}_{\\left\\{ c_{k},s_{k} \\right\\}}} \\mathbf{X} \\|^{2} \\sim \\sigma^{2} \\chi^{2}(2) \\], for Fourier frequencies \\[ 0 &lt; \\omega_{k}=2\\pi k/n &lt; \\pi \\] \\[ I(\\pi)= \\| P_{\\bar{sp}_{\\left\\{ e_{n/2} \\right\\}}} \\mathbf{X} \\|^{2} \\sim \\sigma^{2} \\chi^{2}(1) \\], for \\[ \\pi \\] Because \\[ I(\\omega_{k}) \\] is independent from the projection error sum of squares, we can define our F-test statistic as follows: \\[ \\frac{ 2I(\\omega_{k})}{\\|\\mathbf{X}-P_{\\bar{sp}_{\\left\\{ e_0,c_{k},s_{k} \\right\\}}} \\mathbf{X}\\|^2} \\frac{n-3}{2} \\sim F(2,n-3) \\], for Fourier frequencies \\[ 0 &lt; \\omega_{k}=2\\pi k/n &lt; \\pi \\] \\[ \\frac{ I(\\pi)}{\\|\\mathbf{X}-P_{\\bar{sp}_{\\left\\{ e_0,e_{n/2} \\right\\}}} \\mathbf{X}\\|^2} \\frac{n-2}{1} \\sim F(1,n-2)\\], for \\[ \\pi \\] where - \\[ \\|\\mathbf{X}-P_{\\bar{sp}_{\\left\\{ e_0,c_{k},s_{k} \\right\\}}} \\mathbf{X}\\|^2 = \\sum_{i=1}^{n}\\mathbf{X^2_i}-I(0)-2I(\\omega_{k}) \\sim \\sigma^{2} \\chi^{2}(n-3)\\] for Fourier frequencies \\[ 0 &lt; \\omega_{k}=2\\pi k/n &lt; \\pi \\] - \\[ \\|\\mathbf{X}-P_{\\bar{sp}_{\\left\\{ e_0,e_{n/2} \\right\\}}} \\mathbf{X}\\|^2 = \\sum_{i=1}^{n}\\mathbf{X^2_i}-I(0)-I(\\pi) \\sim \\sigma^{2} \\chi^{2}(n-2) \\] for \\[ \\pi \\] Thus, we reject the null if our F-test statistic computed at a given seasonal frequency (different from \\[ \\pi \\]) is larger than \\[ F_{1-α}(2,n-3)\\]. If we consider \\[ \\pi \\], our test statistic follows a \\[ F_{1-α}(1,n-2)\\] distribution. 4.3.11.2 Implementation of F-test The implementation of JDemetra+ considers simultaneously the whole set of seasonal frequencies (1, 2, 3, 4, 5 and 6 cycles per year). Thus, the resulting test-statistic is: \\[ \\frac{ 2I(\\pi/6)+ 2I(\\pi/3)+ 2I(2\\pi/3)+ 2I(5\\pi/6)+ \\delta I(\\pi)}{\\left\\|\\mathbf{X}-P_{\\bar{sp}_{\\left\\{ e_0,c_{1},s_{1},c_{2},s_{2},c_{3},s_{3},c_{4},s_{4},c_{5},s_{5}, \\delta e_{n/2} \\right\\}}} \\mathbf{X} \\right\\|^2} \\frac{n-12}{11} \\sim F(11-\\delta,n-12+\\delta) \\] where \\[ \\delta=1 \\] if \\[ n \\] is even and 0 otherwise. In small samples, the test performs better when the periodogram is evaluated as the exact seasonal frequencies. JDemetra+ modifies the sample size to ensure the seasonal frequencies belong to the set of Fourier frequencies. This strategy provides a very simple and effective way to eliminate the leakage problem. Example of how results are displayed: #### Identification of seasonal peaks in a Tukey periodogram and in an autoregressive spectrum In order to decide whether a series has a seasonal component that is predictable (stable) enough, these tests use visual criteria and formal tests for the periodogram. The periodogram is calculated using complete years, so that the set of Fourier frequencies contains exactly all seasonal frequencies5. The tests rely on two basic principles: The peaks associated with seasonal frequencies should be larger than the median spectrum for all frequencies and; The peaks should exceed the spectrum of the two adjacent values by more than a critical value. JDemetra+ performs this test on the original series. If these two requirements are met, the test results are displayed in green. The statistical significance of each of the seasonal peaks (i.e. frequencies \\(\\frac{\\pi}{6},\\ \\frac{\\pi}{3},\\ \\frac{\\pi}{2},\\ \\frac{2\\pi}{3}\\) and $$ corresponding to 1, 2, 3, 4 and 5 cycles per year) is also displayed. The seasonal and trading days frequencies depends on the frequency of time series. They are shown in the table below. The symbol \\(d\\) denotes a default frequency and is described below the table. The seasonal and trading day frequencies by time series frequency Number of months per full period Seasonal frequency Trading day frequency (radians) 12 \\(\\frac{\\pi}{6},\\frac{\\pi}{3},\\ \\frac{\\pi}{2},\\frac{2\\pi}{3},\\ \\frac{5\\pi}{6},\\ \\pi\\) \\(d\\), 2.714 6 \\(\\frac{\\pi}{3},\\frac{2\\pi}{3}\\), \\(\\pi\\) \\[d\\] 4 \\(\\frac{\\pi}{2}\\), \\(\\pi\\) \\(d\\), 1.292, 1.850, 2.128 3 \\[\\pi\\] \\[d\\] 2 \\[\\pi\\] \\[d\\] The calendar (trading day or working day) effects, related to the variation in the number of different days of the week per period, can induce periodic patterns in the data that can be similar to those resulting from pure seasonal effects. From the theoretical point of view, trading day variability is mainly due to the fact that the average number of days in the months or quarters is not equal to a multiple of 7 (the average number of days of a month in the year of 365.25 days is equal to \\(\\frac{365.25}{12} =\\) 30.4375 days). This effect occurs \\(\\frac{365.25}{12} \\times \\frac{1}{7} =\\) 4.3482 times per month: one time for each one of the four complete weeks of each month, and a residual of 0.3482 cycles per month, i.e. \\(0.3482 \\times 2\\pi = 2.1878\\) radians. This turns out to be a fundamental frequency for the effects associated with monthly data. In JDemetra+ the fundamental frequency corresponding to 0.3482 cycles per month is used in place of the closest frequency \\(\\frac{\\text{πk}}{60}\\). Thus, the quantity \\(\\frac{\\pi \\times 42}{60}\\) is replaced by \\(\\omega_{42} = 0.3482 \\times 2\\pi = 2.1878\\). The frequencies neighbouring \\(\\omega_{42}\\), i.e. \\(\\omega_{41}\\) and \\(\\omega_{43}\\) are set to, respectively, \\(2.1865 - \\frac{1}{60}\\) and \\(2.1865 + \\frac{1}{60}\\). The default frequencies ($d)$for calendar effect are: 2.188 (monthly series) and 0.280 (quarterly series). They are computed as: \\[ \\omega_{\\text{ce}} = \\frac{2\\pi}{7}\\left( n - 7 \\times \\left\\lbrack \\frac{n}{7} \\right\\rbrack \\right) \\], \\[1\\] where: \\(n = \\frac{365.25}{s}\\), \\(s = 4\\) for quarterly series and \\(s = 12\\) for monthly series. Other frequencies that correspond to trading day frequencies are: 2.714 (monthly series) and 1.292, 1.850, 2.128 (quarterly series). In particular, the calendar frequency in monthly data (marked in red on the figure below) is very close to the seasonal frequency corresponding to 4 cycles per year \\(\\text{ω}_{40} = \\frac{2}{3}\\pi = 2.0944\\). Text Periodogram with seasonal (grey) and calendar (red) frequencies highlighted This implies that it may be hard to disentangle both effects using the frequency domain techniques. 4.3.11.3 Graphical Test based on AR spectrum for AR spectrum definition link to methods part Text Periodogram with seasonal (grey) and calendar (red) frequencies highlighted The statistical significance of the peaks associated to a given frequency can be informally tested using a visual criterion, which has proved to perform well in simulation experiments. Visually significant peaks for a frequency \\[\\lambda_{j}\\] satisfy both conditions: \\[ \\frac{f_{x}(\\lambda_{j})- \\max \\left\\{f_{x}(\\lambda_{j+1}),f_{x}(\\lambda_{j-1}) \\right\\}}{\\left[ \\max_{k}f_{x}(\\lambda_{k})-\\min_{i}f_{x}(\\lambda_{i}) \\right]}\\ge CV(\\lambda_{j}) \\], where \\[ CV(\\lambda_{j})\\] can be set equal to \\[6/52 \\] for all \\[j\\] \\[ f_{x}(\\lambda_{j})&gt; median_{j} \\left\\{ f_{x}(\\lambda_{j}) \\right\\}\\], which guarantees \\[ f_{x}(\\lambda_{j}) \\] it is not a local peak. The first condition implies that if we divide the range \\[\\max_{k}f_{x}(\\lambda_{k})-\\min_{i}f_{x}(\\lambda_{i})\\] in 52 parts (traditionally represented by stars) the height of each pick should be at least 6 stars. 4.3.11.4 Graphical Test based on Tukey spectrum link to methods/spectral analysis section for Tukeys definition The current JDemetra+ implementation of the seasonality test is based on a \\[F(d_{1},d_{2})\\] approximation that has been originally proposed by Maravall (2012) for TRAMO-SEATS. This test is has been designed for a Blackman-Tukey window based on a particular choices of the truncation lag \\[r\\] and sample size. Following this approach, we determine visually significant peaks for a frequency \\[\\omega_{j}\\] when \\[ \\frac{2 f_{x}(\\omega_{j})}{\\left[ f_{x}(\\omega_{j+1})+ f_{x}(\\omega_{j-1}) \\right]} \\ge CV(\\omega_{j}) \\] where \\[ CV(\\omega_{j})\\] is the critical value of a \\[F(d_{1},d_{2})\\] distribution, where the degrees of freedom are determined using simulations. For \\[\\omega_{j}= \\pi\\], we have a significant peak when \\[\\frac{f_{x}(\\omega_{[n/2]})}{\\left[ f_{x}(\\omega_{[(n-1)/2]})\\right]} \\ge CV(\\omega_{j}) \\] Two significant levels for this test are considered: \\[\\alpha=0.05\\] (code “t”) and \\[\\alpha=0.01\\] (code “T”). As opposed to the AR spectrum, which is computed on the basis of the last \\[120\\] data points, we will use here all available observations. Those critical values have been calculated given the recommended truncation lag \\[r=79\\] for a sample size within the interval \\[n \\in [80,119]\\] and \\[r=112\\] for \\[n \\in [120,300]\\] . The \\[F\\] approximation is less accurate for sample sizes larger than \\[300\\]. For quarterly data, \\[r=44 \\], but there are no recommendations regarding the required sample size JDemetra+ considers critical values for \\[ \\alpha=1\\%\\] (code “T”) and \\[ \\alpha=5\\%\\] (code “t”) at each one of the seasonal frequencies represented in the table below, e.g. frequencies $, , ,  $ corresponding to 1, 2, 3, 4, 5 and 6 cycles per year in this example, since we are dealing with monthly data. The codes “a” and “A” correpond to the so-called AR spectrum, so ignore them for the moment. The seasonal and trading day frequencies by time series frequency Number of months per full period Seasonal frequency Trading day frequency (radians) 12 \\(\\frac{\\pi}{6},\\frac{\\pi}{3},\\ \\frac{\\pi}{2},\\frac{2\\pi}{3},\\ \\frac{5\\pi}{6},\\ \\pi\\) \\(d\\), 2.714 6 \\(\\frac{\\pi}{3},\\frac{2\\pi}{3}\\), \\(\\pi\\) \\[d\\] 4 \\(\\frac{\\pi}{2}\\), \\(\\pi\\) \\(d\\), 1.292, 1.850, 2.128 3 \\[\\pi\\] \\[d\\] 2 \\[\\pi\\] \\[d\\] Currently, only seasonal frequencies are tested, but the program allows you to manually plot the Tukey spectrum and focus your attention on both seasonal and trading day frequencies. 4.3.11.5 References Tukey, J. (1949). The sampling theory of power spectrum estimates., Proceedings Symposium on Applications of Autocorrelation Analysis to Physical Problems, NAVEXOS-P-735, Office of Naval Research, Washington, 47-69 Brockwell, P.J., and R.A. Davis (1991). Times Series: Theory and Methods. Springer Series in Statistics. 4.3.12 Example of non seasonal series The ESS Guidelines on Seasonal Adjustment (2015) recommend to apply seasonal adjustment only to those time series for which the seasonal and/or calendar effects can be properly explained, identified and estimated. Therefore, seasonal adjustment of non-seasonal time series is an inappropriate treatment. This case study explains how to recognize a non-seasonal time series using the tools and functionalities implemented in JDemetra+. The picture below shows the results from the seasonal adjustment of a stock market turnover series from Greece using the RSA4c specification. The test diagnostics do not indicate any problems in the modelling phase (residual seasonality statistics and out-of-sample tests are displayed in green). The seasonality seems to be removed from the time series, but the overall assessment is uncertain, due to the failure of the m-statistics and the visual spectral analysis. Text The diagnostic results for stock market turnover in Greece The inspection of a graph hints at the source of the problem. The original time series does not manifest any seasonal movements (left panel). It should be noted that when the X-13ARIMA-SEATS method is used for seasonal adjustment, the seasonal component is estimated regardless of the properties of the original time series (right panel). It means that the seasonal component is estimated even if there are no signs of the presence of seasonal fluctuations in the time series. In the picture below the seasonal component (blue line) is moving rather than being stable and the averages for the specific months (red lines) are not at the same level, suggesting some intra-year differences between seasons. Nevertheless, the SI ratios (dots) are rather far from the seasonal component, indicating that the irregular movements dominate over the seasonal ones. Text Original and seasonally adjusted time series and the trend-cycle component (left) and SI ratios (right) The seasonality tests performed for the original time series6 are ambiguous. Some suggest that seasonality is not present (the outcomes of three tests: the auto-correlation at seasonal lags, the spectral peaks test and the seasonal dummies test all indicate no seasonality in the original time series). These tests are available in the Diagnostic section of the output tree. The seasonality tests can be executed independently from the seasonal adjustment proces. The descriptions of these tests are given in the Seasonality tests scenario. Text Seasonality test for the original (transformed) series Another sign indicating that the presence of seasonality is uncertain should be addressed : the non-seasonal ARIMA model chosen by the automatic model identification procedure. The details of the RegARIMA model are available in the Pre-processing node. Text Estimation results for the RegARIMA model For X-13ARIMA-SEATS the most relevant tool to assess the presence of seasonal movement in the time series is a combined seasonality test. For the series presented in this case study the result of the combined seasonality test confirms that the movements observed in the time series are not stable and regular enough to be recognized as seasonal ones. Text Combined seasonality test result Regardless of the presence and/or significance of seasonal movements in the original time series the seasonal component is always estimated by X-13ARIMA-SEATS, as shown in the picture below (from the panel on the left choose Main results → Table). Therefore X-13ARIMA-SEATS users should always check the outcome of the combined seasonality test. Text Decomposition’s results In general, in the case of a non-seasonal time series the TRAMO-SEATS method produces more coherent results than X-13ARIMA-SEATS. When no seasonal movements are detected the non-seasonal ARIMA model is used and the seasonal component is not estimated. Text Decomposition result for a non-seasonal time series - TRAMO-SEATS Consequently, the SI ratios (dots) estimated by TRAMO-SEATS are equal to the irregular component and for each month the seasonal component is equal to the mean (red, horizontal line), which is one. Text SI ratios for a non-seasonal time series - TRAMO-SEATS 4.4 Calendar correction here defintions, test in the pre adj part ? 4.4.1 Tests for residual trading days We consider below tests on the seasonally adjusted series (\\(sa_t\\)) or on the irregular component (\\(irr_t\\)). When the reasoning applies on both components, we will use \\(y_t\\). The functions \\(stdev\\) stands for “standard deviation” and \\(rms\\) for “root mean squares” The tests are computed on the log-transformed components in the case of multiplicative decomposition. TD are the usual contrasts of trading days, 6 variables (no specific calendar). 4.4.1.1 Non significant irregular When \\(irr_t\\) is not significant, we don’t compute the test on it, to avoid irrelevant results. We consider that \\(irr_t\\) is significant if \\(stdev( irr_t)&gt;0.01\\) (multiplicative case) or if \\(stdev(irr_t)/rms(sa_t) &gt;0.01\\) (additive case). 4.4.1.2 F test The test is the usual joint F-test on the TD coefficients, computed on the following models: 4.4.1.2.1 Autoregressive model (AR modelling option) We compute by OLS: \\[y_t=\\mu + \\alpha y_{t-1} + \\beta TD_t + \\epsilon_t \\] 4.4.1.2.2 Difference model We compute by OLS: \\[\\Delta y_t - \\overline{\\Delta y_t}=\\beta TD_t + \\epsilon_t \\] So, the latter model is a restriction of the first one (\\(\\alpha =1, \\mu =μ=\\overline{\\Delta y_t}\\)) The tests are the usual joint F-tests on \\(\\beta \\quad (H_0:\\beta=0)\\). By default, we compute the tests on the 8 last years of the components, so that they might highlight moving calendar effects. Remark: In Tramo, a similar test is computed on the residuals of the Arima model. More exactly, the F-test is computed on \\(e_t=\\beta TD_t + \\epsilon_t\\), where \\(e_t\\) are the one-step-ahead forecast errors. 4.5 Outliers and intervention variables here just definition and purpose use in the regression in the reg-arima regression part Outliers[^2] are abnormal values of a time series. In general, they cannot be properly explained by the ARIMA model and its underlying normality assumption. They tend to be associated with irregular special events that produce a distortion in the series. The presence of such values disturbs the modelling of time series with methods like X-13ARIMA-SEATS and TRAMO-SEATS because of the linear procedures (e.g. moving averages and regression analysis) implemented by them. The presence of outliers has an adverse effect on the quality of seasonal adjustment because outliers can lead to model misspecification, biased parameter estimation, poor forecasts and inappropriate decomposition of a series. Therefore, it is vital to identify and include them in the modelling step of seasonal adjustment. The aim is to remove the effect of outliers from a time series before its decomposition into its components. Both X-13ARIMA-SEATS and TRAMO-SEATS include automatic procedure for the treatment of outliers (detection and correction). However, a priori information about an event that may have caused the abnormal observations (the date of its occurrence and type of an effect) can be included in the model by the user. This case study explains how to do it. In the automatic outlier detection and correction procedures, three outlier types are considered by default: additive outlier (AO) – an abnormal value in an isolated point of the series; transitory change (TC) – a series of outliers with a temporarily decreasing effect on the level of the series; level shift (LS) – series of innovation outliers with a constant long-term effect on the level of the series, where for an innovation outlier is meant an anomalous value in the innovation series. Seasonal outliers, which are defined as an abrupt increase or decrease of the seasonal component for a specific month or quarter and are of permanent nature can be automatically detected once the user has chosen the appropriate option. The relevant instructions are given in this case study. The user may also introduce into the model a ramp effect, which is described as a smooth, linear transition between two time points unlike the abrupt change associated with level shifts. This case study explains how to add ramp effects into a specification. The formulas that describe outliers are given here. The picture below presents the number of registered unemployed persons in Poland. It is clear that in the beginning of 1999 a sudden, permanent shift in the trend level took place as a result of the poor state of the economy. At the end of 2008 a single peak can be observed, which can be interpreted as a reaction by entrepreneurs to the beginning of the economic crisis. Text Registered unemployed persons in Poland (feed from estp ? + info on SO from …) 4.6 Pre-adjustment 4.6.1 Overview (edit) #### Modeliing part to adapt The algorithms implemented in JDemetra+ enable a modelling of the original time series with the RegARIMA model, including estimation of the regression effects such as outliers and calendar effects. These procedures can be used just for modelling and forecasting of the original time series but also as a pre-treatment before performing a seasonal adjustement of the series. Hence, this pre-treatement will allow for a more reliable estimation of the time series components performed by the seasonal adjustment procedures. This section is divided into two parts: * Specifications, which presents parameters of the modelling procedure. * Output, which details a typical output produced by the modelling procedure. The specifications and output of the modelling procedure are displayed in the Workspace window. Text The Workspace window with the nodes for the modelling procedure marked 4.6.2 Transformation choices The log transformation of the original data is an option that is often applied to achieve a stationary autocovariance function. The decision concerning logging (or not) of a time series has a great impact on seasonal adjustment outcomes7. JDemetra+ offers two options: logging (which means that the multiplicative decomposition is used) or no transformation (the additive decomposition is used). The selection of the transformation type can be done automatically, on the basis of the outcome of a log-level test. The test used by TRAMO-SEATS is based on the maximum likelihood estimation of the parameter \\(\\lambda\\) in the Box-Cox transformations (which is a power transformations such that the transformed values of time series $$are a monotonic function of the observations, i.e.: \\[y_{i}^{\\alpha} = \\left\\{ \\frac{\\left( y_{i}^{\\alpha} - 1 \\right)}{\\begin{matrix} \\lambda \\\\ \\log{y_{i}^{\\alpha},\\lambda = 0\\ } \\\\ \\end{matrix}} \\right.\\ ,\\ \\lambda \\neq 0\\] The automatic procedure first fits two Airline models (i.e. ARIMA (0,1,1)(0,1,1)) on the time series: one in logs (\\(\\lambda = 0\\)), the other without logs (\\(\\lambda = 1\\)). The test compares the sum of squares of the model without logs with the sum of squares multiplied by the square of the geometric mean from the model in logs. Logs are taken in case the last function is the maximum8. The parameter fct controls the bias in the log/level pre-test (the function is active when Function is set to Auto); fct &gt; 1 favours levels, fct &lt; 1 favours logs. This test is used for modelling with the TRAMO model. Text The Transformation options for the TRAMO-SEATS method The test used by X-13ARIMA-SEATS is based on the AICC information criteria9. To choose the transformation type, X-13ARIMA-SEATS fits the RegARIMA model to the untransformed and the transformed series. X-13ARIMA-SEATS will choose the log transformation except when10: \\[\\text{AICC}_{\\log} - \\text{AICC}_{\\text{no\\ log}} &lt; \\Delta_{\\text{AICC}}\\] where: \\(\\text{AICC}_{\\text{no\\ log}}\\) is the value of AICC from fitting the RegARIMA model to the untransformed series; \\(\\text{AICC}_{\\log}\\) is the value of AICC from fitting the RegARIMA model to the transformed series; \\(\\Delta_{\\text{AICC}}\\) is the threshold value; \\(\\Delta_{\\text{AICC}}\\)&gt; 0 favours levels and \\(\\Delta_{\\text{AICC}}\\) &lt; 0 favours logs. The RegARIMA model used in the test is the one specified in the ARIMA part of the specification. If model is specified then the (0,1,1)(0,1,1) model is used. This test is used for modelling with the RegARIMA model. Text The Transformation options for the X-13ARIMA-SEATS method According to the ESS Guidelines on Seasonal Adjustment (2015), the automatic procedures should be applied for the transformation choice, however in case of the most problematic series the manual selection is recommended. The manual selection of the transformation is usually made in the specifications used for a regular data production. To determine the transformation choice first create and open a new specification. For tramo and tramoseats specifications from the Transformation section choose the function option and input the fct parameter’s value. Click OK to confirm your choice. Text Transformation’s option for the tramoseats specification For the regarima and x13 specifications from the Transformation section choose the function option and Aic difference parameter’s value. Click OK to confirm your choice. Text Transformation’s option for X13 specification It is also possible to change the transformation’s options in the currently used specification (see Defining and modifying a specification scenario, step 4). 4.6.3 Reg-Arima linarization Old page mixing * general info, parameters * gui display * pasted in GUI chapter * here + trim the gui part + add link to R (rjdemetra, go version 3 first but reference help pages) The Model node includes basic information about the outcome of the model identification procedure and checking the goodness of fit. The summary information about the final model is available directly from the main Model node. The content of this panel depends on the settings applied to the modelling procedure. Text The Model node in the navigation tree The first part contains fundamental information about the model. Text The Summary section of the Model node Estimation span informs about the first and the last observation used for modelling. The notation of the estimation span varies according to the frequencies (for example, the span \\[2-1993 : 10-2006\\] represents a monthly time series and the span \\[II-1994 : I-2011\\] represents a quarterly time series). The message Series has been log-transformed is only displayed if a logarithmic transformation has been applied. In the case of the pre-defined specifications: TR0, TR1, TR3, RG0, RG1 and RG3 no trading day effect is estimated. For TR2, RG2c, TR4 and RG4c pre-defined specifications, working day effects and the leap year effect are pre-tested and estimated if present. If the working day effect is significant, the pre-processing part includes the message Working days effect (1 regressor). The message Working days effect (2 regressors) means that the leap year effect has also been estimated. For TR5 and RG5c the trading day effect and the leap year effect are pre-tested. If the trading day effect has been detected, either of the messages Trading days effect (6 regressors) or Trading days effect (7 regressors) are displayed, depending whether the leap year effect has been detected or not. If the Easter effect is statistically significant, Easter effect detected is displayed. In this section the total number of detected outliers is displayed. The additional information on detected outliers, i.e. type, location and coefficients’ values, can be found in the Arima model subsection of the Model node. The Final model section informs about the outcome of the estimation process. Number of effective observations is the number of observations used to estimate the model, i.e. the number of observations of the transformed series (regularly and/or seasonally differenced) reduced by the Number of estimated parameters, which is the sum of regular and seasonal parameters for both autoregressive and moving average processes, mean effect, trading/working day effect, outliers, regressors and one. Likelihood is a maximized value of a Likelihood11 function after the iterations processed in Exact Maximum Likelihood Estimation, which is a method used to estimate the model. This value is used by the model selection criteria: AIC, AICC, BIC (corrected by length) and Hannan-Quinn12. Standard error of the regression (ML estimate) is the standard error of the regression from Maximum Likelihood Estimation13. The scores at the solution section presents the gradient of the loglikelihood. The different items of the scores are related to the different parameters of the ARIMA model. The output indicates to which extent the optimization procedure reached the maximum. At the maximum of the likelihood, it should be 0. However, it is never exactly the case, due to numerical approximations. Usually, the scores can be improved by using a higher precision (smaller tolerance). This precision is controlled by the Tolerance parameter in the Estimate section of the Specifications window (see how to use this parameter for Tramo and for Arima). An example of the output is presented in the chart below. Text The content of the Final model section Next, the estimated values of model parameters (Coefficients), t-statistics (T-Stat) and corresponding p-values (P\\[\\|T\\|\\&gt;t\\]) are displayed. JDemetra+ uses the following notation: Phi(p) – the \\(p^{}\\) term in the non-seasonal autoregressive polynomial; Theta(q) – the \\(q^{\\text{th}}\\) term in the non-seasonal moving average polynomial; BPhi(P) – the \\(P^{\\text{th}}\\) term in the seasonal autoregressive polynomial; BTheta(Q) – the \\(Q^{\\text{th}}\\) term in the seasonal moving average polynomial. In the example below, the ARIMA model (0,1,1)(0,1,1) was chosen, which means that one regular and one seasonal moving average parameter were identified and estimated. The p-values indicate that BTheta(1) parameter is significant in contrast to the Theta(1), which is not significant 14. Text The estimation’s results of the ARIMA model For the fixed ARIMA parameters, JDemetra+ shows only the values of the parameters. Figure below presents the output from the manually chosen ARIMA model (2,0,0)(0,1,1) with a fixed parameter BTheta(1). For the fixed parameter the T-Stat and (P|T|&gt;t) are not displayed as no estimation is done for this parameter. Text The results of the estimation of the ARIMA model with a fixed coefficient If the ARIMA model contains a constant term (detected automatically or introduced by the user), the estimated value and related statistics are reported. Text The results of the estimation of a mean effect JDemetra+ presents estimated values of the coefficients of one or six regressors depending on the type of a calendar effect specification. For a working days effect one regressor is estimated. Text The results of the estimation of a working day effect When a trading days effect is estimated the Joint F-test value is reported under the table that presents estimated values. When the result of Joint F-test indicates that the trading day variables are jointly not significant the test result is displayed in red. Text The results of the estimation of a trading day effect: the case of jointly not significant variables In the example below the RSA5c specification has been used and a trading day effect has been detected. In spite of the fact that some trading day regressors are not significant at the 5% significance level, the outcome of the joint F-test indicates that the trading day regressors are jointly significant (the F-test statistic is lower than 5%). Text The results of the estimation of a trading day effect: the case of jointly significant variables If a leap year regressor has been used in the model specification, the value of the estimated leap year coefficient is also reported with the corresponding t-statistics and p-value. As the p-value presented on the picture below is greater than 0.05, it indicates that the leap year effect is not significant. Text The results of the estimation of a leap year effect When the option UserDefined is used, JDemetra+ displays the User-defined calendar variables section with variables and corresponding estimation results (the values of the parameters, corresponding t-statistics and p-values). The outcome of the joint F-test is displayed when more than one user-defined calendar variable is used. Text The results of the estimation of user-defined calendar variables When the Easter effect is estimated, the following table is displayed in the output. In the case presented below Easter has a negative, significant effect on the time series. Text The results of the estimation of the Easter effect JDemetra+ also presents the results of an outlier detection procedure. The table includes the type of outlier, its date, the value of the coefficient and corresponding t-statistics and p-values. Text The results of the outlier identification procedure In all pre-defined specifications, except for TR0 and RG0, only additive outliers, temporary changes and level shifts are considered in the automatic outlier identification procedure. When seasonal outliers are also enabled, they appear in the same table as other outliers. Text The results of the outlier identification procedure that enables seasonal outliers Results for pre-specified outliers are displayed in a separate table. Text The results of the estimation of the pre-specified outliers Regression variables, like ramps and intervention variables, are not identified automatically. They need to be defined by the user. The results of an estimation of ramps that are pre-defined types of regression variables are displayed in a separate table. All information concerning ramps, including spans, estimated coefficients and related statistics, is shown in a separate table. Text The results of the estimation of the ramp effect All other intervention variables with corresponding statistics are shown under the Intervention variable(s) table. Text The results of the estimation of the intervention variable User-defined variables are marked as Vars-1.x_1, Vars-1.x_2, …, Vars-1.x_n and displayed in the separate tables. Text The results of the estimation of the user-defined variables JDemetra+ also reports a list of missing observations, if any. JDemetra+ applies the AO approach to the estimation of the missing observations15. Text The results of the estimation of the missing observations Detailed results are divided into several sections and are investigated in the following sections: - Forecasts - Regressors - ARIMA - Pre-adjustment series - Residuals - Likelihood 4.7 Decomposition 4.7.1 X-11 moving average based decomposition A complete documentation of the X-11 method is available in LADIRAY, D., and QUENNEVILLE, B. (2001). The X-11 program is the result of a long tradition of non-parametric smoothing based on moving averages, which are weighted averages of a moving span of a time series (see hereafter). Moving averages have two important drawbacks: They are not resistant and might be deeply impacted by outliers; The smoothing of the ends of the series cannot be done except with asymmetric moving averages which introduce phase-shifts and delays in the detection of turning points. These drawbacks adversely affect the X-11 output and stimulate the development of this method. To overcome these flaws first the series are modelled with a RegARIMA model that calculates forecasts and estimates the regression effects. Therefore, the seasonal adjustment process is divided into two parts. In a first step, the RegARIMA model is used to clean the series from &gt; non-linearities, mainly outliers and calendar effects. A global &gt; ARIMA model is adjusted to the series in order to compute the &gt; forecasts. In a second step, an enhanced version of the X-11 algorithm is used &gt; to compute the trend, the seasonal component and the irregular &gt; component. Text The flow diagram for seasonal adjustment with X-13ARIMA-SEATS using the X-11 algorithm. 4.7.1.1 Moving averages The moving average of coefficient \\(\\theta_{i}\\) is defined as: \\[M\\left( X_{t} \\right) = \\sum_{k = - p}^{+ f}\\theta_{k}X_{t + k}\\] \\[1\\] The value at time \\(t\\) of the series is therefore replaced by a weighted average of \\(p\\) “past” values of the series, the current value, and \\(f\\) “future” values of the series. The quantity $p + f + 1$is called the moving average order. When \\(p\\) is equal to \\(f\\), that is, when the number of points in the past is the same as the number of points in the future, the moving average is said to be centred. If, in addition, \\(\\theta_{- k} = \\theta_{k}\\) for any \\(k\\), the moving average \\(M\\) is said to be symmetric. One of the simplest moving averages is the symmetric moving average of order \\(P = 2p + 1\\) where all the weights are equal to\\(\\ \\frac{1}{P}\\). This moving average formula works well for all time series observations, except for the first \\(p\\) values and last \\(f\\) values. Generally, with a moving average of order $p + f + 1$calculated for instant \\(t\\)nwith points \\(p\\) in the past and points \\(f\\) in the future, it will be impossible to smooth out the first \\(p\\) values and the last \\(f\\) values of the series because of lack of input to the moving average formula. In the X-11 method, symmetric moving averages play an important role as they do not introduce any phase-shift in the smoothed series. But, to avoid losing information at the series ends, they are either supplemented by ad hoc asymmetric moving averages or applied on the series extended by forecasts. For the estimation of the seasonal component, X-13ARIMA-SEATS uses \\(P \\times Q\\) composite moving averages, obtained by composing a simple moving average of order \\(P\\), which coefficients are all equal to \\(\\frac{1}{P}\\), and a simple moving average of order \\(Q\\), which coefficients are all equal to \\(\\frac{1}{Q}\\). The composite moving averages are widely used by the X-11 method. For an initial estimation of trend X-11 method uses a \\(2 \\times 4\\) moving average in case of a quarterly time series while for a monthly time series a $2 $moving average is applied. The \\(2 \\times 4\\) moving average is an average of order 5 with coefficients \\[\\frac{1}{8}\\left\\{1, 2, 2, 2, 1\\right\\}\\]. It eliminates frequency \\(\\frac{\\pi}{2}\\) corresponding to period 4 and therefore it is suitable for seasonal adjustment of the quarterly series with a constant seasonality. The \\(2 \\times 12\\) moving average, with coefficients \\[\\frac{1}{24}\\left\\{1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1\\right\\} \\]that retains linear trends, eliminates order-\\(12\\) constant seasonality and minimises the variance of the irregular component. The \\(2 \\times 4\\) and \\(2 \\times 12\\) moving averages are also used in the X-11 method to normalise the seasonal factors. The composite moving averages are also used to extract the seasonal component. These, which are used in the purely automatic run of the X-11 method (without any intervention from the user) are \\(3 \\times 3\\), \\(3 \\times 5\\) and \\(3 \\times 9\\). In the estimation of the trend also Henderson moving averages are used. These filters have been chosen for their smoothing properties. The coefficients of a Henderson moving average of order \\(2p + 1\\) may be calculated using the formula: \\(\\theta_{i} = \\frac{315\\left\\lbrack \\left( n - 1 \\right)^{2} - i^{2} \\right\\rbrack\\left\\lbrack n^{2} - i^{2} \\right\\rbrack\\left\\lbrack \\left( n + 1 \\right)^{2} - i^{2} \\right\\rbrack\\left\\lbrack {3n}^{2} - 16 - 11i^{2} \\right\\rbrack}{8n\\left( n^{2} - 1 \\right)\\left( {4n}^{2} - 1 \\right)\\left( {4n}^{2} - 9 \\right)\\left( 4n^{2} - 25 \\right)}\\), \\[2\\] where: \\(n = p + 2\\)\\(n = p + 2\\). 4.7.1.2 The basic algorithm of the X-11 method The X-11 method is based on an iterative principle of estimation of the different components using appropriate moving averages at each step of the algorithm. The successive results are saved in tables. The list of the X-11 tables displayed in JDemetra+ is included at the end of this section. The basic algorithm of the X-11 method will be presented for a monthly time series \\(X_{t}\\) that is assumed to be decomposable into trend, seasonality and irregular component according to an additive model \\(X_{t} = TC_{t} + S_{t} + I_{t}\\). A simple seasonal adjustment algorithm can be thought of in eight steps. The steps presented below are designed for the monthly time series. In the algorithm that is run for the quarterly time series the \\(2 \\times 4\\) moving average instead of the \\(2 \\times 12\\) moving average is used. Step 1: Estimation of Trend by \\(\\mathbf{2 \\times 12}\\) moving average: \\(TC_{t}^{(1)} = M_{2 \\times 12}(X_{t})\\) \\[3\\] Step 2: Estimation of the Seasonal-Irregular component: \\(\\left( S_{t} + I_{t} \\right)^{(1)} = X_{t} - \\text{TC}_{t}^{(1)}\\) \\[4\\] Step 3: Estimation of the Seasonal component by \\(\\mathbf{3 \\times 3}\\) moving average over each month: \\(S_{t}^{(1)} - M_{3 \\times 3}\\left\\lbrack \\left( S_{t} + I_{t} \\right)^{(1)} \\right\\rbrack\\) \\[5\\] The moving average used here is a \\(3 \\times 3\\) moving average over \\(5\\) terms, with coefficients \\[\\frac{1}{9} \\left\\{1, 2, 3, 2, 1 \\right\\}\\]. The seasonal component is then centred using a \\(2 \\times 12\\) moving average. \\[ \\widetilde{S}_{t}^{(1)} = S_{t}^{(1)} - M_{2 \\times 12}\\left( S_{t}^{(1)} \\right) \\] \\[6\\] Step 4: Estimation of the seasonally adjusted series: \\[ SA_{t}^{\\left( 1 \\right)} = \\left( \\text{TC}_{t} + I_{t} \\right)^{(1)} = X_{t} - {\\widetilde{S}}_{t}^{(1)} \\] \\[7\\] This first estimation of the seasonally adjusted series must, by construction, contain less seasonality. The X-11 method again executes the algorithm presented above, changing the moving averages to take this property into account. Step 5: Estimation of Trend by 13-term Henderson moving average: \\[ TC_{t}^{(2)} = H_{13}\\left( \\text{SA}_{t}^{\\left( 1 \\right)} \\right) \\] \\[8\\] Henderson moving averages, while they do not have special properties in terms of eliminating seasonality (limited or none at this stage), have a very good smoothing power and retain a local polynomial trend of degree \\(2\\) and preserve a local polynomial trend of degree \\(3\\). Step 6: Estimation of the Seasonal-Irregular component: \\[ \\left( S_{t} + I_{t} \\right)^{(2)} = X_{t} - \\text{TC}_{t}^{(2)} \\] \\[9\\] Step 7: Estimation of the Seasonal component by \\(\\mathbf{3 \\times 5}\\) moving average over each month: \\[S_{t}^{(2)} - M_{3 \\times 3}\\left\\lbrack \\left( S_{t} + I_{t} \\right)^{(2)} \\right\\rbrack\\] \\[10\\] The moving average used here is a \\(3 \\times 5\\) moving average over \\(7\\) terms, of coefficients \\[\\frac{1}{15} \\left\\{ 1,\\ 2,\\ 3,\\ 3,\\ 3,\\ 2,\\ 1 \\right\\}\\] and retains linear trends. The coefficients are then normalised such that their sum over the whole \\(12\\)-month period is approximately cancelled out: \\[{ \\widetilde{S}}_{t}^{(2)} = S_{t}^{(2)} - M_{2 \\times 12}\\left( S_{t}^{(2)} \\right)\\] \\[11\\] Step 8: Estimation of the seasonally adjusted series: \\[SA_{t}^{\\left( 2 \\right)} = \\left(TC_{t} + I_{t} \\right)^{(2)} = X_{t} - {\\widetilde{S}}_{t}^{(2)}\\] \\[12\\] The whole difficulty lies, then, in the choice of the moving averages used for the estimation of the trend in steps \\(1\\) and \\(5\\) on the one hand, and for the estimation of the seasonal component in steps \\(3\\) and \\(5\\). The course of the algorithm in the form that is implemented in JDemetra+ is presented in the figure below. The adjustment for trading day effects, which is present in the original X-11 program, is omitted here, as since calendar correction is performed by the RegARIMA model, JDemetra+ does not perform further adjustment for these effects in the decomposition step. A workflow diagram for the X-11 algorithm based upon training material from the Deutsche Bundesbank 4.7.1.2.1 The iterative principle of X-11 To evaluate the different components of a series, while taking into account the possible presence of extreme observations, X-11 will proceed iteratively: estimation of components, search for disruptive effects in the irregular component, estimation of components over a corrected series, search for disruptive effects in the irregular component, and so on. The Census X-11 program presents four processing stages (A, B, C, and D), plus 3 stages, E, F, and G, that propose statistics and charts and are not part of the decomposition per se. In stages B, C and D the basic algorithm is used as is indicated in the figure below. A workflow diagram for the X-11 algorithm implemented in JDemetra+. Source: Based upon training material from the Deutsche Bundesbank Part A: Pre-adjustments This part, which is not obligatory, corresponds in X-13ARIMA-SEATS to the first cleaning of the series done using the RegARIMA facilities: detection and estimation of outliers and calendar effects (trading day and Easter), forecasts and backcasts[^61] of the series. Based on these results, the program calculates prior adjustment factors that are applied to the raw series. The series thus corrected, Table B1 of the printouts, then proceeds to part B. Part B: First automatic correction of the series This stage consists of a first estimation and down-weighting of the extreme observations and, if requested, a first estimation of the calendar effects. This stage is performed by applying the basic algorithm detailed earlier. These operations lead to Table B20, adjustment values for extreme observations, used to correct the unadjusted series and result in the series from Table C1. Part C: Second automatic correction of the series Applying the basic algorithm once again, this part leads to a more precise estimation of replacement values of the extreme observations (Table C20). The series, finally “cleaned up”, is shown in Table D1 of the printouts. Part D: Seasonal adjustment This part, at which our basic algorithm is applied for the last time, is that of the seasonal adjustment, as it leads to final estimates: of the seasonal component (Table D10); of the seasonally adjusted series (Table D11); of the trend component (Table D12); of the irregular component (Table D13). Part E: Components modified for large extreme values Parts E includes: Components modified for large extreme values; Comparison the annual totals of the raw time series and seasonally adjusted time series; Changes in the final seasonally adjusted series; Changes in the final trend; Robust estimation of the final seasonally adjusted series. The results from part E are used in part F to calculate the quality measures. Part F: Seasonal adjustment quality measures Part F contains statistics for judging the quality of the seasonal adjustment. JDemetra+ presents selected output for part F, i.e.: M and Q statistics; Tables. Part G: Graphics Part G presents spectra estimated for: Raw time series adjusted a priori (Table B1); Seasonally adjusted time series modified for large extreme values (Table E2); Final irregular component adjusted for large extreme values (Table E3). Originally, graphics were displayed in character mode. In JDemetra+, these graphics are replaced favourably by the usual graphics software. The Henderson moving average and the trend estimation In iteration B (Table B7), iteration C (Table C7) and iteration D (Table D7 and Table D12) the trend component is extracted from an estimate of the seasonally adjusted series using Henderson moving averages. The length of the Henderson filter is chosen automatically by X-13ARIMA-SEATS in a two-step procedure. It is possible to specify the length of the Henderson moving average to be used. X-13ARIMA-SEATS provides an automatic choice between a 9-term, a 13-term or a 23-term moving average. The automatic choice of the order of the moving average is based on the value of an indicator called $$ratio which compares the magnitude of period-on-period movements in the irregular component with those in the trend. The larger the ratio, the higher the order of the moving average selected. Moreover, X-13ARIMA-SEATS allows the user to choose manually any odd‑numbered Henderson moving average. The procedure used in each part is very similar; the only differences are the number of options available and the treatment of the observations in the both ends of the series. The procedure below is applied for a monthly time series. In order to calculate $$ ratio a first decomposition of the SA series (seasonally adjusted) is computed using a 13-term Henderson moving average. For both the trend (\\(C\\)) and irregular (\\(I\\)) components, the average of the absolute values for monthly growth rates (multiplicative model) or for monthly growth (additive model) are computed. They are denoted as $$and \\(\\overline{I}\\), receptively, where \\(\\overline{C} = \\frac{1}{n - 1}\\sum_{t = 2}^{n}\\left| C_{t} - C_{t - 1} \\right|\\) and \\(\\overline{I} = \\frac{1}{n - 1}\\sum_{t = 2}^{n}\\left| I_{t} - I_{t - 1} \\right|\\). Then the value of $$ ratio is checked and in iteration B: If the ratio is smaller than 1, a 9-term Henderson moving average is selected; Otherwise, a 13-term Henderson moving average is selected. Then the trend is computed by applying the selected Henderson filter to the seasonally adjusted series from Table B6. The observations at the beginning and at the end of the time series that cannot be computed by means of symmetric Henderson filters are estimated by ad hoc asymmetric moving averages. In iterations C and D: If the ratio is smaller than 1, a 9-term Henderson moving average is selected; If the ratio is greater than 3.5, a 23-term Henderson moving average is selected. Otherwise, a 13-term Henderson moving average is selected. The trend is computed by applying selected Henderson filter to the seasonally adjusted series from Table C6, Table D7 or Table D12, accordingly. At the both ends of the series, where a central Henderson filter cannot be applied, the asymmetric ends weights for the 7 term Henderson filter are used. 4.7.1.2.2 Choosing the composite moving averages when estimating the seasonal component In iteration D, Table D10 shows an estimate of the seasonal factors implemented on the basis of the modified SI (Seasonal – Irregular) factors estimated in Tables D4 and D9bis. This component will have to be smoothed to estimate the seasonal component; depending on the importance of the irregular in the SI component, we will have to use moving averages of varying length as in the estimate of the Trend/Cycle where the $$ ratio was used to select the length of the Henderson moving average. The estimation includes several steps. Step 1: Estimating the irregular and seasonal components An estimate of the seasonal component is obtained by smoothing, month by month and therefore column by column, Table D9bis using a simple 7-term moving average, i.e. of coefficients \\[\\frac{1}{7} \\left\\{1,\\ 1,\\ 1,\\ 1,\\ 1,\\ 1,\\ 1\\right\\}\\]. In order not to lose three points at the beginning and end of each column, all columns are completed as follows. Let us assume that the column that corresponds to the month is composed of \\(N\\) values \\[ \\left\\{ x_{1},\\ x_{2},\\ x_{3},\\ \\ldots x_{N - 1},\\ x_{N} \\right\\}. \\] It will be transformed into a series \\[\\left\\{ {x_{- 2},x_{- 1}{,x}_{0},x}_{1},\\ x_{2},\\ x_{3},\\ \\ldots x_{N - 1},\\ x_{N},x_{N + 1},\\ x_{N + 1},\\ x_{N + 2},\\ x_{N + 3} \\right\\}\\\\] with \\[x_{- 2} = x_{- 1} = x_{0} = \\frac{x_{1} + x_{2} + x_{3}}{3}\\] and \\[x_{N + 1} = x_{N + 2} = x_{N + 3} = \\frac{x_{N} + x_{N - 1} + x_{N - 2}}{3}\\]. We then have the required estimates: \\(S = M_{7}(D9bis)\\) and \\(I = D9bis - S\\). Step 2: Calculating the Moving Seasonality Ratios For each \\(i^{\\text{th}}\\) month the mean annual changes for each component is obtained by calculating \\[{\\overline{S}}_{i} = \\frac{1}{N_{i} - 1}\\sum_{t = 2}^{N_{i}}\\left| S_{i,t} - S_{i,t - 1} \\right|\\] and \\[{\\overline{I}}_{i} = \\frac{1}{N_{i} - 1}\\sum_{t = 2}^{N_{i}}\\left| I_{i,t} - I_{i,t - 1} \\right|\\], where \\(N_{i}\\) refers to the number of months \\(\\text{i}\\)in the data, and the moving seasonality ratio of month \\(i\\): \\[MSR_{i} = \\frac{\\ {\\overline{I}}_{i}}{ {\\overline{S}}_{i}}\\]. These ratios are presented in Details of the Quality Measures node under the Decomposition (X11) section. These ratios are used to compare the year-on-year changes in the irregular component with those in the seasonal component. The idea is to obtain, for each month, an indicator capable of selecting the appropriate moving average for the removal of any noise and providing a good estimate of the seasonal factor. The higher the ratio, the more erratic the series, and the greater the order of the moving average should be used. As for the rest, by default the program selects the same moving average for each month, but the user can select different moving averages for each month. Step 3: Calculating the overall Moving Seasonality Ratio The overall Moving Seasonality Ratio is calculated as follows: \\[\\text{MSR}_{i} = \\frac{\\sum_{i}^{}{N_{i}\\ }\\ {\\overline{I}}_{i}}{\\sum_{i}^{}N_{i}{\\overline{S}}_{i}}\\] \\[13\\] Step 4: Selecting a moving average and estimating the seasonal component Depending on the value of the ratio, the program automatically selects a moving average that is applied, column by column (i.e. month by month) to the Seasonal/Irregular component in Table D8 modified, for extreme values, using values in Table D9. The default selection procedure of a moving average is based on the Moving Seasonality Ratio in the following way: If this ratio occurs within zone A (MSR &lt; 2.5), a \\(3 \\times 3\\) moving average is used; if it occurs within zone C (3.5 &lt; MSR &lt; 5.5), a \\(3 \\times 5\\) moving average is selected; if it occurs within zone E (MSR &gt; 6.5), a \\(3 \\times 9\\) moving average is used; If the MSR occurs within zone B or D, one year of observations is removed from the end of the series, and the MSR is recalculated. &gt; If the ratio again occurs within zones B or D, we start over &gt; again, removing a maximum of five years of observations. If this &gt; does not work, i.e. if we are again within zones B or D, a &gt; \\(3 \\times 5\\) moving average is selected. The chosen symmetric moving average corresponds, as the case may be 5 (\\(3 \\times 3\\)), 7 $(3 )$or 11 (\\(3 \\times 9\\)\\(3 \\times 9)\\) terms, and therefore does not provide an estimate for the values of seasonal factors in the first 2 (or 3 or 5) and the last 2 (or 3 or 5) years. These are then calculated using associated asymmetric moving averages. Moving average selection procedure, source: DAGUM, E. B.(1999) 4.7.1.2.3 Identification and replacement of extreme values X-13ARIMA-SEATS detects and removes outliers in the RegARIMA part. However, if there is a seasonal heteroscedasticity in a time series i.e. the variance of the irregular component is different in different calendar months. Examples for this effect could be the weather and snow-dependent output of the construction sector in Germany during winter, or changes in Christmas allowances in Germany and resulting from this a transformation in retail trade turnover before Christmas. The ARIMA model is not on its own able to cope with this characteristic. The practical consequence is given by the detection of additional extreme values by X-11. This may not be appropriate if the seasonal heteroscedasticity is produced by political interventions or other influences. The ARIMA models assume a constant variance and are therefore not by themselves able to cope with this problem. Choosing longer (in the case of diverging weather conditions in the winter time for the construction sector) or shorter filters (in the case of a changing pattern of retail trade turnover in the Christmas time) may be reasonable in such cases. It may even be sensible to take into account the possibility of period-specific (e.g. month-specific) standard deviations, which can be done by changing the default settings of the calendarsigma parameter (see Specifications-X13 section). The value of the calendarsigma parameter will have an impact on the method of calculation of the moving standard deviation in the procedure for extreme values detection presented below. Step 1: Estimating the seasonal component The seasonal component is estimated by smoothing the SI component separately for each period using a \\(3 \\times 3\\) moving average, i.e.: \\[ \\frac{1}{9} \\times \\begin{Bmatrix} 1,0,0,0,0,0,0,0,0,0,0,0, \\\\ 2,0,0,0,0,0,0,0,0,0,0,0, \\\\ 3,0,0,0,0,0,0,0,0,0,0,0, \\\\ 2,0,0,0,0,0,0,0,0,0,0,0, \\\\ 1,0,0,0,0,0,0,0,0,0,0,0, \\\\ \\end{Bmatrix} \\] \\[14\\] Step 2: Normalizing the seasonal factors The preliminary seasonal factors are normalized in such a way that for one year their average is equal to zero (additive model) or to unity (multiplicative model). Step 3: Estimating the irregular component The initial normalized seasonal factors are removed from the Seasonal-Irregular component to provide an estimate of the irregular component. Step 4: Calculating a moving standard deviation By default, a moving standard deviation of the irregular component is calculated at five-year intervals. Each standard deviation is associated with the central year used to calculate it. The values in the central year, which in the absolute terms deviate from average by more than the Usigma parameter are marked as extreme values and assigned a zero weight. After excluding the extreme values the moving standard deviation is calculated once again. Step 5: Detecting extreme values and weighting the irregular The default settings for assigning a weight to each value of irregular component are: Values which are more than Usigma (2.5, by default) standard deviations away (in the absolute terms) from the 0 (additive) or 1 (multiplicative) are assigned a zero weight; Values which are less than 1.5 standard deviations away (in the absolute terms) from the 0 (additive) or 1 (multiplicative) are assigned a full weight (equal to one); Values which lie between 1.5 and 2.5 standard deviations away (in the absolute terms) from the 0 (additive) or 1 (multiplicative) are assigned a weight that varies linearly between 0 and 1 depending on their position. The default boundaries for the detection of the extreme values can be changed with LSigma and USigma parameters Step 6: Adjusting extreme values of the seasonal-irregular component Values of the SI component are considered extreme when a weight less than 1 is assigned to their irregular. Those values are replaced by a weighted average of five values: The value itself with its weight; The two preceding values, for the same period, having a full weight(if available); The next two values, for the same period, having full a weight (if available). When the four full-weight values are not available, then a simple average of all the values available for the given period is taken. This general algorithm is used with some modification in parts B and C for detection and replacement of extreme values. 4.7.1.2.4 X-11 tables The list of tables produced by JDemetra+ is presented below. It is not identical to the output produced by the original X-11 program. Part A – Preliminary Estimation of Outliers and Calendar Effects. This part includes prior modifications to the original data made in the RegARIMA part: Table A1 – Original series; Table A1a – Forecast of Original Series; Table A2 – Leap year effect; Table A6 – Trading Day effect (1 or 6 variables); Table A7 – The Easter effect; Table A8 – Total Outlier Effect; Table A8i – Additive outlier effect; Table A8t – Level shift effect; Table A8s – Transitory effect; Table A9 – Effect of user-defined regression variables assigned to the seasonally adjusted series or for which the component has not been defined; Table 9sa – Effect of user-defined regression variables assigned to the seasonally adjusted series; Table9u – Effect of user-defined regression variables for which the component has not been defined. Part B – Preliminary Estimation of the Time Series Components: Table B1 – Original series after adjustment by the RegARIMA model; Table B2 – Unmodified Trend (preliminary estimation using composite moving average); Table B3 – Unmodified Seasonal – Irregular Component (preliminary estimation); Table B4 – Replacement Values for Extreme SI Values; Table B5 – Seasonal Component; Table B6 – Seasonally Adjusted Series; Table B7 – Trend (estimation using Henderson moving average); Table B8 – Unmodified Seasonal – Irregular Component; Table B9 – Replacement Values for Extreme SI Values; Table B10 – Seasonal Component; Table B11 – Seasonally Adjusted Series; Table B13 – Irregular Component; Table B17 – Preliminary Weights for the Irregular; Table B20 – Adjustment Values for Extreme Irregulars. Part C – Final Estimation of Extreme Values and Calendar Effects: Table C1 – Modified Raw Series; Table C2 – Trend (preliminary estimation using composite moving average); Table C4 – Modified Seasonal – Irregular Component; Table C5 – Seasonal Component; Table C6 – Seasonally Adjusted Series; Table C7 – Trend (estimation using Henderson moving average); Table C9 – Seasonal – Irregular Component; Table C10 – Seasonal Component; Table C11 – Seasonally Adjusted Series; Table C13 – Irregular Component; Table C20 – Adjustment Values for Extreme Irregulars. Part D – Final Estimation of the Different Components: Table D1 – Modified Raw Series; Table D2 – Trend (preliminary estimation using composite moving average); Table D4 – Modified Seasonal – Irregular Component; Table D5 – Seasonal Component; Table D6 – Seasonally Adjusted Series; Table D7 – Trend (estimation using Henderson moving average); Table D8 – Unmodified Seasonal – Irregular Component; Table D9 – Replacement Values for Extreme SI Values; Table D10 – Final Seasonal Factors; Table D10A – Forecast of Final Seasonal Factors; Table D11 – Final Seasonally Adjusted Series; Table D11A – Forecast of Final Seasonally Adjusted Series; Table D12 – Final Trend (estimation using Henderson moving average); Table D12A – Forecast of Final Trend Component; Table D13 – Final Irregular Component; Table D16 – Seasonal and Calendar Effects; Table D16A – Forecast of Seasonal and Calendar Component; Table D18 – Combined Calendar Effects Factors. Part E – Components Modified for Large Extreme Values: Table E1 – Raw Series Modified for Large Extreme Values; Table E2 – SA Series Modified for Large Extreme Values; Table E3 – Final Irregular Component Adjusted for Large Extreme Values; Table E11 – Robust Estimation of the Final SA Series. Part F – Quality indicators: Table F2A – Changes, in the absolute values, of the principal components; Table F2B – Relative contribution of components to changes in the raw series; Table F2C – Averages and standard deviations of changes as a function of the time lag; Table F2D – Average duration of run; Table F2E – I/C ratio for periods span; Table F2F – Relative contribution of components to the variance of the stationary part of the original series; Table F2G – Autocorrelogram of the irregular component. 4.7.1.3 Filter length choice A seasonal filter is a weighted average of a moving span of fixed length within a time series that can be used to remove a fixed seasonal pattern. X-13ARIMA-SEATS uses several of these filters, according to the needs of the different stages of the program. As only X-13ARIMA-SEATS allows the user to manually select seasonal filters, this case study can be applied only to the X-13ARIMA-SEATS specifications. The automatic seasonal adjustment procedure uses the default options to select the most appropriate moving average. However there are occasions when the user will need to specify a different seasonal moving average to that identified by the program. For example, if the SI values do not closely follow the seasonal component, it may be appropriate to use a shorter moving average. Also the presence of sudden breaks in the seasonal pattern – e.g. due to changes in the methodology – can negatively impact on the automatic selection of the most appropriate seasonal filter. In such cases the usage of short seasonal filters in the selected months or quarters can be considered. Usually, a shorter seasonal filter \\((3 \\times 1)\\) allows seasonality to change very rapidly over time. However, a very short seasonal filter should not normally be used, as it might often lead to large revisions as new data becomes available. If a short filter is to be used it will usually be limited to one month/quarter with a known reason for wanting to capture a rapidly changing seasonality. In the standard situation one seasonal filter is applied to all individual months/quarters. The estimation of seasonal movements is therefore based on the sample windows of equal lengths for each individual month/quarter (i.e. for each month/quarter the seasonal filter length or the number of years representing the major part of the seasonal filter weights is identical). This approach relies on the assumption that the number of past periods in which the conditions causing seasonal behaviour are sufficiently homogenous is the same in all months/quarters. However, this assumption does not always hold. Seasonal causes may change in one month, while staying the same in others16. For instance, seasonal heteroskedasticity might require different filter lengths in different months or quarters. Another interesting example is industrial production in Germany. It can be influenced by school holidays, since many employees have school-age children, which interrupt their working pattern during these school holidays. Consequently, businesses may temporarily suspend or lower production during these periods. Since school holidays do not occur at the same time throughout Germany and their timing varies from year to year in the individual federal states, the effect is not completely captured by seasonal adjustment. And since school holidays are treated as usual working days, these effects are not captured by calendar adjustment either. The majority of school holidays in Germany can take place either in July or in August. This yields higher variances in the irregular component for these months compared to the rest of the year. Therefore, in this case a longer seasonal filter is used for these months to account for this. Another example might be given by German retail trade. Due to changes in the consumers’ behaviour around Christmas – possibly more gifts of money – the seasonal peak in December has become steadily less pronounced. To account for this moving seasonality, shorter seasonal filters in December than during the rest of the year need to be applied. JDemetra+ offers the options to assign a different seasonal filter length to each period (month or quarter). The program offers these options in the single spec mode as well as in the multispec mode, albeit they are available only in the Specifications window, after a document is created. 4.7.2 Model based decomposition SEATS is a program for estimating unobserved components in a time series. It follows the ARIMA-model-based (AMB) method, developed from the work of CLEVELAND, W.P., and TIAO, G.C. (1976), BURMAN, J.P. (1980), HILLMER, S.C., and TIAO, G.C. (1982), BELL, W.R., and HILLMER, S.C. (1984) and MARAVALL, A., and PIERCE, D.A. (1987). In JDemetra+ the input for the model based signal extraction procedure is always provided by TRAMO and includes the original series \\(y_{t}\\), the linearized series \\(x_{t}\\) (i.e. the original series $y_{t}$with the deterministic effects removed), the ARIMA model for the stochastic (linearized) time series \\(x_{t}\\) and the deterministic effects (calendar effects, outliers and other regression variable effects)17. SEATS decomposes the linearized series (and the ARIMA model) into trend, seasonal, transitory and irregular components, provides forecasts for these components, together with the associated standard errors, and finally assign the deterministic effects to each component yielding the final components18. The Minimum Mean Square Error (MMSE) estimators of the components are computed with a Wiener-Kolmogorov filter applied to the finite series extended with forecasts and backcasts19. One of the fundamental assumptions made by SEATS is that the linearized time series \\(x_{t}\\) follows the ARIMA model \\[\\phi(B)\\delta\\left( B \\right)x_{t} = \\theta(B)a_{t}\\] \\[1\\] where: \\(B\\) – the backshift operator \\((Bx_{t} = x_{t - 1})\\); \\(\\delta\\left( B \\right)\\) – a non-stationary autoregressive (AR) polynomial in \\(B\\) (unit roots); \\(\\theta\\left( B \\right)\\) – an invertible moving average (MA) polynomial in \\(B\\) and in \\(B^{S}\\), which can be expressed in the multiplicative form \\(\\left( 1 + \\vartheta_{1}B + \\ldots{+ \\ \\vartheta}{q}B^{q} \\right)\\left( \\ 1 + \\Theta{1}B^{s} + \\ldots{+ \\ \\Theta}_{Q}B^{\\text{sQ}} \\right)\\) ; \\(\\phi(B)\\) – a stationary autoregressive (AR) polynomial in \\(B\\) and in $B^{S}$containing regular and seasonal unit roots, with s representing the number of observations per year; \\(a_{t}\\) – a white-noise variable with the variance\\(\\ V(a)\\). It should be noted that the stochastic time series can be predicted using its past observations and making an error. The variable \\(a_{t}\\), which is assumed to be white noise, is the fundamental innovation to the series at time t, that is the part that cannot be predicted based on the past history of the series. Denoting $( B ) = ( B )( B ),$ \\[1\\] can be written in a more concise form as \\[\\varphi\\left( B \\right)x_{t} = \\theta(B)a_{t}\\], \\[2\\] where \\(\\varphi\\left( B \\right)\\) contains both the stationary and the nonstationary roots. 4.7.2.1 Derivation of the models for the components Let us consider the additive decomposition model \\[x_{t} = \\sum_{i = 1}^{k}x_{\\text{it}}\\], \\[3\\] where i refers to the orthogonal components: trend, seasonal, transitory or irregular. Apart from the irregular component, supposed to be a white noise, it is assumed that each component follows the ARIMA model which can be represented, using the notation of \\[2\\] , as: \\[\\varphi_{i}\\left( B \\right)\\ x_{\\text{it}} = \\theta_{i}(B)a_{\\text{it}}\\], \\[4\\] where \\(\\varphi_{i}\\left( B \\right) = \\phi_{i}\\left( B \\right)\\delta_{i}\\left( B \\right),\\ \\ x_{\\text{it}}\\) is the i-th unobserved component, \\(\\varphi_{i}\\left( B \\right)\\) and \\(\\theta_{i}\\left( B \\right)\\) are finite polynomials of order \\(p_{i}\\) and \\(q_{i}\\), respectively, and \\(a_{\\text{it}},\\) the disturbance associated with such component, is a white noise process with zero mean and constant variance \\(V(a_{i})\\) and \\(a_{\\text{it}}\\) and $a_{}$are not correlated for $i j$and for any \\(t\\).. These disturbances are functions of the innovations in the series and are called “pseudo-innovations” in the literature concerning the AMB decomposition as they refer to the components that are never observed 20. In the JDemetra+ documentation the term “innovations” is used to refer to the “pseudo-innovations”. The following assumptions hold for \\[4\\] . For each \\(\\text{i}\\) the polynomials \\(\\phi_{i}\\left( B \\right)\\), \\(\\delta_{i}\\left( B \\right)\\) and \\(\\theta_{i}(B)\\) are prime and of finite order. The roots of \\(\\delta_{i}\\left( B \\right)\\) lies on the unit circle; those of \\(\\phi_{i}\\left( B \\right)\\) lie outside, while all the roots of $_{i}( B )$are on or outside the unit circle. This means that nonstationary and noninvertible components are allowed. Since different roots of the AR polynomial induce peaks in the spectrum21 of the series at different frequencies, and given that different components are associated with the spectral peaks for different frequencies, it is assumed that for \\(i \\neq j\\) the polynomials\\(\\ \\phi_{i}\\left( B \\right)\\) and \\(\\phi_{j}\\left( B \\right)\\) do not share any common root (they are coprime). Finally, it is assumed that the polynomials \\(\\theta_{i}\\left( B \\right),\\ i = 1,\\ldots,k\\) are prime share no unit root in common, guaranteeing the invertibility of the overall series. In fact, since the unit root of \\(\\theta_{i}\\left( B \\right)\\) induce a spectral zero, when the polynomials \\(\\theta_{i}\\left( B \\right),\\ i = 1,\\ldots,k\\) share no unit root in common, there is no frequency for which all component spectra become zero22. Since aggregation of ARIMA models yields ARIMA models, the series $x_{t}$will also follow an ARIMA model, as in \\[2\\] , and consequently the following identity can be derived: \\[\\frac{\\theta(B)}{\\varphi(B)}a_{t} = \\sum_{i = 1}^{k}{\\frac{\\theta_{i}(B)}{\\varphi_{i}(B)}a_{\\text{it}}}\\]. \\[5\\] In the ARIMA model based approach implemented in SEATS, the ARIMA model identified and estimated for the observed series \\(x_{t}\\) is decomposed to derive the models for the components. In particular, the AR polynomials for the components, \\(\\varphi_{i}\\left( B \\right),\\) are easily derived through the factorization of the AR polynomial \\(\\varphi\\left( B \\right)\\): \\[\\varphi\\left( B \\right) = \\prod_{i = 1}^{k}{\\varphi_{i}\\left( B \\right)}\\], \\[6\\] while the MA polynomials for the components, together with the innovation variances \\(V(a_{i})\\), cannot simply be obtained through the relationship: \\[\\theta(B)a_{t} = \\sum_{i = 1}^{k}{\\varphi_{\\text{ni}}\\left( B \\right)}\\theta_{i}(B)a_{\\text{it}}\\], \\[7\\] where \\(\\varphi_{\\text{ni}}\\left( B \\right)\\) is the product of all \\(\\varphi_{j}\\left( B \\right),\\ j = 1,\\ldots,k\\), except from \\(\\varphi_{i}\\left( B \\right)\\). Further assumptions are therefore needed to cope with the underidentification problem: i) \\(p_{i} \\geq q_{i}\\) and ii) the canonical decomposition, i.e. the decomposition that allocate all additive white noise to the irregular component (yielding noninvertible components except the irregular). To understand how SEATS factorizes the AR polynomials, first a concept of a root will be explored23. The equation \\[2\\] can be expressed as: \\[\\psi^{- 1}(B)x_{t} = a_{t}(1 + \\varphi_{1}B + \\ldots\\varphi_{p}B^{p})x_{t} =(1 + \\theta_{1}B + \\ldots\\theta_{q}B^{q})a_{t}\\], \\[8\\] Let us now consider \\[2\\] in the inverted form: \\[\\theta\\left( B \\right)y_{t} = \\varphi(B)a_{t}\\], \\[9\\] If both sides of \\[8\\] are multiplied by \\(x_{t - k}\\) with \\(k &gt; q\\), and expectations are taken, the right hand side of the equation vanishes and the left hand side becomes: \\[\\varphi(B)\\gamma_{k} = \\gamma_{k} + \\varphi_{1}\\gamma_{k - 1} + \\ldots\\varphi_{p}\\gamma_{k - p} = 0 \\], \\[10\\] where \\(B\\) operates on the subindex \\(k\\). The autocorrelation function \\(\\gamma_{k}\\) is a solution of \\[10\\] with the characteristic equation: \\[z^{p} + \\varphi_{1}z^{p - 1} + \\ldots\\varphi_{p - 1}z + \\varphi_{p} = 0\\]. \\[11\\] If \\(z_{1}\\),…,\\(\\ z_{p}\\) are the roots of \\[11\\] , the solutions of \\[10\\] can be expressed as: \\(\\gamma_{k} = \\sum_{i = 1}^{p}z_{i}^{k}\\), \\[12\\] and will converge to zero as \\(k \\rightarrow \\infty\\) when \\(\\left| r_{i} \\right| &lt; 1,\\ i = 1,\\ldots,p\\). From \\[10\\] and \\[12\\] it can be noticed that \\(z_{1} = B_{i}^{- 1}\\), meaning that \\(z_{1}\\),…,\\(\\ z_{p}\\) are the inverses of the roots \\(B_{1},\\ldots,B_{p}\\) of the polynomial \\(\\varphi(B)\\). The convergence of \\(\\gamma_{k}\\) implies that the roots of the \\(\\varphi(B)\\) are larger than 1 in modulus (lie outside the unit circle). Therefore, from the equation \\[ {\\varphi(B)}^{- 1} = \\frac{1}{(1 - z_{1})\\ldots(1 - z_{1})} \\] \\[13\\] it can be derived that \\({\\varphi(B)}^{- 1}\\) is convergent and all its inverse roots are less than 1 in modulus. Equation \\[11\\] has real and complex roots (solutions). Complex number \\(x = a + bi\\), with \\(a\\) and \\(\\text{b}\\) both real numbers, can be represented as \\(x = r\\left( cos(\\omega) + i\\ sin(\\omega \\right))\\), where \\(i\\) is the imaginary unit\\({\\ (i}^{2} = - 1)\\), \\(r\\) is the modulus of \\(x\\), that is \\(\\ r = \\left| x \\right| = \\sqrt{a^{2} + b^{2}}\\) and \\(\\omega\\) is the argument (frequency). When roots are complex, they are always in pairs of complex conjugates. The representation of the complex number \\(x = a + bi\\) has a geometric interpretation in the complex plane established by the real axis and the orthogonal imaginary axis. Text Geometric representation of a complex number and of its conjugate Representing the roots of the characteristic equation \\[11\\] in the complex plane enhances understanding how they are allocated to the components. When the modulus \\(r\\) of the roots in \\(\\text{z}\\) are greater than 1 (i.e. modulus of the roots in \\(\\varphi(B)\\ &lt; 1\\)), the solution of the characteristic equation has a systematic explosive process, which means that the impact of the given impulse on the time series is more and more pronounced in time. This behaviour is not in line with the developments that can be identified in actual economic series. Therefore, the models estimated by TRAMO-SEATS (and X-13ARIMA-SEATS) have never inverse roots in \\(B\\) with modulus greater than 1. The characteristic equations associated with the regular and the seasonal differences have roots in \\(\\varphi(B)\\) with modulus \\(r = 1\\). They are called non-stationary roots and can be represented on the unit circle. Let us consider the seasonal differencing operator applied to a quarterly time series \\((1 - B^{4})\\). Its characteristic equation is \\({(z}^{4} - 1) = 0\\) with solutions given by\\(\\ z = \\sqrt[4]{1}\\), i.e. \\(z_{1,2} = \\pm 1\\) and \\(z_{3,4} = \\pm i1\\). The first two solutions are real and the last two are complex conjugates. They are represented by the black points on the unit circle on the figure below. Text Unit roots on the unit circle For the seasonal differencing operator \\((1 - B^{12})\\) applied to the monthly time series the characteristic equation \\({\\ (z}^{12} - 1) = 0\\) has twelve non-stationary solutions given by\\(\\ z = \\sqrt[12]{1}:\\) two real and ten complex conjugates, represented by the white circles in unit roots figure above. The complex conjugates roots generate the periodic movements of the type: \\[z_{t} = A^{t}\\cos\\left( \\omega t + W \\right).\\] \\[14\\] where: \\(A\\) – amplitude; \\(\\omega\\) – angular frequency (in radians); \\(W\\) – phase (angle at \\(t = 0)\\). The frequency \\(f\\), i.e. the number of cycles per unit time, is \\(\\frac{\\omega}{2\\pi}\\). If it is multiplied by s, the number of observations per year, the number of cycles completed in one year is derived. The period of function \\[14\\] , denoted by \\(\\tau\\), is the number of units of time (months/quarters) it takes for a full circle to be completed. For quarterly series the seasonal movements are produced by complex conjugates roots with angular frequencies at \\(\\frac{\\pi}{2}\\) (one cycle per year) and \\(\\pi\\) (two cycles per year). The corresponding number of cycles per year and the length of the movements are presented in the table below. Seasonal frequencies for a quarterly time series {: .table .table-style} | Angular frequency (\\(\\omega\\)) | Frequency (cycles per unit time) (\\(f\\)) | Cycles per year | Length of the movement measured in quarters (\\(\\tau\\)) | |—————–|—————–|—————–|—————–| | \\(\\frac{\\pi}{2}\\) | 0.25 | 1 | 4 | | \\(\\pi\\) | 0.5 | 2 | 2 | For monthly time series the seasonal movements are produced by complex conjugates roots at the angular frequencies: $ ,, , , $and \\(\\pi\\). The corresponding number of cycles per year and the length of the movements are presented in the table below: Seasonal frequencies for a monthly time series. Seasonal frequencies for a monthly time series {: .table .table-style} | Angular frequency (\\(\\omega\\)) | Frequency (cycles per unit time) (\\(f\\)) | Cycles per year | Length of the movement measured in months (\\(\\tau\\)) | |—————–|—————–|—————–|—————–| | \\(\\frac{\\pi}{6}\\) | 0.083 | 1 | 12 | | \\(\\frac{\\pi}{3}\\) | 0.167 | 2 | 6 | | \\(\\frac{\\pi}{2}\\) | 0.250 | 3 | 4 | | \\(\\frac{2\\pi}{3}\\) | 0.333 | 4 | 3 | | \\(\\frac{5\\pi}{6}\\) | 0.417 | 5 | 2.4 | | \\[\\pi\\] | 0.500 | 6 | 2 | In JDemetra+ SEATS assigns the roots of the AR full polynomial to the components according to their associated modulus and frequency, i.e.:24 Roots of \\(\\left( 1 - B \\right)^{d}\\) are assigned to trend component. Roots of \\(\\ \\left( 1 - B^{s} \\right)^{d_{s}} = {((1 - B)(1 + B + \\ldots + B^{s - 1}))}^{d_{s}}\\$are assigned to the trend component (root of\\){ ( 1 - B )}^{d_{s}}\\() and to the seasonal component (roots of\\){ (1 + B + + B^{s - 1})}^{d_{s}}$). When the modulus of the inverse of a real positive root of \\(\\varphi(B)\\) is greater than \\(k\\) or equal to \\(k\\), where \\(k\\) is the threshold value controlled by the Trend boundary parameter(in the original SEATS it is controlled by rmod)25, then the root is assigned to the trend component. Otherwise it is assigned to the transitory component. Real negative inverse roots of $_{p}( B )$associated with the seasonal two-period cycle are assigned to the seasonal component if their modulus is greater than k, where \\(k\\) is the threshold value controlled by the Seasonal boundary and the Seas. boundary (unique) parameters. Otherwise they are assigned to the transitory component. Complex roots, for which the argument (angular frequency) is close enough to the seasonal frequency are assigned to the seasonal component. Closeness is controlled by the Seasonal tolerance and Seasonal tolerance (unique) parameters (in the original SEATS it is controlled by epsphi). Otherwise they are assigned to the transitory component. If \\(d_{s}\\$(seasonal differencing order) is present\\)$and \\(\\text{Bphi} &lt; 0\\) (\\(\\text{Bphi}\\) is the estimate of the seasonal autoregressive parameter), the real positive inverse root is assigned to the trend component and the other (\\(s - 1\\)) inverse roots are assigned to the seasonal component. When \\(d_{s} = 0\\), the root is assigned to the seasonal when \\(\\text{Bphi} &lt; - 0.2\\) and/or the overall test for seasonality indicates presence of seasonality. Otherwise it goes to the transitory component. Also, when \\(\\text{Bphi} &gt; 0\\), roots are assigned to the transitory component. For further details about JDemetra+ parameters see section TramoSeats. It should be highlighted that when\\(\\ Q &gt; P\\), where \\(Q\\) and \\(P\\) denote the orders of the polynomials \\(\\varphi\\left( B \\right)\\) and \\(\\theta(B)\\), the SEATS decomposition yields a pure MA \\((Q - P)\\) component (hence transitory). In this case the transitory component will appear even when there is no AR factor allocated to it. Once these rules are applied, the factorization of the AR polynomial presented by \\[2\\] yields to the identification of the AR polynomials for the components which contain, respectively, the AR roots associated with the trend component, the seasonal component and the transitory component.26 Then with the partial fraction expansion the spectrum of the final components are obtained. For example, the Airline model for a monthly time series: \\[(1 - B)(1 - B^{12})x_{t} = (1 + \\theta_{1}B)(1 + \\Theta_{1}B^{12})\\ a_{t}\\], \\[15\\] is decomposed by SEATS into the model for the trend component: \\[(1 - B)(1 - B)c_{t} = (1 + \\theta_{c,1}B + \\theta_{c,2}B^{2})a_{c,t}\\], \\[16\\] and the model for the seasonal component: \\[\\left( 1 + B + \\ldots + B^{11} \\right)s_{t} = \\left( 1 + \\theta_{s,1}B + \\ldots + {\\theta_{s,11}B}^{11} \\right)a_{s,t},\\] \\[17\\] As a result, the Airline model is decomposed as follows: \\[\\frac{(1 + \\theta_{1}B)(1 + \\Theta_{1}B^{12})}{(1 - B)(1 - B)}a_{t} = \\frac{\\left( 1 + \\theta_{s,1}B + \\ldots + {\\theta_{s,11}B}^{11} \\right)}{\\left( 1 + B + \\ldots + B^{11} \\right)}a_{s,t} + \\frac{(1 + \\theta_{c,1}B + \\theta_{c,2}B^{2})}{(1 - B)(1 - B)}a_{c,t} + u_{t}\\]. \\[18\\] The transitory component is not present in this case and the irregular component is the white noise. The partial fractions decomposition is performed in a frequency domain. In essence, it consists in portioning of the pseudo-spectrum27 of \\(x_{t}\\) into additive spectra of the components. When the AMB decomposition of the ARIMA model results in the non-negative spectra for all components, the decomposition is called admissible28. In such case an infinite number of admissible decompositions exists, i.e. decompositions that yield the non-negative spectra of all components. Therefore, the MA polynomials and the innovation variances cannot be yet identified from the model of \\(x_{t}\\). As sketched above, to solve this underidentification problem and identify a unique decomposition, it is assumed that for each component the order of the MA polynomial is no greater than the order of the AR polynomial and the canonical solution of S.C. Hillmer and G.C. Tiao is applied29, i.e. all additive white noise is added to the irregular component As a consequence all components derived from the canonical decomposition, except from the irregular, have a spectral minimum of zero and are thus noninvertible30. Given the stochastic features of the series, it can be shown by that the canonical decomposition produces as stable as possible trend and seasonal components since it maximizes the variance of the irregular and minimizes the variance of the other components31. However, there is a price to be paid as canonical components can produce larger revisions in the preliminary estimators of the component32 than any other admissible decomposition. The figure below represents the pseudo-spectrum for the canonical trend and an admissible trend. Text A comparison of canonical trend and admissible trend A pseudo-spectrum is denoted by\\(\\ g_{i}(\\omega)\\), where \\(\\omega\\) represents the angular frequency. The pseudo-spectrum of \\(x_{\\text{it}}\\) is defined as the Fourier transform of ACGF of\\(\\ x_{t}\\) which is expressed as: \\[\\frac{\\psi_{i}\\left( B \\right)\\psi_{i}\\left( F \\right)}{\\delta_{i}\\left( B \\right)\\delta_{i}\\left( F \\right)}V(a_{i})\\], \\[19\\] where: \\(\\psi_{i}\\left( F \\right) = \\frac{\\theta_{i}\\left( F \\right)}{\\phi_{i}\\left( F \\right)}\\) \\(\\psi_{i}\\left( B \\right) = \\frac{\\theta_{i}\\left( B \\right)}{\\phi_{i}\\left( B \\right)}\\) \\(B\\) is the backward operator, \\(F\\) is the forward operator. A pseudo-spectrum for a monthly time series $x_{t}$is presented in the figure below: The pseudo-spectrum for a monthly series. The frequency \\(\\omega = 0\\) is associated with the trend, frequencies in the range \\[$0 + \\epsilon_{1},\\ \\frac{\\pi}{6} - \\epsilon_{2}\\]$ with \\(\\left[0 + \\epsilon_{1},\\ \\frac{\\pi}{6} - \\epsilon_{2}\\right]\\) \\(\\epsilon_{1},\\ \\epsilon_{2} &gt; 0\\) and ${1} &lt;   - {2}$ are usually associated with the business-cycle and correspond to a period longer than a year and bounded33. The frequencies in the range \\[$\\frac{\\pi}{6},\\ \\pi\\]$ are associated with the short term movements, whose cycle is completed in less than a year. If a series contains an important periodic component, its spectrum reveals a peak around the corresponding frequency and in the ARIMA model it is captured by an AR root. In the example below spectral peaks occur at the frequency \\(\\omega = 0\\) and at the seasonal frequencies ( \\(\\frac{\\pi}{6}\\), \\(\\frac{2\\pi}{6},\\ \\frac{3\\pi}{6},\\ \\frac{4\\pi}{6},\\frac{5\\pi}{6},\\pi\\)). 34 Text The pseudo-spectrum for a monthly series In the decomposition procedure, the pseudo-spectrum of the time series \\(x_{t}\\) is divided into the spectra of its components (in the example figure below, four components were obtained). Text The pseudo-spectra for the components 4.7.2.2 Estimation of the components with the Wiener-Kolmogorow filter The various components are estimated using Wiener-Kolmogorow (WK) filters. JDemetra+ includes three options to estimate the WK filter, namely Burman, KalmanSmoother and MCElroyMatrix35. Here the first of abovementioned options, proposed by BURMAN, J.P. (1980) will be explained. The estimation procedure and the properties of the WK filter are easier to explain with a two-component model. Let the seasonally adjusted series (\\(s_{t}\\)) be the signal of interest and the seasonal component (\\(n_{t}\\)) be the remainder, “the noise”. The series is given by the model \\[2\\] and from \\[4\\] the models for theoretical components are: \\[\\varphi_{s}(B)s_{t} = \\theta_{s}(B)a_{\\text{st}}\\] \\[20\\] and \\[\\varphi_{n}(B)n_{t} = \\theta_{n}(B)a_{\\text{nt}}\\]. \\[21\\] From \\[6\\] and \\[7\\] it is clear that \\(\\varphi\\left( B \\right) = \\varphi_{s}(B)\\varphi_{n}(B)\\) and \\(\\theta\\left( B \\right)a_{t} = \\theta_{s}(B)a_{\\text{st}}+\\theta_{n}(B)a_{\\text{nt}}\\). As the time series components are never observed, their estimators have to be used. Let us note \\(X_{T}\\) an infinite realization of the time series \\(x_{t}\\). SEATS computes the Minimum Mean Square Error (MMSE) estimator of \\(s_{t}\\), e.g. the estimator \\[\\widehat{s}_{t}\\] that minimizes \\[E\\lbrack\\left({s_{t}-{\\widehat{s}}_{t})}^{2}|X_{T} \\right)\\rbrack\\]. Under the normality assumption \\[{\\widehat{s}}_{t|T}\\] is also equal to the conditional expectation \\[E\\left(s_{t}|X_{T}\\right)\\], so it can be presented as a linear function of the elements in \\[X_{T}\\].36 WHITTLE (1963) shows that the MMSE estimator of \\[{\\widehat{s}}_{t}\\] is: \\[{\\widehat{s}}_{t} = k_{s}\\frac{\\psi_{s}(B)\\psi_{s}(F)}{\\psi(B)\\psi(F)}x_{t}\\], \\[22\\] where \\[\\psi(B)= \\frac{\\theta(B)}{\\phi(B)}\\], \\[F = B^{- 1}\\] and \\[k_{s}=\\frac{V(a_{s})}{V(a)}\\], \\[V(a_{s})\\] is the variance of \\[a_{st}\\] and \\[V(a)\\] is the variance of \\[a_{t}\\]. Expressing the \\[\\psi\\left(B\\right)\\] polynomials as functions of the AR and MA polynomials, after cancelation of roots, the estimator of \\[s_{t}\\] can be expressed as: \\[{\\widehat{s}}_{t} = k_{s}\\frac{\\theta_{s}\\left(B\\right)\\theta_{s}\\left(F\\right)\\varphi_{n}\\left(B \\right)\\delta_{n}\\left(B\\right)\\varphi_{n}\\left(F\\right)\\delta_{n}\\left(F\\right)}{\\theta\\left(B\\right)\\theta\\left(F \\right)}x_{t}\\], \\[23\\] where: \\[\\nu_{s}\\left( B,F \\right) = k_{s}\\frac{\\theta_{s}\\left( B \\right)\\theta_{s}\\left( F \\right)\\varphi_{n}\\left( B \\right)\\delta_{n}\\left( B \\right)\\varphi_{n}\\left( F \\right)\\delta_{n}\\left( F \\right)}{\\theta\\left( B \\right)\\theta\\left( F \\right)}\\] \\[24\\] is a WK filter. Equation \\[24\\] shows that the WK filter is two-sided (uses observations both from the past and from the future), centered (the number of points in the past is the same as in the future) and symmetric (for any \\(k\\) the weight applied to \\(x_{t - k}\\) and \\(x_{t + k}\\) is the same), which allows the phase effect to be avoided. Due to invertibility of \\(\\theta\\left( B \\right)\\) (and \\(\\theta\\left( F \\right)\\)) the filter is convergent in the past and in the future. The estimator can be presented as \\[{\\widehat{s}}_{t} = \\nu_{i}\\left(B,F\\right)x_{t}\\], \\[25\\] where \\[\\nu_{i}\\left(B,F\\right)=\\nu_{0}+ \\sum_{j = 1}^{\\infty}\\nu_{ij}(B^{j}+F^{j})\\] is the WK filter. The example of the WK filters obtained for the pseudo-spectra of the series illustrated above is shown on the figure below: WK filters for components. Text WK filters for components The WK filter from \\[24\\] can also be expressed as a ratio of two pseudo-autocovariance generating functions (p-ACGF). The p-ACGF function summarizes the sequence of absolutely summable autocovariances of a stationary process \\(x_{t}\\) (see section section Spectral Analysis). The ACGF function of an ARIMA process is expressed as: \\[acgf(B) = \\frac{\\theta\\left( B \\right)\\theta\\left( F \\right)}{\\phi\\left( B \\right)\\delta\\left( B \\right)\\phi\\left( F \\right)\\delta\\left( F \\right)}V(a)\\] \\[26\\] And, the WK filter can be rewritten as: \\[\\nu_{s}\\left( B,F \\right) = \\frac{\\gamma_{s}(B,F)}{\\gamma(B,F)}\\], \\[27\\] where: \\[\\gamma_{s}\\left( B,F \\right) = \\frac{\\theta_{s}\\left( B \\right)\\theta_{s}\\left( F \\right)}{\\phi_{s}\\left( B \\right)\\delta_{s}\\left( B \\right)\\phi_{s}\\left( F \\right)\\delta_{s}\\left( F \\right)}V(a_{s})\\] is the p-ACGF of \\[s_{t}\\]; \\(\\gamma\\left( B,F \\right) = \\frac{\\theta\\left( B \\right)\\theta\\left( F \\right)}{\\phi\\left( B \\right)\\delta\\left( B \\right)\\phi\\left( F \\right)\\delta\\left( F \\right)}V(a)\\) is the p-ACGF of \\(x_{t}\\). From \\[24\\] it can be seen that the WK filter depends on both the component and the series models. Consequently, the estimator of the component and the WK filter reflect the characteristic of data and by construction, the WK filter adapts itself to the series under consideration. Therefore, the ARIMA model is of particular importance for the SEATS method. Its misspecification results in an incorrect decomposition. This adaptability, if the model has been correctly determined, avoids the dangers of under and overestimation with an ad-hoc filtering. For example, for the series with a highly stochastic seasonal component the filter adapts to the width of the seasonal peaks and the seasonally adjusted series does not display any spurious seasonality37. Examples of WK filters for stochastic and stable seasonal components are presented on the figure below. Text WK filters for stable and stochastic seasonal components The derivation of the components requires an infinite realization of \\(x_{t}\\) in the direction of the past and of the future. However, the convergence of the WK filter guarantees that, in practice, it could be approximated by a truncated (finite) filter and, in most applications, for large \\[k\\] the estimator for the central periods of the series can be safely seen as generated by the WK filter38: \\[{\\widehat{s}}_{t}=\\nu_{k}x_{t-k} + \\ldots + \\nu_{0}x_{t} + \\ldots + \\nu_{k}x_{t+k}\\]. \\[28\\] When \\(T &gt; 2L + 1\\), where \\(T\\) is the last observed period, and \\(L\\) is an a priori number that typically expands between 3 and 5 years, the estimator expressed by \\[23\\] can be assumed as the final (historical) estimator for the central observations of the series39. In practice, the Wiener-Kolmogorov filter is applied to \\(x_{t}\\) extended with forecasts and backcasts from the ARIMA model. The final or historical estimator of \\[{\\widehat{s}}_{t}\\], is obtained with a doubly infinite filter, and therefore contains an error \\[e_{st}\\] called final estimation error, which is equal \\[e_{st}=s_{t}-{\\widehat{s}}_{t}\\]. In the frequency domain, the Wiener-Kolmogorov filter\\(\\ \\nu(B,F)\\) that provides the final estimator of $s_{t}$is expressed as the ratio of the $s_{t}$and \\(x_{t}\\) pseudo-spectra: \\[\\widetilde{\\nu}\\left( \\omega \\right) = \\frac{g_{s}(\\omega)}{g_{x}(\\omega)}\\]. \\[29\\] The function $( )$is also referred as the gain of the filter.40 GÓMEZ, V., and MARAVALL, A. (2001a) show that when for some frequency the signal (the seasonally adjusted series) dominates the noise (seasonal fluctuations) the gain \\(\\widetilde{\\nu}\\left( \\omega \\right)\\) approaches 1. On the contrary, when for some frequency the noise dominates the gain $( )$approaches 0. The spectrum of the estimator of the seasonal component is expressed as: \\[g_{\\widehat{s}}\\left( \\omega \\right) = \\left\\lbrack \\frac{g_{s}(\\omega)}{g_{x}(\\omega)} \\right\\rbrack^{2}g_{x}(\\omega)\\], \\[30\\] where\\(\\ \\left\\lbrack \\widetilde{\\nu}\\left( \\omega \\right) \\right\\rbrack^{2} = \\left\\lbrack \\frac{g_{s}(\\omega)}{g_{x}(\\omega)} \\right\\rbrack^{2} = \\left\\lbrack \\frac{g_{s}(\\omega)}{g_{s}(\\omega) + g_{n}(\\omega)} \\right\\rbrack^{2} = \\left\\lbrack \\frac{1}{1 + \\frac{1}{r(\\omega)}} \\right\\rbrack^{2}\\) is the squared gain of the filter and \\(r\\left( \\omega \\right) = \\frac{g_{s}(\\omega)}{g_{n}(\\omega)}\\) represents the signal-to-noise ratio. For each \\(\\omega\\), the MMSE estimation gives the signal-to-noise ratio. If this ratio is high, then the contribution of that frequency to the estimation of the signal will be also high. Assume that the trend is a signal that needs to be extracted from a seasonal time series. Then $R( 0 ) = 1$and the frequency \\(\\omega = 0\\) will only be used for trend estimations. For seasonal frequencies \\(R\\left( \\omega \\right) = 0,\\) so that these frequencies are ignored in computing the trend resulting in spectral zeros in \\(g_{\\widehat{s}}\\left( \\omega \\right)\\). For this reason, unlike the spectrum of the component, the component spectrum contains dips as it can be seen on the figure below: Component spectrum and estimator spectrum for trend. Text Component spectrum and estimator spectrum for trend From the equation \\[29\\] it is clear that the squared gain of the filter determines how the variance of the series contributes to the variance of the seasonal component for the different frequencies. When \\(\\widetilde{\\nu}\\left( \\omega \\right) = 1\\), the full variation of \\(x_{t}\\) for that frequency is passed to \\[{\\widehat{s}}_{t}\\], while if \\[\\widetilde{\\nu}\\left(\\omega\\right) = 0 \\] the variation of \\(x_{t}\\) for that frequency is fully ignored in the computation of \\[{\\widehat{s}}_{t}\\]. These two cases are well illustrated by the figure below that shows the square gain of the WK filter for two series already analysed in the figure above (Figure: WK filters for stable and stochastic seasonal components). Text The squared gain of the WK filter for stable and stochastic seasonal components. Since \\(r\\left( \\omega \\right) \\geq 0\\), then \\(\\widetilde{\\nu}\\left( \\omega \\right) \\leq 1\\) and from \\[29\\] it can be derived that \\(g_{\\widehat{s}}\\left( \\omega \\right) = \\widetilde{\\nu}\\left( \\omega \\right)g_{s}(\\omega)\\). As a result, the estimator will always underestimate the component, i.e. it will be always more stable that the component.41 Since \\(g_{\\widehat{n}}\\left( \\omega \\right) &lt; g_{n}\\left( \\omega \\right)\\) and\\(\\ g_{\\widehat{s}}\\left( \\omega \\right) &lt; g_{s}\\left( \\omega \\right)\\) the expression: \\(g_{x}\\left( \\omega \\right) - \\left\\lbrack g_{\\widehat{n}}\\left( \\omega \\right) + g_{\\widehat{s}}\\left( \\omega \\right) \\right\\rbrack \\geq 0\\) is the cross-spectrum. As it is positive, the MMSE yields correlated estimators. This effect emerges since variance of estimator is smaller than the variance of component. Nevertheless, if at least one non-stationary component exists, cross-correlations estimated by TRAMO-SEATS will tend to zero as cross-covariances between estimators of the components are finite. In practice, the inconvenience caused by this property will likely be of little relevance. Preliminary estimators for the components GÓMEZ, V., and MARAVALL, A. (2001a) point out that the properties of the estimators have been derived for the final (or historical) estimators. For a finite (long enough) realization, they can be assumed to characterize the estimators for the central observations of the series, but for periods close to the beginning of the end the filter cannot be completed and some preliminary estimator has to be used. Indeed, the historical estimator shown in \\[28\\] is obtained for the central periods of the series. However, when \\(t\\) approaches \\(T\\) (last observation), the WK filter requires observations, which are not available yet. For this reason a preliminary estimator needs to be used. To introduce preliminary estimators let us consider a semi-finite realization \\(\\lbrack x_{- \\infty}\\),…\\(\\ x_{T}\\)], where \\(T\\) is the last observed period. The preliminary estimator of \\[x_{\\text{it}}\\] obtained at \\(T\\) \\[(T - t = k \\geq 0)\\] can be expressed as \\[ {\\widehat{x}}_{it|t + k}=\\nu_{i}\\left(B,F\\right)x_{t|T}^{e} \\], \\[31\\] where \\[\\nu_{i}\\left(B,F \\right)\\] is the WK filter and \\[x_{t|T}^{e}\\] is the extended series, such that \\(x_{t|T}^{e} = x_{t}\\) for \\(t \\leq T\\) and \\[x_{t|T}^{e}={\\widehat{x}}_{t|T}\\] for \\[t&gt;T\\], where \\[{\\widehat{x}}_{t|T}\\] denotes the forecast of \\(x_{t}\\) obtained at period \\(T\\). The future \\(k\\) values necessary to apply the filter are not yet available and are replaced by their optimal forecasts from the ARIMA model on \\[x_{t}\\]. When \\[k=0\\] the preliminary estimator becomes the concurrent estimator. As the forecasts are linear functions of present and past observations of \\[x_{t}\\], the preliminary estimator \\[{\\widehat{x}}_{it}\\] will be a truncated asymmetric filter applied to \\[x_{t}\\] that generates a phase effect42. When a new observation \\[x_{T + 1}\\] becomes available the forecast \\[{\\widehat{x}}_{T + 1|T}\\] is replaced by the observation and the forecast \\[{\\widehat{x}}_{iT + j|T}\\], \\[j &gt; 1\\] are updated to \\[x_{T + j|T + 1}\\] resulting in the revision error43. The total error in the preliminary estimator \\[d_{it|t + k}\\] is expressed as a sum of the final estimation error (\\[e_{it}\\]) and the revision error (\\[r_{it|t + k}\\]), i.e.: \\[ d_{it|t + k} = x_{it}-{\\widehat{x}}_{it|t + k} = \\left(x_{it} - {\\widehat{x}}_{it}\\right) + \\left( {\\widehat{x}}_{it} - {\\widehat{x}}_{it|t + k} \\right) = e_{it} + r_{it|t + k} \\], \\[32\\] where: \\[x_{it}-i^{th}\\] component; \\[{\\widehat{x}}_{it|t + k}\\]- the estimator of \\[x_{it}\\] when the last observation is \\[x_{t + k}\\]. Therefore the preliminary estimator is subject not only to the final error but also to a revision error, which are orthogonal to each other44. The revision error decreases as \\[k\\] increases, until it can be assumed equal to 0 for large enough \\[k\\]. It’s worth remembering that SEATS estimates the unobservable components of the time series so the “true” components are never observed. Therefore, MARAVALL, A. (2009) stresses that the error in the historical estimator is more of academic rather than practical interest. In practice, interest centres on revisions. (…) the revision standard deviation will be an indicator of how far we can expect to be from the optimal estimator that will be eventually attained, and the speed of convergence of \\({\\theta\\left( B \\right)\\ }^{- 1}\\) will dictate the speed of convergence of the preliminary estimator to the historical one. The analysis of an error is therefore useful for making decision concerning the revision policy, including the policy for revisions and horizon of revisions. 4.7.2.3 PsiE-weights The estimator of the component is calculated as \\[{\\widehat{x}}_{it} = \\nu_{s}\\left(B,F\\right)x_{t}\\]. By replacing \\[x_{it}=\\frac{\\theta(B)}{\\gamma(B)\\delta(B)}a_{t}\\], the component estimator can be expressed as45: \\[ {\\widehat{x}}_{it} = \\xi_{s}\\left(B,F\\right)a_{t} \\], \\[33\\] where \\(\\xi_{s}\\left( B,F \\right) = \\ldots + \\xi_{j}B^{j} + \\ldots + \\xi_{1}B + \\xi_{0} + \\xi_{- 1}F\\ldots\\xi_{- j}F^{j} + \\ldots\\). This representation shows the estimator as a filter applied to the innovation \\[a_{t}\\], rather than on the series \\[x_{t}\\]46. Hence, the filter from \\[32\\] can be divided into two components: the first one, i.e. \\[\\ldots + \\xi_{j}B^{j}+ \\ldots+ \\xi_{1}B + \\xi_{0}\\], applies to prior and concurrent innovations, the second one, i.e. \\[\\xi_{- 1}F + \\ldots + \\xi_{- j}F^{j}\\] applies to future (i.e. posterior to \\[t\\]) innovations. Consequently, \\[\\xi_{j}\\] determines the contribution of \\[a_{t - j}\\] to \\[{\\widehat{s}}_{t}\\] while \\[\\xi_{- j}\\] determines the contribution of \\[a_{t + j}\\] to \\[{\\widehat{s}}_{t}\\]. Finally, the estimator of the component can be expressed as: \\[ {\\widehat{x}}_{it} =\\xi_{i}(B)^{-}a_{t} + \\xi_{i}(F)^{+}a_{t + 1} \\], \\[34\\] where: \\(\\xi_{i}{(B)}^{-}a_{t}\\) is an effect of starting conditions, present and past innovations in series; \\(\\xi_{i}{(F)}^{+}a_{t + 1}\\) is an effect of future innovations. For the two cases already presented in figure WK filters for stable and stochastic seasonal components and figure The squared gain of the WK filter for stable and stochastic seasonal components above, the psi-weights are shown in the figure below. Text It can be shown that \\[{\\xi}_{- 1},\\ldots,\\xi_{- j}\\] are convergent and \\[\\xi_{j},\\ldots, {\\xi}_{1},\\xi_{0}\\] are divergent. From \\[33\\] , the concurrent estimator is equal to \\[ {\\widehat{x}}_{it|t} = E_{t}x_{it}=E_{t}{\\widehat{x}}_{it} = {\\xi}_{i}(B)^{-}a_{t} \\], \\[35\\] so that the revision \\[ r_{it} = {\\widehat{x}}_{it} - {\\widehat{x}}_{it|t} = \\xi_{i}(F)^{+}a_{t + 1} \\] \\[36\\] is a zero-mean stationary MA process. As a result, historical and preliminary estimators are cointegrated. From expression \\[25\\] the relative size of the full revision and the speed of convergence can be obtained. 4.7.3 Quality assesment of the seasonal adjustement process 4.7.3.1 Diagnostics reading 4.7.3.2 Residual seasonality We consider below tests on the seasonally adjusted series (\\(sa_t\\)) or on the irregular component (\\(irr_t\\)). When the reasoning applies on both components, we will use \\(y_t\\). The functions \\(stdev\\) stands for “standard deviation” and \\(rms\\) for “root mean squares” The tests are computed on the log-transformed components in the case of multiplicative decomposition. 4.7.4 Non significant irregular When \\(ir_t\\) is not significant, we don’t compute the tests on it, to avoid irrelevant results. We consider that \\(ir_t\\) is significant if \\(stdev( ir_t)&gt;0.01\\) (multiplicative case) or if \\(stdev(ir_t)/rms(sa_t) &gt;0.01\\) (additive case). 4.7.5 QS test The QS test is similar to the Ljung-Box test computed on the first two seasonal lags, except that negative auto-correlations are set to 0. The tests are computed on \\(\\tilde \\Delta sa_t\\) and on \\(\\tilde \\Delta irr_t\\). The operator \\(\\tilde \\Delta\\) applies as much differencing as needed on the (log-transformed) series and corrects the result for mean effect. The tests are not computed if the differences are not “significant”. \\(\\tilde \\Delta y_t\\) is significant if \\(stdev(\\tilde \\Delta y_t)/rms(y_t) &gt;0.005\\). 4.7.6 F test We compute by OLS the following model (SD = contrasts of seasonal dummies, freq-1 variables): \\[y_t=\\mu + \\alpha y_{t-1} + \\beta SD_t + \\epsilon_t \\] The tests are the usual joint F-tests on \\(\\beta \\quad (H_0:\\beta=0)\\). By default, we compute the tests on the 8 last years of the components, so that they might highlight moving seasonal effects. We could consider various modelling of the series. For instance, \\(y_t\\) could follow an ARIMA model, pre-specified or automatically identified. The solution used in JD+ 2.2 has the advantage of being fast and robust. It is rather similar to what is used, for instance, in the Canova-Hansen test. 4.7.6.1 Revision history link to implementation in GUI add implemenation in R ? add illustrations ? (gui graphs ? estp training) Revisions are calculated as differences between the first (earliest) adjustment of an observation at time \\(t\\), computed when this observation is the last observation of the time series (concurrent adjustment, denoted as \\(A_{t|t}\\)) and a later adjustment based on all future data available at the time of the diagnostic analysis (the most recent adjustment, denoted as \\(A_{t|N}\\)). In the case of the multiplicative decomposition the revision history of the seasonal adjustment from time $N_{0}$to \\(N_{1}\\) is a sequence of \\(R_{t|N}^{A}\\) calculated in the following way : \\[ R_{t|N}^{A} = 100 \\times \\frac{A_{t|N} - A_{t|t}}{A_{t|t}} \\] The revision history of the trend is computed in the same manner. With an additive decomposition \\(R_{t|N}^{A}\\) is calculated in the same way if all values \\(A_{t|t}\\) have the same sign. Otherwise differences are calculated as: \\[ R_{t|N}^{A} = A_{t|N} - A_{t|t} \\] The analogous expression for the trend component is: \\[ R_{t|N}^{T} = T_{t|N} - T_{t|t} \\] Revision in the period-to-period (month-on-month or quarter-to-quarter) change in the seasonally adjusted series at time \\(t\\) calculated from the series \\(y_{1},y_{2},\\ldots y_{n}\\) is defined as: \\[ R_{t}^{A} = C_{t|N} - C_{t|t} \\] where \\[ \\text{C}_{t|n}^{A} = \\frac{A_{t|n} - A_{t - 1|n}}{A_{t - 1|n}} \\]. Revisions for the period-to-period changes in the trend component are computed in the same manner. 4.7.7 Sliding spans The sliding spans technique involves the comparison of the correlated seasonal adjustments of a given period obtained by applying the adjustment procedure to a sequence of two, three or four overlapping spans of data, all of which contain this period (month or quarter)47. Each period that belongs to more than one span is examined to see if its seasonal adjustments vary more than a specified amount across the spans48. For the multiplicative decomposition a seasonal factor is regarded to be unreliable if the following condition is fulfilled: \\[ S_{t}^{\\max} = \\frac{\\max_{k \\in N_{t}}S_{t}\\left( k \\right) - \\min_{k \\in N_{t}}S_{t}(k)}{\\min_{k \\in N_{t}}S_{t}(k)} &gt; 0.03 \\] where: \\(S_{t}(k)\\) – the seasonal factor estimated from span \\(k\\) for month (quarter) \\(t\\); \\(N_{t}\\) – {\\(\\text{k}\\): month (quarter) \\(\\text{t}\\) is in the \\(k\\)-th span}. For the additive decomposition JDemetra+ uses the rule in equation [2] for checking for the reliability of the seasonal factor. \\[ S_{t}^{\\max} = \\frac{\\max_{k \\in N_{t}}S_{t}\\left( k \\right) - \\min_{k \\in N_{t}}S_{t}(k)}{\\sqrt{\\frac{\\sum_{i}^{n}y_{i}^{2}}{n}}} &gt; 0.03 \\] where: \\(n\\) – number of observations of the orginal time series \\(y_{i}\\). The month-to-month percentage change in the seasonally adjusted value from span \\(k\\) for month \\(t\\) is calculated as: \\[ \\text{MM}_{m}\\left(k\\right) = \\frac{A_{m}\\left(k\\right) - A_{m - 1}\\left(k\\right)}{A_{m - 1}\\left(k\\right)} \\] where: \\(A_{m}\\left( k \\right)\\) – the seasonally (and trading day) adjusted value from span \\(k\\) for month \\(t\\); \\(\\text{MM}_{m}\\left( k \\right)\\) is considered unreliable if the statistics below is higher than 0.03. \\[ \\text{MM}_{m}^{\\max} = \\max_{k \\in {N1}_{m}}\\text{MM}_{m}\\left( k \\right) - \\min_{k \\in {N1}_{m}}\\text{MM}_{m}\\left( k \\right) &gt; 0.03 \\] where: \\({N1}_{t}\\) – {\\(\\text{k}\\): month \\(\\text{t}\\) and \\(t\\)-1 are in the \\(k\\)-th span}. The respective formula for the quarter-to-quarter percentage change in the seasonally adjusted value from span \\(k\\) for quarter \\(t\\) is calculated as: \\[ \\text{QQ}_{q}\\left( k \\right) = \\frac{A_{q}\\left( k \\right) - A_{q - 1}\\left( k \\right)}{A_{q - 1}\\left( k \\right)} \\] where: \\(A_{q}\\left( k \\right)\\) – the seasonally (and trading day) adjusted value from span \\(k\\) for quarter \\(q\\). \\(\\text{QQ}_{q}\\left( k \\right)\\) is considered unreliable if the statistics below is higher than 0.03. \\[ \\text{QQ}_{q}^{\\max} = \\max_{k \\in {N1}_{q}}\\text{QQ}_{q}\\left( k \\right) - \\min_{k \\in {N1}_{t}}\\text{QQ}_{q}\\left( k \\right) &gt; 0.03 \\] where: \\({N1}_{q}\\) – {\\(\\text{k}\\): quarter \\(\\text{t}\\) and \\(t\\)-1 are in the \\(k\\)-th span}. The respective diagnostic can be also performed for the trading days/working days component. 4.7.8 Revision policies harmonize voc (cf. cruncher vignette) and guidelines pb add controlled current add bbk plugin replace descp by my pdf ? (how to allow modif in the project ?) (auxiliary file) A description of the options is presented in the following table. Option Description Partial concurrent adjustment → Fixed model The ARIMA model, outliers and other regression parameters are not re-identified and the values of all parameters are fixed. The transformation type remains unchanged. Partial concurrent adjustment → Estimate regression coefficients The ARIMA model, outliers and other regression parameters are not re-identified. The coefficients of the ARIMA model are fixed, other coefficients are re-estimated. The transformation type remains unchanged. Partial concurrent adjustment → Estimate regression coefficients + Arima parameters The ARIMA model, outliers and other regression parameters are not re-identified. All parameters of the RegARIMA model are re-estimated. The transformation type remains unchanged. Partial concurrent adjustment → Estimate regression coefficients + Last outliers The ARIMA model, outliers (except from the outliers in the last year of the sample) and other regression parameters are not re-identified. All parameters of the RegARIMA model are re-estimated. The outliers in the last year of the sample are re-identified. The transformation type remains unchanged. Partial concurrent adjustment → Estimate regression coefficients + all outliers The ARIMA model and regression parameters, except from outliers) are not re-identified. All parameters of the RegARIMA model are re-estimated. All outliers are re-identified. The transformation type remains unchanged. Partial concurrent adjustment → Estimate regression coefficients + Arima model Re-identification of the ARIMA model, outliers and regression variables, except from the calendar variables. The transformation type remains unchanged. Concurrent Re-identification of the whole RegARIMA model. 4.7.8.0.1 Partial concurrent adjustment According to the ESS Guidelines on Seasonal Adjustment (2015), partial concurrent adjustment is the strategy in which the model, filters, outliers and calendar regressors are re-identified once a year and the respective parameters and factors re-estimated every time new or revised data become available. JDemetra+ offers several types of partial concurrent adjustment. 4.7.9 Practical steps 4.7.9.1 Graphical analysis recommendations (edit) (link to GUI graphical capabilities, graphs with rjd) The Tools menu includes, among other functionalities, tools that are helpful for the graphical analysis of a time series. The ‘X-13ARIMA-SEATS Reference Manual’ (2015) strongly recommends studying a high resolution plot of the time series as it is helpful to gain insight on issues, such as, seasonal patterns, potential outliers and stochastic non-stationarity. Also the ‘ESS Guidelines on Seasonal Adjustment’ (2015) recommends carrying out a graphical analysis on both unadjusted data and the initial run of the seasonal adjustment. Graphical analysis should consider: * The length of the series and the model span; * The presence of zeros, outliers or problems in the data; * The structure of the series: presence of; long term and cyclical movements, seasonal components, volatility etc.; * The presence of possible breaks in the seasonal behaviour; * The decomposition scheme (additive, multiplicative). The ‘ESS Guidelines on Seasonal Adjustment’ (2015), recommend that this exercise should be performed and documented for the most important series to be adjusted at least once a year. The following functionalities are available from the Tools menu: When the series are non-stationary differentiation is performed before the seasonality tests.↩︎ The unmodified Seasonal-Irregular component corresponds to the Seasonal-Irregular factors with the extreme values.↩︎ DAGUM, E.B. (1987).↩︎ For definition of the periodogram and Fourier frequencies see section Spectral Analysis↩︎ For definition of the periodogram and Fourier frequencies see section Spectral Analysis↩︎ When the series are non-stationary differentiation is performed before the seasonality tests.↩︎ ESS Guidelines on Seasonal Adjustment (2015).↩︎ Gómez, V., and Maravall, A. (1998).↩︎ Formula and further information available in Grudkowska, S. (2015).↩︎ Description from Guide to seasonal adjustment with X-12-ARIMA (2007).↩︎ The likelihood function is the joint probability (density) function of observable random variables. It is viewed as the function of the parameters given the realized random variables. Therefore, this function measures the probability of observing the particular set of dependent variable values that occur in the sample.↩︎ AIC, AICC, BIC and Hannan-Quinn criteria are used by X-13ARIMA-SEATS while BIC (TRAMO definition) by TRAMO/SEATS. These criteria are used in seasonal adjustment procedures for the selection of the optimum ARIMA model. The model with the smaller value of the model selection criteria is preferred.↩︎ Maximum Likelihood Estimation is a statistical method for estimating the coefficients of a model. This method determines the parameters that maximize the probability (likelihood) of the sample data.↩︎ The variable is called statistically significant if it is so extreme that such a result would be expected to arise simply by chance only in rare circumstances (with the probability equal to p-value). Generally, the regressor is thought to be significant if p-value is lower than 5%.↩︎ Missing observations are treated as additive outliers and interpolated accordingly.↩︎ When the series are non-stationary differentiation is performed before the seasonality tests.↩︎ In the original software SEATS can be used either with TRAMO, operating on the input received from the latter, or alone, fitting an ARIMA model to the series.↩︎ GÓMEZ, V., and MARAVALL, A. (1998).↩︎ GÓMEZ, V., and MARAVALL, A. (1997).↩︎ GÓMEZ, V., and MARAVALL, A. (2001a).↩︎ For description of the spectrum see section Spectral Analysis.↩︎ MARAVALL, A. (1995).↩︎ Description based on KAISER, R., and MARAVALL, A. (2000) and MARAVALL, A. (2008c).↩︎ For details see MARAVALL, A., CAPORELLO, G., PÉREZ, D., and LÓPEZ, R. (2014).↩︎ In JDemetra+ this argument is called Trend boundary.↩︎ The AR roots close to or at the trading day frequency generates a stochastic trading day component. A stochastic trading day component is always modelled as a stationary ARMA(2,2), where the AR part contains the roots close to the TD frequency, and the MA(2) is obtained from the model decomposition (MARAVALL, A., and PÉREZ, D. (2011)). This component, estimated by SEATS, is not implemented by the current version of JDemetra+.↩︎ The term pseudo-spectrum is used for a non-stationary time series, while the term spectrum is used for a stationary time series.↩︎ If the ARIMA model estimated in TRAMO does not accept an admissible decomposition, SEATS replaces it with a decomposable approximation. The modified model is therefore used to decompose the series. There are also other rare situations when the ARIMA model chosen by TRAMO is changed by SEATS. It happens when, for example, the ARIMA models generate unstable seasonality or produce a senseless decomposition. Such examples are discussed by MARAVALL, A. (2009).↩︎ HILLMER, S.C., and TIAO, G.C. (1982).↩︎ GÓMEZ, V., and MARAVALL, A. (2001a).↩︎ HILLMER, S.C., and TIAO, G.C. (1982).↩︎ MARAVALL, A. (1986).↩︎ Ibid.↩︎ KAISER, R., and MARAVALL, A. (2000).↩︎ The choice of the estimation method is controlled by the Method parameter, explained in the SEATS specification section.↩︎ MARAVALL, A. (2008c).↩︎ MARAVALL, A. (1995).↩︎ MARAVALL, A., and PLANAS, C. (1999).↩︎ MARAVALL, A. (1998).↩︎ GÓMEZ, V., and MARAVALL, A. (2001a).↩︎ Ibid.↩︎ KAISER, R., and MARAVALL, A. (2000).↩︎ MARAVALL, A. (1995).↩︎ MARAVALL, A. (2009).↩︎ The section is based on KAISER, R., and MARAVALL, A. (2000).↩︎ See section PsiE-weights. For further details see MARAVALL, A. (2008).↩︎ FINDLEY, D., MONSELL, B.C., SHULMAN, H.B., and PUGH, M.G. (1990).↩︎ FINDLEY, D., MONSELL, B.C., BELL, W., OTTO, M., and CHEN, B.-C. (1990).↩︎ "],["high-frequency-data-seasonal-adjustment.html", "Chapter 5 High Frequency data (seasonal adjustment) 5.1 Motivation 5.2 Unobserved Components 5.3 Seasonality tests 5.4 Calendar correction 5.5 Outliers and intervention variables 5.6 Pre-adjustment 5.7 Decomposition", " Chapter 5 High Frequency data (seasonal adjustment) 5.1 Motivation 5.2 Unobserved Components 5.3 Seasonality tests 5.4 Calendar correction 5.5 Outliers and intervention variables 5.6 Pre-adjustment 5.7 Decomposition "],["generating-calendar-regressors-and-other-input-variables.html", "Chapter 6 Generating Calendar regressors and other input variables 6.1 Motivation 6.2 Underlying theory 6.3 Available Tools", " Chapter 6 Generating Calendar regressors and other input variables 6.1 Motivation 6.2 Underlying theory 6.3 Available Tools 6.3.1 GUI 6.3.2 Rjd3modelling package "],["outlier-detection.html", "Chapter 7 Outlier detection 7.1 Motivation 7.2 With Reg Arima models 7.3 With structural models", " Chapter 7 Outlier detection 7.1 Motivation 7.2 With Reg Arima models 7.2.1 Part of preadjustment 7.2.2 specific TERROR tool 7.3 With structural models "],["modelling-time-series.html", "Chapter 8 Modelling Time Series 8.1 Motivation", " Chapter 8 Modelling Time Series 8.1 Motivation "],["benchmarking-and-temporal-disagreggation.html", "Chapter 9 Benchmarking and temporal disagreggation 9.1 Benchmarking overview 9.2 Underlying Theory 9.3 Tools 9.4 References", " Chapter 9 Benchmarking and temporal disagreggation 9.1 Benchmarking overview Often one has two (or multiple) datasets of different frequency for the same target variable. Sometimes, however, these datasets are not coherent in the sense that they don’t match up. Benchmarking49 is a method todeal with this situation. An aggregate of a higher-frequency measurement variables is not necessarily equal to the corresponding lower-frequency less-aggregated measurement. Moreover, the sources of data may have different reliability levels. Usually, less frequent data are considered more trustworthy as they are based on larger samples and compiled more precisely. The more reliable measurements, hence often the less frequent, will serve as benchmark. In seasonal adjustment methods benchmarking is the procedure that ensures the consistency over the year between adjusted and non-seasonally adjusted data. It should be noted that the ESS Guidelines on Seasonal Adjustment (2015), do not recommend benchmarking as it introduces a bias in the seasonally adjusted data. The U.S. Census Bureau also points out that “forcing the seasonal adjustment totals to be the same as the original series annual totals can degrade the quality of the seasonal adjustment, especially when the seasonal pattern is undergoing change. It is not natural if trading day adjustment is performed because the aggregate trading day effect over a year is variable and moderately different from zero”[^2]. Nevertheless, some users may need that the annual totals of the seasonally adjusted series match the annual totals of the original, non-seasonally adjusted series50. According to the ESS Guidelines on Seasonal Adjustment (2015), the only benefit of this approach is that there is consistency over the year between adjusted and the non-seasonally adjusted data; this can be of particular interest when low-frequency (e.g. annual) benchmarking figures officially exist (e.g. National Accounts, Balance of Payments, External Trade, etc.) and where users’ needs for time consistency are stronger. 9.2 Underlying Theory Benchmarking51 is a procedure widely used when for the same target variable the two or more sources of data with different frequency are available. Generally, the two sources of data rarely agree, as an aggregate of higher-frequency measurements is not necessarily equal to the less-aggregated measurement. Moreover, the sources of data may have different reliability. Usually it is thought that less frequent data are more trustworthy as they are based on larger samples and compiled more precisely. The more reliable measurement is considered as a benchmark. Benchmarking also occurs in the context of seasonal adjustment. Seasonal adjustment causes discrepancies between the annual totals of the seasonally unadjusted (raw) and the corresponding annual totals of the seasonally adjusted series. Therefore, seasonally adjusted series are benchmarked to the annual totals of the raw time series52. Therefore, in such a case benchmarking means the procedure that ensures the consistency over the year between adjusted and non-seasonally adjusted data. It should be noted that the ‘ESS Guidelines on Seasonal Adjustment’ (2015) do not recommend benchmarking as it introduces a bias in the seasonally adjusted data. Also the U.S. Census Bureau points out that: Forcing the seasonal adjustment totals to be the same as the original series annual totals can degrade the quality of the seasonal adjustment, especially when the seasonal pattern is undergoing change. It is not natural if trading day adjustment is performed because the aggregate trading day effect over a year is variable and moderately different from zero.53 Nevertheless, some users may prefer the annual totals for the seasonally adjusted series to match the annual totals for the original, non-seasonally adjusted series54. According to the ‘ESS Guidelines on Seasonal Adjustment’ (2015), the only benefit of this approach is that there is consistency over the year between adjusted and non-seasonally adjusted data; this can be of particular interest when low-frequency (e.g. annual) benchmarking figures officially exist (e.g. National Accounts, Balance of Payments, External Trade, etc.) where user needs for time consistency are stronger. The benchmarking procedure in JDemetra+ is available for a single seasonally adjusted series and for an indirect seasonal adjustment of an aggregated series. In the first case, univariate benchmarking ensures consistency between the raw and seasonally adjusted series. In the second case, the multivariate benchmarking aims for consistency between the seasonally adjusted aggregate and its seasonally adjusted components. Given a set of initial time series \\[\\left\\{ z_{i,t} \\right\\}_{i \\in I}\\], the aim of the benchmarking procedure is to find the corresponding \\[\\left\\{ x_{i,t} \\right\\}_{i \\in I}\\] that respect temporal aggregation constraints, represented by \\[X_{i,T} = \\sum_{t \\in T}^{}x_{i,t}\\] and contemporaneous constraints given by \\[q_{k,t} = \\sum_{j \\in J_{k}}^{}{w_{\\text{kj}}x_{j,t}}\\] or, in matrix form: \\[q_{k,t} = w_{k}x_{t}\\]. The underlying benchmarking method implemented in JDemetra+ is an extension of Cholette's55 method, which generalises, amongst others, the additive and the multiplicative Denton procedure as well as simple proportional benchmarking. The JDemetra+ solution uses the following routines that are described in DURBIN, J., and KOOPMAN, S.J. (2001): The multivariate model is handled through its univariate transformation, The smoothed states are computed by means of the disturbance smoother. The performance of the resulting algorithm is highly dependent on the number of variables involved in the model (\\(\\propto \\ n^{3}\\)). The other components of the problem (number of constraints, frequency of the series, and length of the series) are much less important (\\(\\propto \\ n\\)). From a theoretical point of view, it should be noted that this approach may handle any set of linear restrictions (equalities), endogenous (between variables) or exogenous (related to external values), provided that they don’t contain incompatible equations. The restrictions can also be relaxed for any period by considering their \"observation\" as missing. However, in practice, it appears that several kinds of contemporaneous constraints yield unstable results. This is more especially true for constraints that contain differences (which is the case for non-binding constraints). The use of a special square root initialiser improves in a significant way the stability of the algorithm. 9.3 Tools 9.4 References DAGUM, E.B. (1980).↩︎ GÓMEZ, V., and MARAVALL, A. (2001b).↩︎ Description of the idea of benchmarking is based on DAGUM, B.E., and CHOLETTE, P.A. (1994) and QUENNEVILLE, B. et all (2003). Detailed information can be found in: DAGUM, B.E., and CHOLETTE, P.A. (2006).↩︎ DAGUM, B.E., and CHOLETTE, P.A. (2006).↩︎ ’X-12-ARIMA Reference Manual’ (2011).↩︎ HOOD, C.C.H. (2005).↩︎ CHOLETTE, P.A. (1979).↩︎ "],["trend-cycle-estimation-1.html", "Chapter 10 Trend-cycle estimation 10.1 Motivation 10.2 Underlying Theory 10.3 Tools", " Chapter 10 Trend-cycle estimation 10.1 Motivation 10.2 Underlying Theory 10.3 Tools is there a specific package "],["nowcasting.html", "Chapter 11 Nowcasting", " Chapter 11 Nowcasting "],["graphical-user-interface.html", "Chapter 12 Graphical User Interface 12.1 Overview 12.2 List of Available Algorithms and features 12.3 Installation Procedure 12.4 Global structure 12.5 Seasonal adjustment 12.6 An automatic seasonal adjustment 12.7 Adding User-defined variables 12.8 Seasonality tests 12.9 Series Modelling 12.10 Outlier detection (TERROR) 12.11 Benchmarking 12.12 Direct-Indirect comparison 12.13 Generatiing Output", " Chapter 12 Graphical User Interface 12.1 Overview why use the graphical user interface ? what is not directly available in R yet? obj here: make parts shorter and steps more obvious SA will be detailed as the most used (the common pars just linked ) 12.1.1 Chapter oragnisation remarks 12.1.1.1 Modelling vs Pre-adjustment for linearization via Reg-Arima (or tramo) 2 functions : modelling and SA, where it is the pre-adjustment part : we’ll describe everything in sa and just ref modelleing (indem rjd) #### Single series vs multiple series 12.2 List of Available Algorithms and features solve the double “documents” in SA and in modelling describe difference doc vs ws (non output in doc ?) see if the same in version 3 in fine Table (same in R packages and in 03 Main-features ?) Algorithm Graphical User interface R packages Seasonality tests Menu Statistical Methods rjd3toolkit direct-indirect Col1 Col2 Col3 12.3 Installation Procedure JDemetra+ is a stand-alone application packed in a zip package. To run JDemetra+ the Java RE 8 or higher is needed. Java RE can be downloaded from Oracle website. The official release of JDemetra+ is accessible at a dedicated Github page. The site presents all available releases - both official releases (labelled in green as latest releases) and pre-releases (labelled in red) - packed in zip packages. From the Latest release section either choose the installer appropriate for your operating system (Windows, Linux, Mac OS, Solaris) or take the portable zip-file. The installation process is straightforward and intuitive. For example, when the zip-file is chosen and downloaded, then under Windows OS the application can be found in the “bin”-folder of the installation/unpacked zip. To open an application, double click on nbdemetra.exe or nbdemetra64.exe depending on the system version (nbdemetra.exe for the 32-bit system version and nbdemetra64.exe for the 64-bit system version). Text Launching JDemetra+ If the launching of JDemetra+ fails, you can try the following operations: Check if Java SE Runtime Environment (JRE) is properly installed by typing in the following command in a terminal: java –version Check the logs in your home directory: %appdata%/.nbdemetra/dev/var/log/ for Windows; ~/.nbdemetra/dev/var/log/ for Linux and Solaris; ~/Library/Application Support/.nbdemetra/dev/var/log/ for Mac OS X. In order to remove a previously installed JDemetra+ version, the user should delete an appropriate JDemetra+ folder. 12.3.1 Running JDemetra+ To open an application, navigate to the destination folder and double click on nbdemetra.exe or nbdemetra64.exe depending on the system version (nbdemetra.exe for the 32-bit system version and nbdemetra64.exe for the 64-bit system version). Text Running JDemetra+ 12.3.2 Closing JDemetra+ To close the application, select File → Exit from the File menu. Text Closing JDemetra+ The other way is to click on the close box in the upper right-hand corner of the JDemetra+ window. If there is any unsaved work, JDemetra+ will display a warning and provide the user with the opportunity to save it. The message box is shown below. Text The warning from leaving JDemetra+ without saving the workspace 12.4 Global structure here menus and functions common to all the algos 12.4.1 Interface Starting Winwow The default view of the JDemetra+ window, which is displayed after launching the program, is shown below. Text JDemetra+ default window By default, on the left hand side of the window two panels are visible: the Workspace panel and the Providers panel. The Workspace panel stores the work performed by the user in a coherent and structured way. The Providers panel presents the list of the data sources and organizes the imported series within each data provider. By default, JDemetra+ supports the following data sources: * JDBC; * ODBC; * SDMX; * Excel spreadsheets; * TSW (input files for the Tramo-Seats-Windows application by the Bank of Spain); * TXT; * USCB (input files for the X-13-ARIMA-SEATS application by the U.S. Census Bureau); * XML. All standard databases (Oracle, SQLServer, DB2, MySQL) are supported by JDemetra+ via JDBC, which is a generic interface to many relational databases. Other providers can be added by users by creating plugins. We will now focus on the Spreadsheets data source, which corresponds to the series prepared in an Excel file. The file should have dates in Excel date format. Dates should be placed in the first column (or in the first row) and titles of the series in the corresponding cell of the first row (or in the first column). The top-left cell [A1] can include text or it can be left empty. The empty cells are interpreted by JDemetra+ as missing values and they can appear at the beginning, in the middle and at the end of the time series. The example is shown below. Text Example of an Excel spreadsheet that can be imported to JDemetra+ Once the spreadsheet is prepared and saved, it can be imported to JDemetra+ as it is shown by the tutorial below. Alt An example of importing process for the Excel file The default JDemetra+ window, which is displayed after launching the program, is clearly divided into several panels. Text JDemetra+ default view The key parts of the user interface are: * The application menu. * The Providers window, which organises time series; * The Workspace window, which stores results generated by the software as well as settings used to create them; * A central empty zone for presenting the actual analyses further called the Results panel. 12.4.2 Application menu The majority of functionalities are available from the main application menu, which is situated at the very top of the main window. If the user moves the cursor to an entry in the main menu and clicks on the left mouse button, a drop-down menu will appear. Clicking on an entry in the drop-down menu selects the highlighted item. Text The main menu with selected drop-down menu The functions available in the main application menu are: * File * Statistical methods * X-13Doc * RegArimaDoc * TramoDoc * TramoSeatsDoc * View * Tools * Window * Help 12.4.3 Results panel The blank zone in the figure above (on the right of the view) is the location where JDemetra+ displays various windows. More than one window can be displayed at the same time. Windows can overlap with each other with the foremost window being the one in focus or active. The active window has a darkened title bar. The windows in the results panel can be arranged in many different ways, depending on the user’s needs. The example below shows one of the possible views of this panel. The results of the user’s analysis are displayed in an accompanying window. The picture below shows two panels – a window containing seasonal adjustment results (upper panel) and another one containing an autoregressive spectrum (lower panel). Text The Results panel filled with two windows 12.4.4 Menus 12.4.4.1 File Menu The File menu is intended for working with workspaces and data sources. It offers the following functions: * New Workspace – creates a new workspace and displays it in the Workspace window with a default name (Workspace_#number); * Open Workspace – opens a dialog window, which enables the user to select and open an existing workspace; * Open Recent Workspace – presents a list of workspaces recently created by the user and enables the user to open one of them; * Save Workspace – saves the project file named by the system under the default name (Workspace_#number) and in a default location. The workspace can be re-opened at a later time; * Save Workspace As… – saves the current workspace under the name chosen by the user in the chosen location. The workspace can be re-opened at a later time; * Open Recent – presents a list of datasets recently used and enables the user to open one of them; * Exit – closes an application. Text The content of the File menu 12.4.4.2 View Menu The View menu contains functionalities that enable the user to modify how JDemetra+ is viewed. It offers the following items: * Split – the function is not operational in the current version of the software. * Toolbars – displays selected toolbars under the main menu. The File toolbar contains the Save all icon. The Performance toolbar includes two icons: one to show the performance of the application, the other to stop the application profiling and taking a snapshot. The Other toolbar determines the default behaviour of the program when the user double clicks on the data. It may be useful to plot the data, visualise it on a grid, or to perform any pre-specified action, e.g. execute a seasonal adjustment procedure. * Show Only Editor – displays only the Results panel and hides other windows (e.g. Workspace and Providers). * Full Screen – displays the current JDemetra+ view in full screen. Text The View menu 12.4.4.3 Window menu The Window menu offers several functions that facilitate the analysis of data and enables the user to adjust the interface view to the user’s needs. Text The Window menu Preview Time Series – opens a window that plots any of the series the user selects from Providers. Debug – opens a Preview Time Series window that enables a fast display of the graphs for time series from a large dataset. To display the graph click on the series in the Providers window. Providers – opens (if closed) and activates the Providers window. Variables – opens (if closed) and activates the Variable window. Workspace – opens (if closed) and activates the Workspace window. Output – a generic window to display outputs in the form of text; useful with certain plug-ins (e.g. tutorial descriptive statistics). Editor – activates the editor panel (and update the main menu consequently). Configure Window – enables the user to change the way that the window is displayed (maximise, float, float group, minimise, minimise group). This option is active when some window is displayed in the JD+ interface. Properties – opens the Properties window and displays the properties of the marked item (e.g. time series, data source). Reset Windows – restores the default JDemetra+ view. Close Window – closes all windows that are open. Close All Documents – closes all documents that are open. Close Other Documents – closes all documents that are open except for the one that is active (which is the last activated one). Document Groups – enables the user to create and manage the document groups. Documents – lists all documents that are active. 12.4.4.4 Workspace Menu Workspace is a JDemetra+ functionality that stores the work performed by the user in a coherent and structured way. By default, each workspace contains the pre-defined modelling and seasonal adjustment specifications and a basic calendar. A specification is a set of modelling and/or seasonal adjustment parameters. Within the workspace the following items can be saved: * User-defined modelling specifications and seasonal adjustment specifications; * Documents that contain results from time series modelling and output from the seasonal adjustment process; * User-defined calendars; * User-defined regression variables. Together with the results from modelling and seasonal adjustment, the original data, paths to the input files and parameters of processes are all saved. These results can then be re-opened, updated, investigated and modified in further JDemetra+ sessions. The workspace saved by JDemetra+ includes: * Main folder containing several folders that correspond to the different types of items created by the user and; * The xml file that enables the user to import the workspace to the application and to display its content. An example of the workspace is shown in the figure below. Text A workspace saved on PC The workspace can be shared with other users, which eases the burden of work with defining specifications, modelling and seasonal adjustment processes. The content of the workspace is presented in the Workspace window. It is divided into three sections: * Modelling (contains the default and user-defined specifications for modelling; and the output from the modelling process) * Seasonal adjustment (contains the default and user-defined specifications for seasonal adjustment and the output from the seasonal adjustment process), * Utilities (calendars and user defined variables). Text The Workspace window 12.4.4.5 Statistical Methods Menu The Statistical methods menu includes functionalities for modelling, analysis and the seasonal adjustment of a time series. They are divided into three groups: * Anomaly Detection – allows for a purely automatic identification of regression effects; * Modelling – enables time series modelling using the TRAMO and RegARIMA models; * Seasonal adjustment – intended for the seasonal adjustment of a time series with the TRAMO-SEATS and X-13ARIMA-SEATS methods. Text The Statistical methods menu. 12.4.5 Anomaly Detection The primary goal of the functionalities that are available in the Anomaly Detection section is the identification of atypical values called outliers. According to the ‘ESS Guidelines on Seasonal Adjustment’ (2015), seasonal adjustment methods are likely to be severely affected by the presence of such values; therefore they should be detected and replaced simultaneously or before estimating the seasonal and calendar components in order to avoid a distorted or biased estimation. The use of the RegARIMA models is recommended by the ‘ESS Guidelines on Seasonal Adjustment’ (2015) to estimate and remove outliers before estimating the seasonal effect. As the presence of outliers could greatly affect the quality of the decomposition, the various types of outlier (i.e. additive outliers, transitory changes, level shifts, etc.) should be detected and corrected for. This element of quality control should be performed each time new or revised data become available. Manual inspection of the data is problematic, especially in the case of large datasets. Also, it usually relies on some simple measures, which do not consider the full information contained in the series, but just a few values. Therefore the results of the manual inspection can be severaly affected by seasonality, noise, or special events. JDemetra+ includes two tools dedicated to the automatic identification of outliers: Check Last and Outliers Detection. Both are based on the TERROR program, which is an application from TRAMO. TERROR is executed by both tools to automatically (with several available options) detect outliers. Text The Anomaly detection menu. 12.4.5.1 Check Last The Check Last tool automatically detects using the TRAMO model forecasts up to three last observations and marks the observations that are too different from their forecasted values. The Check Last Batch window is divided into three panels. The panel on the left presents the list of the analysed series. The results are dispalyed in the panels on the right. To launch the analysis, drag and drop a series from the Providers window into the left hand side panel of the Check Last Batch window and click the Start button (denoted with the green arrow) from the menu in the top part of the window. The analysis will be performed using the TRAMO specification selected from the list. By default, TR4 is used. Text The Check Last initial window with series to be processed and the list of available specifications expanded. JDemetra+ removes the last observations from the series and calculates a one-period-ahead out-of-sample forecast of the series. The forercasted values are then compared with the actual values. The user may decide how many of the last observations will be considered (one, two, or three) in this procedure (click on the 123 button and specify the number). The number of columns visible in the panel on the left will be adjusted accordingly to the user’s choice. Text The options for number of observations to be examined. The default settings can be changed in the Properties dialog box (the number of last observations that will be compared to the forecasted values, specification used for modelling and the threshold values used to decide if observations are abnormal). To open it, click on the button marked with the working tools. Text The properties for the Check Last functionality. Once the process is executed, click on the series on the list to display the results. For each series the program automatically identifies an ARIMA model, detects several types of outliers, interpolates missing values and estimates the calendar effects, if appropriate. Study the detailed results section using the vertical scrollbar. Text The results of the outlier’s detection process. The last observations (one, two or three, depending of the user’s choice) are compared with the forecasted values. If, for a given observation, the forecast error divided by a standard deviation of residuals is greater than the first threshold value and lower than the second threshold value, then this observation is classified as containing a “possible error” and marked in orange. If this value is greater than the second threshold value, then the new observation is classified as containing a “likely error” and marked in red. Otherwise, the observation is accepted as “without an error”56. Text Investigation of the outliers’ detection results. JDemetra+ enables the user to save the results of this analysis in the compact form of a report. To generate it, click on the Generate Report button and specify the sorting options. Text The Generate Report functionality. To save the report click OK and select a destination folder. Text Report from the outliers’ detection process. 12.4.5.2 Outlier detection The Outlier Detection tool allows for the identification of an ARIMA model, including the detection of outliers, interpolation of missing values and the estimation of any calendar effects. A step-by-step demonstration of the Outlier Detection tool capabilities and options can be found here. 12.4.6 Modelling The aim of the Modelling section is to provide tools for time series modelling and forecasting without performing the estimation of its components and decomposition. The estimated results can be useful for time series analysis and in the prediction of short-term developments. The Modelling section includes all capabilities from the TRAMO and RegARIMA models. It is flexible in specifying model parameters. The results can be saved and refreshed with updated series. Instructions on how to use this functionality is given in the step-by-step demonstration on advanced time series analysis. Text The Modelling menu. 12.4.7 Seasonal Adjustment The Seasonal Adjustment section provides tools to perform seasonal adjustment for a single time series as well as for multiple time series using the TRAMO-SEATS or X-13ARIMA-SEATS methods. It also offers several seasonality tests that can be used to scrutinize the presence and the nature of seasonal movements in a time series independently from the seasonal adjustment. Finally, the Direct-Indirect Seasonal Adjustment tool enables a comparison of the results from direct and indirect seasonal adjustment performed on the aggregated series. The guidance for using these functionalities is given in the Case studies section: - basic scenario detailing the use of automatic modelling in seasonal adjustment; - different types of user interventions; - Seasonality Tests; - Direct-Indirect Seasonal Adjustment. Text The Seasonal Adjustment menu. 12.4.8 Importing data : the providers window The Providers window presents the list of the data sources and organises the imported series within each data provider. Text The Providers window The allowed data sources include: * JDBC; * ODBC; * SDMX; * Spreadsheets; * TSW; * TXT; * USCB; * XML. All standard databases (Oracle, SQLServer, DB2, MySQL) are supported by JDemetra+ via JDBC, which is a generic interface to many relational databases. Other providers can be added by users by creating plugins (see Plugins section in the Tools menu). To import data, right-click on the appropriate provider from the Providers panel and specify the required parameters. For all providers the procedure follows the same logic. An example is provided here. The Providers window organises data in a tree structure reflecting the manner in which data are presented in the original source. The picture below presents how JDemetra+ visualises the imported spreadsheet file. If the user expands all the pluses under the spreadsheet all the series within each sheet that has been loaded are visible. Here two time series are visible: Japan (under the Asia branch) and United States (under the North America branch) while the Europe branch is still folded. The names of the time series have been taken from the column headings of the spreadsheet while the names of the branches come from sheets’ names. Text A structure of a dataset Series uploaded to the Providers window can be displayed, modified and tested for seasonality and used in estimation routines (see Modelling and Seasonal adjustment). The data sources can be restored after re-starting the application so that there is no need to get them again. This functionality can be set in the Behaviour tab available at the Option item from the Tools menu. 12.4.8.1 Spreadsheets The Spreadsheets data source corresponds to the series prepared in the Excel file. The file should have true dates in the first column (or in the first row) and titles of the series in the corresponding cell of the first row (or in the first column). The top-left cell \\[A1\\] can include a text or it can be left empty. The empty cells are interpreted by JDemetra+ as missing values and they can appear in the beginning, in the middle and in the end of time series. An example is presented below: Text Example of an Excel spreadsheet that can be imported to JDemetra+ Time series are identified by their names. JDemetra+ derives some information (like data periodicity, starting and ending period) directly from the first column (or from the first row, depending on the chosen data orientation (vertical or horizontal)). 12.4.8.2 Import data To import data from a given data source, click on this data source in the Providers window shown below, choose Open option and specify the import details, such as a path to a data file. These details vary according to data providers. The example below show how to import the data from an Excel file. From the Providers window right-click on the Spreadsheets branch and choose Open option. Text Data provider available by default The Open data source window contains the following options: Spreadsheet file – a path to access the Excel file. Data format – the data format used to read dates and values. It includes three fields: locale (country), date pattern (data format, e.g. yyyy-mm-dd), number pattern (a metaformat of numeric value, e.g. 0.## represents two digit number). Frequency – time series frequency. This can be undefined, yearly, half-yearly, four-monthly, quarterly, bi-monthly, or monthly. When the frequency is set to undefined, JDemetra+ determines the time series frequency by analysing the sequence of dates in the file. Aggregation type – the type of aggregation (over time for each time series in the dataset) for the imported time series. This can be None, Sum, Average, First, Last, Min or Max. The aggregation can be performed only if the frequency parameter is specified. For example, when frequency is set to Quarterly and aggregation type is set to Average, a monthly time series is transformed to quarterly one with values that are equal to the one third of the sum of the monthly values that belong to the corresponding calendar quarter. Clean missing – erases the missing values of the series. Next, in the Source section click the grey “….” button (see below) to open the file. Text Data source window Choose a file and click OK. Text Choice of an Excel spreadsheet The user may specify Data format, Frequency and Aggregation type, however this step is not compulsory. When these options are specified JDemetra+ is able to convert the time series frequency. Otherwise, the functionality that enables the time series frequency to be converted will not be available. Text Options for importing data The data are organized in a tree structure. Text Dataset structure Once the data has been successfully imported, it is available to the user for various analyses (e.g. visualization, modelling, seasonal adjustment, etc.) 12.4.9 Charts 12.4.9.1 Data visualization To display a given series, right click on it and choose the Chart &amp; grid option from the local menu. The graph is displayed in the panel on the right. Text Time series graph Using the local menu that is available for the chart the user may adjust the view of the picture, save it and/or save it in a given location. Text Local menu basic options for the time series graph Once the time series is marked by clicking on it with the left mouse button, more sophisticated options are available, in addition to the standard ones shown above. Text Full local menu options These additional options include the Open with option, which opens time series in a separate window according to the user choice (chart &amp; grid or only chart). All ts views option is not available at the moment. The picture below shows the view displayed once Chart &amp; grid option was chosen. By clicking on the marked buttons the user can switch between chart and grid view. For both views (chart and grid) the local menu is available. Text Chart &amp; grid view Rename option (see below) enables the user to change the time series name. Text Renaming a time series The option Split into the yearly components opens an additional window that presents the analysed series data split by year. This chart is useful to investigate the differences in time series values caused by the seasonal factors. The graph gives some idea about the existence and size of deterministic and stochastic seasonality in data. Text Split into the yearly components option’s result To display more than one series on the graph, select Tools → Container → Chart from the main menu. Next, drag a drop series to the Chart window. Text Chart window 12.4.9.2 Charts for spectral Analysis Text Auto-regressive spectrum’s properties The spectral graphs are available from: Tools → Spectral analysis. Text Tools for spectral analysis When the first option is chosen JDemetra+ displays an empty Auto-regressive spectrum window. To start an analysis drag a single time series from the Providers window and drop it into the Drop data here area. Text Launching an auto-regressive spectrum An auto-regressive spectrum graph available in JDemetra+ is based on the relevant tool from the X-13ARIMA-SEATS program. It shows the spectral density (spectrum) function, which reformulates the content of the stationary time series’ autocovariances in terms of amplitudes at frequencies of half a cycle per month or less. The number of observations, data transformations and other options such as the specification of the frequency grid and the order of the autoregressive polynomial (30 by default) can be specified by opening the Window → Properties from the main menu. The Auto-regressive - Properties window contains the following options: Log - a log transformation of a time series; Differencing - transforms a data by calculating a regular (order 1,2..) or seasonal (order 4, 12, depending on the time series frequency) differences; Differencing lag - the number of lags that the program will use to take differences. For example, if Differencing lag = 3 then the differencing filter does not apply to the first lag (default) but to the third lag. Last years - a number of years at the end of the time series taken to produce autoregresive spectrum. By default, it is 0, which means that the whole time series is considered. Auto-regressive polynomial order - the number of lags in the AR model that is used to estimate the spectral density. By default, the order of the autoregressive polynomial is set to 30 lags. Resolution - the value 1 plots the spectral density estimate for the frequencies \\(\\omega_{j} = \\frac{2\\pi j}{n}\\), where \\(n \\in ( - \\pi;\\pi)\\) is the size of the sample used to estimate the AR model. Increasing this value, which is set to 5 by default, will increase the precision of this grid. The seasonality test described above uses an empirical criterion to check whether the series has a seasonal component that is predictable (stable) enough that it can be estimated with reasonable success. The peak in the auto-regressive spectrum has to be greater than the median of the 61 spectrum ordinates and has to exceed the two adjacent spectral values by more than a critical value. When such a case is detected, the test results are displayed in green. Text An example of an-auto-regressive spectrum The second spectral graph is a periodogram. To perform the analysis of a single time series using this tool, choose Tools →Spectral analysis → Periodogram and drag and drop a series from the Providers window to the empty Periodogram window. Text Launching a periodogram The sample size and data transformations can be specified by opening the Window → Properties, in the main menu. The Periodogram - Properties window contains the following options: Log - a log transformation of a time series; Differencing - transforms the data by calculating regular (order 1,2..) or seasonal (order 4, 12, depending on the time series frequency) differences; Differencing lag - the number of lags that you will use to take differences. For example, if Differencing lag = 3 then the differencing filter does not apply to the first lag (default) but to the third lag. Last years - the number of years at the end of the time series taken to produce periodogram. By default it is 0, which means that the whole time series is considered. Text Periodogram’s properties The periodogram was one of the earliest tools used for the analysis of time series in the frequency domain. It enables the user to identify the dominant periods (or frequencies) of a time series. In general, the periodogram is a wildly fluctuating estimate of the spectrum with a high variance and is less stable than an auto-regressive spectrum. Text An example of a periodogram The third spectral graph is the Tukey spectrum. To perform the analysis of time series using this tool, choose Tools → Spectral analysis → Tukey spectrum and drag and drop a single series from the Providers window to the empty Periodogram window. Text Launching a Tukey spectrum The Tukey spectrum estimates the spectral density by smoothing the periodogram. Text An example of a Tukey spectrum The options for the Tuckey window can be specified by opening the Window → Properties from the main menu. The Periodogram - Properties window contains the following options: Log - a log transformation of a time series. Differencing - transforms the data by calculating regular (order 1, 2..) or seasonal (order 4, 12, depending on the time series frequency) differences. Differencing lag - the number of lags that you will use to take differences. For example, if Differencing lag = 3 then the differencing filter does not apply to the first lag (default) but to the third lag. Taper part – parameter larger than 0 and smaller or equal to one that shapes the curvature of the smoothing function that is applied to the auto-covariance function. Window length – the size of the window that is used to smooth the auto-covariance function. A value of zero includes the whole series. Window type – it refers to the weighting scheme that it is used to smooth the auto-covariance function. The available windows types (Square, Welch, Tukey, Barlett, Hamming, Parzen) are suitable to estimate the spectral density. Text Tukey spectrum’s properties 12.4.10 Workspace Window : set up your work environnement 12.4.10.1 Customize specifications eg the parameters the algo will run on, here they are globally defined for your whole data set and you will be able to fine tune them series by series 12.4.11 12.5 Seasonal adjustment Table : list of steps with links (obj : replace a lot of text with tables) 12.5.1 Run a fully automatic SA processing List of steps * import data * choose a pre-defined specification (link to list) * create a multi document * run SA * generate output + from the GUI + link to cruncher () In the following parts you can learn how to customize you processing doncument : one series vs muti document check output if document : no output check with v3 12.5.1.1 Quick demonstration For a detailed demonstration of the seasonal adjustment process, please refer to the JDemetra+ Tutorial video developed by Andreea Mirica (Statistics Romania). For a step-by-step illustration of the key stages for seasonal adjustment, refer to the guidance below. The default view of the JDemetra+ window, which is displayed after launching the program, is shown below. Text JDemetra+ default window By default, on the left hand side of the window two panels are visible: the Workspace panel and the Providers panel. The Workspace panel stores the work performed by the user in a coherent and structured way. The Providers panel presents the list of the data sources and organizes the imported series within each data provider. 12.5.1.1.1 Importing the series Once the spreadsheet is prepared as it is shown here and saved, it can be imported to JDemetra+ as it is shown in the video below. Text Importing the series The data are organized in a tree structure. If you expand all the plus-signs under the spreadsheet you will see all the series within each sheet that has been loaded. Here all time series are visible under the Production in construction branch. The names of time series have been taken from the columns’ headings of the spreadsheet while the names of the branches come from sheets’ names. 12.6 An automatic seasonal adjustment To execute a seasonal adjustment process of given time series dataset go to the main menu and follow the path: Statistical methods → Seasonal adjustment → Multi Processing → New. This command opens an empty SAProcessing window. Next, drag and drop series in the SAProcessing window and launch the seasonal adjustment process. Explore the results and save them. The gif below shows the details of the actions performed in an automatic seasonal adjustment process. Text An example of an automatic seasonal adjustment process 12.6.0.0.1 Manual interventions The specifications used for seasonal adjustment of an individual series can be changed as shown in the presentation below. Be aware that the changes introduced in a given settings’ section may lead to changes in the output for other parts of the results. The options for the pre-processing of the series are explained under modelling specifications and the options for the pure seasonal adjustment process are explained under seasonal adjustment specifications. Text An example of manual interventions to the seasonal adjustment process 12.6.0.0.2 Output options To export the results of seasonal adjustment for the whole dataset, select the Output item from the SAProcessing menu. Text The SAProcessing menu The procedure to extract the output is explained here in detail. The results can be exported to CSV, CVS matrix, TXT and XLS formats and the list of available items depends on the format choice. The CVS, TXT and XLS formats are used to save the results in a form of the time series. The CVS matrix is a format for saving various diagnostics and statistics. The list of available items is available here. 12.6.1 Simple seasonal adjustment of a single time series This scenario guides the user through all the steps of the process required to seasonally adjust a single time series. Links to the appropriate parts of the Reference Manual for detailed explanations are provided when necessary. Go to the main menu and follow the path: Statistical methods → Seasonal adjustment → Single analysis. Select a seasonal adjustment method (TramoSeats (i.e. the TRAMO-SEATS method will be used) or X13 (i.e. the X13ARIMA-SEATS will be used). Text Launching seasonal adjustment for a single time series An empty panel will be opened. The picture below shows the view when the X-13ARIMA-SEATS method is chosen. Text Single analysis window Select a data provider and unfold an already imported dataset. Drag and drop one time series from the Providers window to the Drop data here box as shown below. The window contains two panels. The one on the left presents the structure of the output in the form of an output tree. The other one is empty. Once the seasonal adjustment has been performed, this panel will show detailed results for any item chosen by the user from the output tree. Text Starting a seasonal adjustment process When the user drops a series into the document window (X13Doc in the example presented in this scenario) JDemetra+ starts the seasonal adjustment process automatically. By default, a summary of results is displayed. It is accompanied by two graphs: an overlay graph on the left with the original unadjusted series, the seasonally-adjusted series and the trend-cycle and the SI ratio graph on the right. The diagnostics and graphs for the modelling part are discussed here. The user can find also find some explanations of the results of seasonal adjustment for X-13ARIMA-SEATS and for TRAMO-SEATS). The Main results panel provides a quick summary of the quality of the adjustment. Study the diagnostics section using the vertical scrollbar. Text Simple seasonal adjustment, single time series: main results panel The results are highlighted in green, yellow or red, depending on the result of the statistical test used. Green indicates that problematic characteristics have not been detected (e.g. lack of normality of residuals, autocorrelation in residuals). Yellow indicates that the test outcome is uncertain. Those in red indicate that there are issues that should be addressed. The user is expected to investigate the problematic test statistics and try to improve the model, so that no uncertain or rejected test results are present. Text Diagnostic results - simple seasonal adjustment of single time series To explore the results, expand the tree in the left panel and click on the desired node. Here Out-of-sample test was chosen. Text Exploring the results The default specification used for a seasonal adjustment can be modified by clicking on the Specifications button. Text Changing a default specification The Specifications panel presents settings that have been used to generate the current output. Text Specification panel To change a given setting, click on it and choose an option from the list and/or enter a value. In the picture below the series span was shortened by the first 12 observations. Inputs in green indicate that the entered values are acceptable (appropriate format, data within the allowed range and so on). To apply the changes click on the Apply button in the bottom part of the window. Text Modifying settings Be aware that the changes introduced may lead to changes in the output for other parts of the results. The example below illustrates that omitting the first 12 observations results in automatic detection of trading day effects and the Easter effect, which were not present in the previous model. Text The effect of applying modified settings To copy the estimated series (seasonally adjusted, trend, seasonal and irregular) to another file go to the Table item in the Main results section of the output tree. Then click on the upper-left cell in the table. Text Highlighting the output series Copy the series by clicking the Copy item from the context menu or use the standard Ctrl+C keys. Other options from this menu are explained here. Text Copying the series Paste the series to the destination file (e.g. TXT, Excel). Text Easy exporting data to an Excel file To save the document created in JDemetra+ (named X13Doc-1 in our example) select Save Workspace As… item from the File menu. Text Saving a workspace Enter the location, workspace name and click Save. Text Choosing a destination folder The document is visible in the Workspace window under the appropriate section (x13 in the case presented in the picture). The document can be opened, deleted or renamed from the context menu. Text The content of the context menu for a single document The user can also add comments to the document. To display the comments and modify them, click on Edit comments from the context menu. Text An example of an Edit comments window At the end of this process, the user is presented with the results already saved in the workspace. The option Refresh data becomes active when the given workspace is opened again. This option can be activated either from a local menu or from the main menu. Once it is activated, JDemetra+ refers to the data source defined in the workspace and uses this current version of data to perform seasonal adjustment with the settings saved in the document. Text Refreshing the data 12.6.2 Specifications, documents, multi-documents Text **The Workspace window with the nodes for the seasonal adjustment procedure marked Seasonal adjustment can be performed with pre-defined or user-defined specifications. In general, these specifications include also a modelling part, which is descibed under the Modelling section. Specifications are sets of parameters and values assigned to them that contain all information necessary for seasonal adjustment. The specifications for seasonal adjustment can be used for processing of a single time series as well as a large dataset. The default critical values used by the tests included in the specifications can be changed by the user in the Tools \\(\\) Options menu. 12.6.2.1 Pre-defined specifications The Seasonal adjustment section of the Workspace window contains a set of pre-defined specifications that enables the user to seasonally adjust the time series using two methods: TRAMO-SEATS and X-13ARIMA-SEATS. Text A set of pre-defined seasonal adjustment specifications The set of pre-defined specifications for seasonal adjustment encompasses the most commonly used sets of seasonal adjustment parameters. The names of these pre-defined specifications correspond to the terminology used in TSW+. The users are strongly recommended to start their analysis with one of those specifications (usually RSA4c or RSA5c for X-13ARIMA-SEATS and RSA4, RSA5 or RSAfull for TRAMO-SEATS). The user is expected to adjust the specification to the specific needs the using the Specification button. The default specification for TRAMO-SEATS and multi-documents is RSAfull, while for X-13ARIMA-SEATS it is RSA4c. Pre-defined seasonal adjustment specifications Speci ficat ion T ransf ormat ion *Pre- adjus tment for l eap-y ear** Wo rking d ays Tr ading d ays E aster eff ect Outli ers ARIMA mo del RSA0 no no no no no no (0,1 ,1)(0 ,1,1) RSA1 test no no no no test (0,1 ,1)(0 ,1,1) RSA2 test no test no test test (0,1 ,1)(0 ,1,1) RSA3 test no no no no test AMI RSA4 test no test no test test AMI RSA5 test no no yes test (Stan dard) test AMI RS Afull test yes no test test (In clude Ea ster) test AMI X11 no no no no no no (0,1 ,1)(0 ,1,1) RSA1 test no no no no test (0,1 ,1)(0 ,1,1) RSA2c test test test no test test (0,1 ,1)(0 ,1,1) RSA3 test no no no no test AMI RSA4c test test test no test test AMI RSA5 test test no test test test AMI Explanations for settings: Transformation test – a test is performed to choose between an additive decomposition (no transformation) and a multiplicative decomposition (logarithmic transformation). Pre-adjustment for leap-year – a correction of the February values, which is applied to the original series before the logarithmic transformation. The original values in February are multiplied by \\(\\) for leap years and by \\(\\) for non-leap years. Values for other months are not modified. Working days – a pre-test is made for a presence of a working day effect. Trading days – a pre-test is made for a presence of a trading day effect. Easter – a pre-test for a presence of the Easter effect. The default length of the Easter effect is 6 days (for TRAMO-SEATS specifications) and 8 days (for X-13ARIMA-SEATS specifications). Outliers – an automatic identification of three types of outliers: AO (additive outliers), LS (level shifts) and TC (transitory changes), using a default critical value. ARIMA model – the choice between fixing the ARIMA model structure to (0,1,1)(0,1,1) or searching for the ARIMA model using an automatic model identification procedure (AMI). The (0,1,1)(0,1,1) model (called the Airline model) is used as a default model in several TRAMO-SEATS and X-13ARIMA-SEATS specifications because it has been shown in many studies that this model is appropriate for many real seasonal monthly or a quarterly time series. Moreover, the Airline model approximates well many other models and provides an excellent “benchmark” model57. 12.6.2.2 User-defined specifications The user may add new seasonal adjustment specifications to the Workspace window. To do it, go to the Seasonal adjustment section, right click on the tramoseats or x13 item in the specifications node and select New from the local menu. Text Creating a new specification in the Seasonal adjustment section Next, double click on the newly created specification, change the settings accordingly and confirm with the OK button. Text Changing settings of seasonal adjustment specification 12.6.3 Defining and modifying a specification In general, the user can choose the parameters of the seasonal adjustment process by creating a specification with given settings or by changing some settings in the specification currently in use. To create a new specification go to the Workspace window and choose a node, to which you wish to add a specification (Modelling or Seasonal adjustment). Then choose a method (tramo or regarima for Modelling, tramoseats or x13 for Seasonal adjustment). Click on the left mouse button and choose a New option. The user can also import the specification from the external file with the Import from option. Text Creating a new specification Next, unfold the node (the tramoseats node in the case presented here) and right click on the newly created specification (TramoSeatsSpec-1 in the case presented below) to open the local menu. The local menu offers the following options: Open – displays the specification’s settings Export to – enables the user to save the specification in a config file. Delete – removes the specification from the workspace. Rename – enables the user to change the name of the user-defined specification. Edit comments – a functionality for monitoring how a seasonal adjustment process is implemented. The user can add and modify short notes concerning a given time series. These notes are visible in the Comments column in the Processing window. The notes are displayed when the user hovers the mouse on the given cell. Create document – adds a new document to the relevant place in the Seasonal adjustment → documents section and assigns the specification selected by the user to it. Clone – creates the copy of the specification and adds it to the list. Chose Open from the menu. Text Opening a new specification The Specification window is divided into several sections. The actual content depends on the choice made by the user in step 1 of this scenario. To introduce changes unfold the sections, modify the current settings (choose from the list or insert the value by hand) and confirm the changes with the OK button. Text Modifying a new specification User-defined specifications are usually used for seasonal adjustment of many time series (Statistical methods → Seasonal adjustment → Multi Processing → New). The user can also make changes to the specification after the modelling/seasonal adjustment process. In such a case, to introduce changes click on the Specification button. JDemetra+ opens the Specifications panel on the right. Unfold the sections, modify the current settings (choose from the list or type a new value) and confirm the changes with the Apply button. JDemetra+ automatically applies the new settings and displays the outcome resulting from the modified specification. Text Modifying a specification, which is currently in use 12.6.4 Tramo-seats specification setting This section discusses the options available for the TRAMO-SEATS specifications, which are based on the original program developed by Agustin Maravall and Victor Gómez. It is divided into five parts that correspond to the TRAMO-SEATS specification sections and are presented in the order in which they are displayed in the graphical interface of JDemetra+. Text A list of the TRAMO-SEATS specification’s sections To avoid unnecessary repetitions, click on the respective link for a description of the following sections of the TRAMO/SEATS specification: Estimate; Transformation; Regression; Outliers; Arima. This section focuses on the decomposition part of the seasonal adjustment process and the options for benchmarking. To facilitate the comparison between JDemetra+ specifications and specifications used in TSW+, under each option the name of the corresponding parameter from the original software is given, if any. For an exact description of the different parameters originating from TSW+, the user should refer to the documentation of TSW+. For each parameter the default parameter value, which is displayed for a template created in the Workspace window, is reported. For the pre-defined specifications the items are fixed, while in the case of the user-defined specifications the user can set them individually. However, in some cases the choice of a given value results in a limitation of the possible alternatives for other parameters. Therefore, the user is not entirely free to set the parameters values. 12.6.4.1 Series In the context of seasonal adjustment it is usually assumed that long time series are those exceeding twenty years of length. Performing seasonal adjustment of long time series can be difficult. Over such a long period the underlying data generating process can change, determining changes also in the components and in the components structure. In this case the adjustment over the whole series may produce sub-optimal results mainly in the most recent period and in the initial parts of the series. Therefore it is reasonable to limit long time series to the most recent observations. The Series section allows the user to limit the span (data interval) of the data to be modelled or seasonally adjusted. 12.6.4.1.0.1 Series span → type –; – Specifies the span (data interval) of the time series to be used in the seasonal adjustment process. When the user limits the original time series to a given span, only this span will be used in the computations. The available parameters for this option are: * All – full time series span is considered in the seasonal adjustment; * From – date of the first time series observation is included in the seasonal adjustment; * To – date of the last time series observation is included in the seasonal adjustment; * Between – dates of the first and the last time series observations are included in the seasonal adjustment; * Last – a specific number of observations from the end of the time series is included in the seasonal adjustment; * First – a specific number of observations from the beginning of the time series is included in the seasonal adjustment; * Excluding – a specific number of observations is excluded from the beginning (First) and/or end (Last) of the time series in the seasonal adjustment. With the options Last, First and Excluding the span can be computed dynamically on the series. The default setting is All. 12.6.4.1.0.2 Series span → Preliminary check –; – When marked, it checks the quality of the input series and excludes from a processing the highly problematic ones: e.g. these with a high number of outliers, identical observations and/or missing values above the respective threshold values. When unmarked, the thresholds are ignored and process is performed, when possible. By default, the checkbox is marked. 12.6.4.2 Seats This section includes the settings relevant for the decomposition step, performed by the SEATS algorithm. 12.6.4.2.0.1 Prediction length Seats parameters; npred Number of forecasts used in the decomposition. Positive values correspond to the number of months while negative values correspond to the numbers of years. 12.6.4.2.0.2 Approximation mode Seats parameters; noadmiss In general, SEATS decomposes the ARIMA model received from TRAMO. On some occasions, the ARIMA model identified by TRAMO results in a so called non-admissible decomposition i.e. a decomposition for which the condition that all components have a non-negative spectrum for all frequencies has failed. In such cases an approximation might be used to choose an acceptable ARIMA model. The available actions that can be performed in the case of a non-admissible decomposition are: None – when the model does not accept an admissible decomposition, no approximation is made, which means that no decomposition is performed by SEATS. Legacy – when the model does not accept an admissible decomposition, it is automatically replaced with a decomposable one. The forecasts of the components obtained from SEATS with a new ARIMA model (sum of the components forecasts) will not add to the series forecast of the model passed by TRAMO. Noisy – a new ARIMA model is obtained by adding a white noise to the non-admissible model estimated by TRAMO. In this case, the forecasts of the series from TRAMO and from SEATS are the same; also the sum of the components forecasts is the same as the forecast of the series with the TRAMO model. The default setting is Legacy. 12.6.4.2.0.3 MA unit root boundary Seats parameters; xl A parameter to control the estimation of the AR and MA roots of the model. When the modulus of a root converges within an interval around 1, the program automatically fixes the root. More specifically, when the modulus of an estimated root falls in the range (xl, 1), it is set to 1 if it is a root in the AR polynomial. If a root is in the MA polynomial, it is set to xl. The default value is 0.95. 12.6.4.2.0.4 Trend boundary Seats parameters; rmod The trend boundary is defined for the modulus of the inverse of the real AR roots. If the modulus of the inverse of the real root is greater than the Trend boundary, the AR root is integrated into the trend component. Otherwise the root is integrated into the seasonal component or transitory component (see Seasonal boundary). The default parameter value is 0.5. 12.6.4.2.0.5 Seasonal tolerance Seats parameters; epsphi The tolerance (measured in degrees) to allocate the AR non-real roots to the seasonal component (if the modulus of the inverse complex AR root is greater than the Trend boundary and the frequency of this root differs from one of the seasonal frequencies by less than Seasonal tolerance) or the transitory component (otherwise). The default parameter value is \\(\\frac{\\pi}{90}\\) rad (2 degrees). 12.6.4.2.0.6 Seasonal boundary Seats parameters; smod The seasonal boundary is defined for the modulus of the inverse of the real negative AR roots. If the modulus of the inverse negative real root is greater (or equal) than Seasonal boundary, the AR root is integrated into the seasonal component. Otherwise the root is integrated into the trend or transitory component (see Trend boundary). The default parameter value is 0.8. 12.6.4.2.0.7 Seasonal boundary (unique) Seats parameters; smod A boundary, from which a negative AR root is integrated in the seasonal component when the root is the unique seasonal root. The default parameter value is 0.8. 12.6.4.2.0.8 Method –; – The estimation method of the unobserved components. Options: - Burman – the algorithm, which is used by the original TRAMO/SEATS method. Although it is the most efficient one, it cannot handle MA unit roots and it may become numerically unstable when some roots of the MA polynomial are near 1. In such cases the Wiener-Kolmogorov approach may lead to a significant underestimation of the standard deviations of the components. - KalmanSmoother – the most robust algorithm. It is not disturbed by (quasi-) unit roots in MA. It is slightly slower than the Burman’s algorithm58. It should also be noted that it provides exact measures of the standard errors of the estimates (identical to the McElroy’s results). - McElroyMatrix – the algorithm, which is much slower than the two other options and presents the same stability issues as the Burman’s algorithm. However, it provides additional results (full covariance matrix of the estimates) that may be useful. The default setting is Burman. 12.6.5 Reg arima linearization mixed old page * in chap 4 on SA : infos on algos * here cut out infos on gui only The Model node includes basic information about the outcome of the model identification procedure and checking the goodness of fit. The summary information about the final model is available directly from the main Model node. The content of this panel depends on the settings applied to the modelling procedure. Text The Model node in the navigation tree The first part contains fundamental information about the model. Text The Summary section of the Model node Estimation span informs about the first and the last observation used for modelling. The notation of the estimation span varies according to the frequencies (for example, the span \\[2-1993 : 10-2006\\] represents a monthly time series and the span \\[II-1994 : I-2011\\] represents a quarterly time series). The message Series has been log-transformed is only displayed if a logarithmic transformation has been applied. In the case of the pre-defined specifications: TR0, TR1, TR3, RG0, RG1 and RG3 no trading day effect is estimated. For TR2, RG2c, TR4 and RG4c pre-defined specifications, working day effects and the leap year effect are pre-tested and estimated if present. If the working day effect is significant, the pre-processing part includes the message Working days effect (1 regressor). The message Working days effect (2 regressors) means that the leap year effect has also been estimated. For TR5 and RG5c the trading day effect and the leap year effect are pre-tested. If the trading day effect has been detected, either of the messages Trading days effect (6 regressors) or Trading days effect (7 regressors) are displayed, depending whether the leap year effect has been detected or not. If the Easter effect is statistically significant, Easter effect detected is displayed. In this section the total number of detected outliers is displayed. The additional information on detected outliers, i.e. type, location and coefficients’ values, can be found in the Arima model subsection of the Model node. The Final model section informs about the outcome of the estimation process. Number of effective observations is the number of observations used to estimate the model, i.e. the number of observations of the transformed series (regularly and/or seasonally differenced) reduced by the Number of estimated parameters, which is the sum of regular and seasonal parameters for both autoregressive and moving average processes, mean effect, trading/working day effect, outliers, regressors and one. Likelihood is a maximized value of a Likelihood59 function after the iterations processed in Exact Maximum Likelihood Estimation, which is a method used to estimate the model. This value is used by the model selection criteria: AIC, AICC, BIC (corrected by length) and Hannan-Quinn60. Standard error of the regression (ML estimate) is the standard error of the regression from Maximum Likelihood Estimation61. The scores at the solution section presents the gradient of the loglikelihood. The different items of the scores are related to the different parameters of the ARIMA model. The output indicates to which extent the optimization procedure reached the maximum. At the maximum of the likelihood, it should be 0. However, it is never exactly the case, due to numerical approximations. Usually, the scores can be improved by using a higher precision (smaller tolerance). This precision is controlled by the Tolerance parameter in the Estimate section of the Specifications window (see how to use this parameter for Tramo and for Arima). An example of the output is presented in the chart below. Text The content of the Final model section Next, the estimated values of model parameters (Coefficients), t-statistics (T-Stat) and corresponding p-values (P\\[\\|T\\|\\&gt;t\\]) are displayed. JDemetra+ uses the following notation: Phi(p) – the \\(p^{}\\) term in the non-seasonal autoregressive polynomial; Theta(q) – the \\(q^{\\text{th}}\\) term in the non-seasonal moving average polynomial; BPhi(P) – the \\(P^{\\text{th}}\\) term in the seasonal autoregressive polynomial; BTheta(Q) – the \\(Q^{\\text{th}}\\) term in the seasonal moving average polynomial. In the example below, the ARIMA model (0,1,1)(0,1,1) was chosen, which means that one regular and one seasonal moving average parameter were identified and estimated. The p-values indicate that BTheta(1) parameter is significant in contrast to the Theta(1), which is not significant 62. Text The estimation’s results of the ARIMA model For the fixed ARIMA parameters, JDemetra+ shows only the values of the parameters. Figure below presents the output from the manually chosen ARIMA model (2,0,0)(0,1,1) with a fixed parameter BTheta(1). For the fixed parameter the T-Stat and (P|T|&gt;t) are not displayed as no estimation is done for this parameter. Text The results of the estimation of the ARIMA model with a fixed coefficient If the ARIMA model contains a constant term (detected automatically or introduced by the user), the estimated value and related statistics are reported. Text The results of the estimation of a mean effect JDemetra+ presents estimated values of the coefficients of one or six regressors depending on the type of a calendar effect specification. For a working days effect one regressor is estimated. Text The results of the estimation of a working day effect When a trading days effect is estimated the Joint F-test value is reported under the table that presents estimated values. When the result of Joint F-test indicates that the trading day variables are jointly not significant the test result is displayed in red. Text The results of the estimation of a trading day effect: the case of jointly not significant variables In the example below the RSA5c specification has been used and a trading day effect has been detected. In spite of the fact that some trading day regressors are not significant at the 5% significance level, the outcome of the joint F-test indicates that the trading day regressors are jointly significant (the F-test statistic is lower than 5%). Text The results of the estimation of a trading day effect: the case of jointly significant variables If a leap year regressor has been used in the model specification, the value of the estimated leap year coefficient is also reported with the corresponding t-statistics and p-value. As the p-value presented on the picture below is greater than 0.05, it indicates that the leap year effect is not significant. Text The results of the estimation of a leap year effect When the option UserDefined is used, JDemetra+ displays the User-defined calendar variables section with variables and corresponding estimation results (the values of the parameters, corresponding t-statistics and p-values). The outcome of the joint F-test is displayed when more than one user-defined calendar variable is used. Text The results of the estimation of user-defined calendar variables When the Easter effect is estimated, the following table is displayed in the output. In the case presented below Easter has a negative, significant effect on the time series. Text The results of the estimation of the Easter effect JDemetra+ also presents the results of an outlier detection procedure. The table includes the type of outlier, its date, the value of the coefficient and corresponding t-statistics and p-values. Text The results of the outlier identification procedure In all pre-defined specifications, except for TR0 and RG0, only additive outliers, temporary changes and level shifts are considered in the automatic outlier identification procedure. When seasonal outliers are also enabled, they appear in the same table as other outliers. Text The results of the outlier identification procedure that enables seasonal outliers Results for pre-specified outliers are displayed in a separate table. Text The results of the estimation of the pre-specified outliers Regression variables, like ramps and intervention variables, are not identified automatically. They need to be defined by the user. The results of an estimation of ramps that are pre-defined types of regression variables are displayed in a separate table. All information concerning ramps, including spans, estimated coefficients and related statistics, is shown in a separate table. Text The results of the estimation of the ramp effect All other intervention variables with corresponding statistics are shown under the Intervention variable(s) table. Text The results of the estimation of the intervention variable User-defined variables are marked as Vars-1.x_1, Vars-1.x_2, …, Vars-1.x_n and displayed in the separate tables. Text The results of the estimation of the user-defined variables JDemetra+ also reports a list of missing observations, if any. JDemetra+ applies the AO approach to the estimation of the missing observations63. Text The results of the estimation of the missing observations Detailed results are divided into several sections and are investigated in the following sections: - Forecasts - Regressors - ARIMA - Pre-adjustment series - Residuals - Likelihood 12.6.5.1 Theoretical spectrum of the Arima model The Arima section demonstrates a theoretical spectrum of the stationary and non-stationary models. The local menu, which is available for the graph, offers the copy and export options, including sending the graph to the printer and saving the graph as clipboard or as a file in the PNG format. The Copy all visible option enables the user to export time series data to the another application. Text 12.6.6 X-11 Specification setting 12.6.6.1 Customised seasonal filters JDemetra+ offers the options to assign a different seasonal filter length to each period (month or quarter). The program offers these options in the single spec mode as well as in the multispec mode, albeit they are available only in the Specifications window, after a document is created. Go to the X11-part of the Specification window (see step 8 from the Simple seasonal adjustment of a single time series scenario). By default a single pre-defined filter length (Msr) is used for all months or quarters. Text Default seasonal filter options To change the seasonal filter for all months/quarters, use a drop down menu, which is displayed after clicking on the cell next to the Seasonal filter option. Text Choosing a seasonal filter for all periods To change the filter length for a single month, click on the empty cell next to Details on seasonal filters. A new window appears in which the filter lengths for each month is given. Click on the cell next to the month in which the filter length is to be changed. Again a drop down menu appears where the filter length can be selected. Once the changes are introduced close this window. Text Seasonal filters’ choice To apply new settings click the Apply button. Text Launching a seasonal adjustment process with modified set of seasonal filters 12.7 Adding User-defined variables User defined variables are simply time series used as explanatory regressors in the RegARIMA and the TRAMO models. Although JDemetra+ allows the user to indicate any time series as a variable to avoid misleading or erroneous results, the following rules should be kept: * User-defined regression variables are used for measuring abnormalities and therefore they should not contain a seasonal pattern. * JDemetra+ assumes that user-defined regressors are already in an appropriately centred form. Therefore the mean of each user-defined regressor needs to be subtracted from the regressor or means for each calendar period (month or quarter) need to be subtracted from each of the user-defined regressors. JDemetra+ considers two kinds of user-defined regression variables: * Static variables, usually imported directly from external software (by drag/drop or copy/paste). The observations for static variables cannot be changed. The only way to update static series is to remove them from the list and to re-import them with the same names. * Dynamic variables that are imported into the Variables panel by dragging and dropping series from a browser of the application, available in the Providers window. Dynamic variables are automatically updated each time the application is re-opened. Therefore, it is a convenient solution for creating user-defined variables. To create a dynamic variable first right-click on the Variables node in the Workspace window and chose the option New. Text Creating an empty dataset for the user-defined variables Next, double click on the newly created Vars-1 item to display it in the Results panel. By default, JDemetra+ uses the conventions Vars_#number to name the tabs under the Variables node. Text Activation of an empty dataset for the user-defined variables Then, go to Providers window and open your file that contains external variables following the instructions provided here. Drag and drop your external regressors from the Providers window to the Vars-1 window. Text Importing the user-defined variables to JDemetra+ The original name of the series is recorded in the Description column of the Variables window. Text Assigning regressors from the Providers window to the user-defined variables In order to rename the series in the Variables window, right click on the series and chose Rename. Text A local menu for the user-defined variables Once the variables are imported they can be used for further analysis (e.g. as regressors in the specifications for RegArima and TRAMO). 12.8 Seasonality tests without adjusting (can be done in or out of SA) Statistical Methods &gt;&gt; Seasonal Adjustment &gt;&gt; Tools &gt;&gt; Seasonality Tests 12.8.0.1 Graphical Test based on AR spectrum The test can be applied directly to any series by selecting the option Statistical Methods &gt;&gt; Seasonal Adjustment &gt;&gt; Tools &gt;&gt; Seasonality Tests. This is an example of how results are displayed for the case of a monthly series: 12.9 Series Modelling why use this out of SA ? 12.10 Outlier detection (TERROR) 12.11 Benchmarking in fact inside sa (which has to be run!) The Benchmarking section allows for a benchmarking, i.e. forcing the annual sums of the seasonally adjusted data to be equal to the annual sums of the raw or calendar adjusted data. The Benchmark algorithm is not run stand-alone but inside an SA-Processig, using a multi document. With the pre-defined specifcations the benchmarking functionality is not applied by default following the ESS Guidelines on Seasonal Adjustment (2015) recommendations. It means that once the user has seasonally adjustd the series with a pre-defined specifcation the Benchmarking node is empty. To execute benchmarking click on the Specifications button and activate the checkbox in the Benchmarking section. Text Benchmarking option – a default view Three parameters can be set here. Target specifies the target variable for the benchmarking procedure. It can be either the Original (the raw time series) or the Calendar Adjusted (the time series adjusted for calendar effects). Rho is a value of the AR(1) parameter (set between 0 and 1). By default it is set to 1. Finally, Lambda is a parameter that relates to the weights in the regression equation. It is typically equal to 0 (for an additive decomposition), 0.5 (for a proportional decomposition) or 1 (for a multiplicative decomposition). The default value is 1. To launch the benchmarking procedure click on the Apply button. The results are displayed in four panels. The top-left one compares the original output from the seasonal adjustment procedure with the result from applying a benchmarking to the seasonal adjustment. The bottom-left panel highlights the differences between these two results. The outcomes are also presented in a table in the top-right panel. The relevant statistics concerning relative differences are presented in the bottom-right panel. Text The results of the benchmarking procedure Both pictures and the table can be copied the usual way (see the Simple seasonal adjustment of a single time series scenario). Text Options for benchmarking results To export the result of the benchmarking procedure (benchmarking.result) and the target data (benchmarking.target) one needs to once execute the seasonal adjustment with benchmarking using the muli-processing option (see the Simple seasonal adjustment of multiple time series scenario. Once the muli-processing is executed, select the Output item from the SAProcessing menu. Text The SAProcessing menu Expand the “+” menu and choose an appropriate data format (here Excel has been chosen). It is possible to save the results in TXT, XLS, CSV, and CSV matrix formats. Note that the available content of the output depends on the output type. Text Exporting data to an Excel file Chose the output items that refer to the results from the benchmarking procedure, move them to the window on the right and click OK. Text Exporting the results of the benchmarking procedure 12.11.1 Specification setting 12.11.1.0.0.1 Is enabled –; – Enables the user to perform a benchmarking. By default, the checkbox is unmarked. 12.11.1.0.0.2 Target –; – Specifies the target variable for the benchmarking procedure. Original – the raw time series are considered as a target data; Calendar Adjusted – the time series adjusted for calendar effects are considered as a target data. The default setting is Original. 12.11.1.0.0.3 Use forecast –; – The forecasts of the seasonally adjusted series and of the target variable (Target) are used in the benchmarking computation so the benchmarking constraint is applied also to the forecasting period. By default, the checkbox in unmarked (forecasts are not used). 12.11.1.0.0.4 Rho –; – The value of the AR(1) parameter (set between 0 and 1). The default value of 1 is equivalent to the Denton benchmarking. 12.11.1.0.0.5 Lambda –; – A parameter that relates to the weights in the regression equation; it is typically equal to 0, 1/2 or 1. A parameter equal to 1 (default value) makes the method equivalent to the multiplicative benchmarking, while a parameter equal to 0 makes the method equivalent to the additive benchmarking. 12.12 Direct-Indirect comparison Economic time series are often computed and reported according to a certain classification or a breakdown. For example, in National Accounts total consumption expenditures are a sum of individual consumption expenditures and General Government &amp; NPISHs consumption expenditures. Therefore, the seasonally adjusted aggregates can be computed either by aggregating the seasonally adjusted components (indirect adjustment) or adjusting the aggregate and the components independently (direct adjustment). The point is that these two strategies result in different seasonally adjusted aggregates. As neither theoretical nor empirical evidence uniformly favours one approach over the other, the choice of the seasonal adjustment strategy concerning aggregated series depends on the user64. Guidance in this field is given in the ESS Guidelines on Seasonal Adjustment (2015) JDemetra+ offers a Direct–Indirect Seasonal Adjustment functionality that facilitates the comparison of the results from these two strategies, which is launched from the main menu. Text The Direct-Indirect Seasonal Adjustment tool To start the analysis drag and drop time series to the top-left panel. The panel on the right presents the sum of selected series. Text Choosing series for an analysis By going to the main menu and clicking on Window → Properties, one can specify benchmarking options for direct-indirect comparison. Be aware that the properties window displays the properties of an active item. Therefore, first click on the time series graph in the picture below and then activate the Properties window. Text The properties of the Direct - Indirect seasonal adjustment functionality By default, the pre-defined TRAMO-SEATS specification is used (RSAfull) for seasonal adjustment of a dataset. To change it, click on the button marked in the picture below. This will provide you with the alternative specifications. Here the user defined specification named My spec is chosen. Text Choosing a specification for the analysis Next, run the process by clicking the button with the green arrow. Text Running a process The bottom panel presents the detailed results. The seasonality test node presents the outcome of the seasonality tests performed for the aggregated series adjusted directly (Direct sa) and indirectly (Indirect sa). The reason for presenting these tests here is that the presence of residual seasonality and calendar effects should be monitored, especially in the indirectly adjusted series65. It might happen that the seasonality is successfully removed from the components but it is still present in the aggregated series. Text Seasonality tests’ results for a direct seasonal adjustment The Differences node presents selected different results between the direct and the indirect seasonal adjustment approaches. The Statistics section shows basic statistics (average, standard deviation, minimum and maximum) for the relative differences (%) between the direct and the indirect SA series. Chart contains the graph of the differences, while Table includes the actual values. The Periodogram section presents graphs for two spectral estimators – the periodogram and the auto-regressive spectrum. Text Graph presenting the differences between direct and indirect seasonal adjustment results The Details node include the basic statistics for the relative differences between the benchmarked and original series as well as the actual time series adjusted directly (Sa series) and indirectly (Benchmarked Sa series). Text Details of the differences between direct and indirect seasonal adjustment results 12.13 Generatiing Output add : some explanations + link to cruncher (production chapter) 12.13.1 Steps Once a seasonal adjustment process for the dataset is performed (see the Simple seasonal adjustment of a single time series scenario) the results can be exported to the external file. Go to the main menu and follow the path: SAProcessing → Output… In the Batch output window the user can specify which output items will be saved and the folder in which JDemetra+ saves the results. It is possible to save the results in the TXT, XLS, CSV, and CSV matrix formats. In the first step the user should choose the output format from the list. Text Default output formats The user may choose more than one format as the output can be generated in different formats at the same time. Text Adding an output format to the list To display and modify the settings click on the given output format on the list. The available options depend on the output format. For Csv format the following options are available: folder (location of the file), file prefix (name of the file), presentation (controls how the output is divided into separate files) and series (series included in the file). These options are presented in the next points of this case study. Text Options for a Csv format The user can define the folder in which the selected results and components will be saved (click the folder item and choose the final destination). Text Specifying a destination folder With the option File Prefix the user can modify the default name of the output saved in the CSV file. Text Setting a File Prefix option Layout controls how the output is divided into separate files. Expand the list to display available options: HTable – the output series will be presented in the form of horizontal tables (time series in rows). VTable – the output series will be presented in the form of vertical tables (time series in columns). List – the output series will be presented in the form of vertical tables (time series in rows). Apart from that, for each time series each file contains in separate columns: the data frequency, the first year and of estimation span, the first period (month or quarter) of observation span and the number of observations. The files do not include dates. Text Layout options for a Csv format The Content section presents a list of series that will be included into a set of output files. To modify the initial settings click on the grey button in the Content section. The CVS-series window presents two panels: the panel on the left includes a list of all valuable output items. The panel on the right presents the selected output items. Mark the series and use the arrows to change the settings. Confirm your choice with the OK button. Text Specifying a content of the output file Options available for the XLS format are the same as for the TXT format with an exception of the Layout section. The list of available codes in the Content section is given here. BySeries – all results for a given time series are placed in one sheet; ByComponent – results are grouped by components. Each component type is saved in a separate sheet. OneSheet – all results are saved in one sheet. Text Layout options for an Excel format If the user sets the option layout to ByComponent, the output will be generated as follows: Text An Excel file view for the ByComponent option The option OneSheet will produce the following XLS file: Text An Excel file view for the OneSheet option By default, the series in the Excel output files are organised vertically. When the user unmarks the check box the horizontal orientation is used. Text The VerticalOrientation option In the case of the TXT format the only available options are folder (location of the file) and series (results included in the output file). The list of available codes in the Content section is given here. Text Options for the Txt output The CSV matrix produces the CSV file containing information about the model and quality diagnostics of the seasonal adjustment. The user may generate the list of default items or create their own quality report. By default, all the available items are included in the output. The list of the items is given here. Text List of items available for the Csv matrix output type Once the output settings are selected, click the OK button. Text Options for the Csv matrix output For each output JDemetra+ provides information on the status of the operation. An example is presented below. Text Generating output - status information 12.13.2 Output items intro add : outputs common with R packages The CSV, TXT and XLS outputs of JDemetra+ may contain the items shown in table below. A list of output items of JDemetra+ CSV, TXT and XLS formats. {: .table .table-style} |Code | Meaning| | ——————————–| ——————————————–| |\\[y\\] | Original series| |\\[y\\_ f\\] | Forecasts of the original series| |\\[y\\_ ef\\] | Standard errors of the forecasts of the original series| |\\[y\\_ c\\] | Interpolated series| |\\[yc\\_ f\\] | Forecasts of the interpolated series| |\\[yc\\_ ef\\] | Standard errors of the forecasts of the interpolated series| |\\[y\\_ lin\\] | Linearised series (not transformed)| |\\[l\\] | Linearised series (transformed)| |\\[{ycal}\\] | Series corrected for calendar effects| |\\[ycal\\_ f\\] |Forecasts of the series corrected for calendar effects| |\\[l\\_ f\\] |Forecasts of the linearised series| |\\[l\\_ b\\] |Backcasts of the linearised series| |\\[t\\] |Trend (including deterministic effects)| |\\[t\\_ f\\] |Forecasts of the trend| |\\[{sa}\\] |Seasonally adjusted series (including deterministic effects)| |\\[sa\\_ f\\] |Forecasts of the seasonally adjusted series| |\\[s\\] |Seasonal component (including deterministic effects)| |\\[s\\_ f\\] |Forecasts of the seasonal component| |\\[i\\] |Irregular component (including deterministic effects)| |\\[i\\_ f\\] |Forecasts of the irregular component| |\\[{det}\\] |All deterministic effects| |\\[det\\_ f\\] |Forecasts of the deterministic effects| |\\[{cal}\\] |Calendar effects| |\\[cal\\_ f\\] |Forecasts of the calendar effects| |\\[{tde}\\] |Trading day effect| |\\[tde\\_ f\\] |Forecasts of the trading day effect| |\\[{mhe}\\] |Moving holidays effects| |\\[mhe\\_ f\\] |Forecasts of the moving holidays effects| |\\[{ee}\\] |Easter effect| |\\[ee\\_ f\\] |Forecasts of the Easter effect| |\\[{omhe}\\] |Other moving holidays effects| |\\[omhe\\_ f\\] |Forecasts of the other moving holidays effects| |\\[{out}\\] |All outliers effects| |\\[out\\_ f\\] |Forecasts of all outliers effects| |\\[out\\_ i\\] |Outliers effects related to irregular (AO, TC)| |\\[out\\_ i\\_ f\\] |Forecasts of outliers effects related to irregular (TC)| |\\[out\\_ t\\] |Outliers effects related to trend (LS)| |\\[out\\_ t\\_ f\\] |Forecasts of outliers effects related to trend (LS)| |\\[out\\_ s\\] |Outliers effects related to seasonal (SO)| |\\[out\\_ s\\_ f\\] |Forecasts of outliers effects related to seasonal (SO)| |\\[{reg}\\] |All other regression effects| |\\[reg\\_ f\\] |Forecasts of all other regression effects| |\\[reg\\_ i\\] |Regression effects related to irregular| |\\[reg\\_ i\\_ f\\] |Forecasts of regression effects related to irregular| |\\[reg\\_ t\\] |Regression effects related to trend| |\\[reg\\_ t\\_ f\\] |Forecasts of regression effects related to trend| |\\[reg\\_ s\\] |Regression effects related to seasonal| |\\[reg\\_ s\\_ f\\] |Forecasts of regression effects related to seasonal| |\\[reg\\_ sa\\] |Regression effects related to seasonally adjusted series| |\\[reg\\_ sa\\_ f\\] |Forecasts of regression effects related to seasonally adjusted series| |\\[reg\\_ y\\] |Separate regression effects| |\\[reg\\_ y\\_ f\\] |Forecasts of separate regression effects| |\\[{fullresiduals}\\] |Full residuals of the RegARIMA model| |\\[decomposition.y\\_ lin\\] |Linearised series used as input in the decomposition| |\\[decomposition.y\\_ lin\\_ f\\] |Forecast of the linearised series used as input in the decomposition| |\\[decomposition.t\\_ lin\\] |Trend produced by the decomposition| |\\[decomposition.t\\_ lin\\_ f\\] |Forecasts of the trend produced by the decomposition| |\\[decomposition.s\\_ lin\\] |Seasonal component produced by the decomposition| |\\[decomposition.s\\_ lin\\_ f\\] |Forecasts of the Seasonal component produced by the decomposition| |\\[decomposition.i\\_ lin\\] |Irregular produced by the decomposition| |\\[decomposition.i\\_ lin\\_ f\\] |Forecasts of the irregular produced by the decomposition| |\\[decomposition.sa\\_ lin\\] |Seasonally adjusted series produced by the decomposition| |\\[decomposition.sa\\_ lin\\_ f\\] |Forecasts of the seasonally adjusted series produced by the decomposition| |\\[decomposition.si\\_ lin\\] |Seasonal-Irregular produced by the decomposition| |\\[decomposition.x - tables.y\\] |For X-13ARIMA-SEATS only. Series from the X-11 decomposition (x = a, b, c, d, e; y=a1…)| |\\[{benchmarking.result}\\] |Benchmarked seasonally adjusted series| |\\[{benchmarking.target}\\] |Target for the benchmarking| The CSV matrix of JDemetra+ may contain: {: .table .table-style} | Code | Meaning | | \\[{span.start}\\ \\] | Start of the series span | | \\[{span.end}\\] | End of the series span | | \\[{span.n}\\] | Length of the series span | | \\[{espan.start}\\] | Start of the estimation span | | \\[{espan.end}\\] | End of the estimation span | | \\[{espan.n}\\] | Length of the estimation span | | \\[{likelihood.neffectiveobs}\\] | Number of effective observations in the likelihood function | | \\[{likelihood.np}\\] | Number of parameters in the likelihood | | \\[{likelihood.logvalue}\\] | Log likelihood | | \\[{likelihood.adjustedlogvalue}\\] | Adjusted log likelihood | | \\[{likelihood.ssqerr}\\] | Sum of the squared errors in the likelihood | | \\[{likelihood.aic}\\] | AIC statistics | | \\[{likelihood.aicc}\\] | Corrected AIC statistics | | \\[{likelihood.bic}\\] | BIC statistics | | \\[{likelihood.bicc}\\] | BIC corrected for length | | \\[{residuals.ser}\\] | Standard error of the residuals (unbiased, TRAMO-like) | | \\[residuals.ser - ml\\] | Standard error of the residuals (ML, X-13ARIMA-SEATS-like) | | \\[{residuals.mean}\\] | Test on the mean of the residuals | | \\[{residuals.skewness}\\] | Test on the skewness of the residuals | | \\[{residuals.kurtos}\\] | Test on the kurtosis of the residuals | | \\[{residuals.dh}\\] | Test on the normality of the residuals (Doornik-Hansen tests) | | \\[{residuals.lb}\\] | The Ljung-Box test on the residuals | | \\[{residuals.lb2}\\] | The Ljung-Box test on the squared residuals | | \\[{residuals.seaslb}\\] | The Ljung-Box test on the residuals at seasonal lags | | \\[{residuals.bp}\\] | The Box-Pierce test on the residuals | | \\[{residuals.bp2}\\] | The Box-Pierce test on the squared residuals | | \\[{residuals.seasbp}\\] | The Box-Pierce test on the residuals at seasonal lags | | \\[{residuals.nruns}\\] | Test on the number of runs of the residuals | | \\[{residuals.lruns}\\] | Test on the length of runs of the residuals | | \\[mstatistics.m1\\] | The relative contribution of the irregular over three months span | | \\[mstatistics.m2\\] | The relative contribution of the irregular component to the stationary portion of the variance | | \\[mstatistics.m3\\] | The amount of period to period change in the irregular component as compared to the amount of period to period change in the trend-cycle | | \\[mstatistics.m4\\] | The amount of autocorrelation in the irregular as described by the average duration of run | | \\[mstatistics.m5\\] | The number of periods it takes the change in the trend-cycle to surpass the amount of change in the irregular | | \\[mstatistics.m6\\] | The amount of year to year change in the irregular as compared to the amount of year to year change in the seasonal | | \\[mstatistics.m7\\] | The amount of moving seasonality present relative to the amount of stable seasonality | | \\[mstatistics.m8\\] | The size of the fluctuations in the seasonal component throughout the whole series | | \\[mstatistics.m9\\] | The average linear movement in the seasonal component throughout the whole series | | \\[mstatistics.m10\\] | The size of the fluctuations in the seasonal component in the recent years | | \\[mstatistics.m11\\] | The average linear movement in the seasonal component in the recent years | | \\[{mstatistics.q}\\] | Summary of the M-Statistics | | \\[mstatistics.q - m2\\] | Summary of the M-Statistics without M2 | | \\[{diagnostics.quality}\\] | Summary of the diagnostics | | \\[{diagnostics.basic\\ checks.definition:2}\\] | Definition test | | \\[{diagnostics.basic\\ checks.annual\\ totals:2}\\] | Annual totals test | | \\[{diagnostics.visual\\ spectral\\ analysis.spectral\\ seas\\ peaks}\\] | Test of the presence of the visual seasonal peaks in SA and/or irregular | | \\[{diagnostics.visual\\ spectral\\ analysis.spectral\\ td\\ peaks}\\] | Test of the presence of the visual trading day peaks in SA and/or irregular | | \\[{diagnostics.regarima\\ residuals.normality:2}\\] | Test of the normality of the residuals | | \\[{diagnostics.regarima\\ residuals.independence:2}\\] | Test of the independence of the residuals | | \\[{diagnostics.regarima\\ residuals.spectral\\ td\\ peaks:2}\\] | Test of the presence of trading day peaks in the residuals | | \\[{diagnostics.regarima\\ residuals.spectral\\ seas\\ peaks:2}\\] | Test of the presence of seasonal peaks in the residuals | | \\[{diagnostics.residual\\ seasonality.on\\ sa:2}\\] | Test of the presence of residual seasonality in the SA series | | \\[{diagnostics.residual\\ seasonality.on\\ sa\\ (last\\ 3\\ years):2}\\] | Test of the presence of residual seasonality on sa (last 3 years):2\\[ | | \\]{diagnostics.residual seasonality.on irregular:2}\\[ | Test of the presence of residual seasonality in the irregular series (last periods) | | \\]diagnostics.seats.seas variance:2\\[ | Test on the variance of the seasonal component | | \\]diagnostics.seats.irregular variance:2\\[ | Test on the variance of the irregular component | | \\]diagnostics.seats.seas/irr cross - correlation:2\\[ | Test on the cross-correlation between the seasonal and the irregular component | | \\]{log}\\[ | Log transformation | | \\]{adjust}\\[ | Pre-adjustment of the series for leap year | | \\]{arima.mean}\\[ | Mean correction | | \\]{arima.p}\\[ | The regular autoregressive order of the ARIMA model | | \\]{arima.d}\\[ | The regular differencing order of the ARIMA model | | \\]{arima.q}\\[ | Regular moving average order of the ARIMA model | | \\]{arima.bp}\\[ | The seasonal autoregressive order of the ARIMA model | | \\]{arima.bd}\\[ | The seasonal differencing order of the ARIMA model | | \\]{arima.bq}\\[ | The seasonal moving average order of the ARIMA model | | \\]arima.phi(i)\\[ | Regular autoregressive parameter (lag=$i$, max $i$=3) of the ARIMA model | | \\]arima.th(i)\\[ | Regular moving average parameter (lag=$i$, max $i$=3) of the ARIMA model | | \\]arima.bphi(i)\\[ | Seasonal autoregressive parameter (lag=$i$, max $i$=1) of the ARIMA model | | \\]arima.bth(i)\\[ | Seasonal moving average parameter (lag=$i$ max $i$=1) of the ARIMA model | | \\]regression.lp:3\\[ | Coefficient and test on the leap year | | \\]{regression.ntd}\\[ | Number of trading day variables | | \\]{regression.td}( i ):3\\[ | Coefficient and test on the $i^{{th}}\\ $trading day variable | | \\]{regression.nmh}\\[ | Number of moving holidays | | \\]regression.easter:3\\[ | Coefficient and test on the Easter variable | | \\]{regression.nout}\\[ | Number of outliers | | \\]{regression.out}( i ):3\\[ | Coefficient and test on $i^{{th}}\\ $the outlier (max $i$=16) | | \\]{decomposition.seasonality}\\[ | Presence of a seasonal component (1 -- present, 0 -- not present) | | \\]{decomposition.trendfilter}\\[ | The order of the trend filter | | \\]{decomoposition.seasfilter}$$ | The order of the seasonal filter | Description of the idea of benchmarking is based on Dagum, B.E., Cholette, P.A. 1994), and Quenneville, B. et all (2003). Detailed information can be found in: Dagum, B.E., Cholette, P.A. (2006).↩︎ MARAVALL, A. (2009), ’Identification of Reg-ARIMA Models and of Problematic Series in Large Scale Applications: Program TSW (TRAMO-SEATS for Windows), Banco de España↩︎ Description of the idea of benchmarking is based on Dagum, B.E., Cholette, P.A. 1994), and Quenneville, B. et all (2003). Detailed information can be found in: Dagum, B.E., Cholette, P.A. (2006).↩︎ The likelihood function is the joint probability (density) function of observable random variables. It is viewed as the function of the parameters given the realized random variables. Therefore, this function measures the probability of observing the particular set of dependent variable values that occur in the sample.↩︎ AIC, AICC, BIC and Hannan-Quinn criteria are used by X-13ARIMA-SEATS while BIC (TRAMO definition) by TRAMO/SEATS. These criteria are used in seasonal adjustment procedures for the selection of the optimum ARIMA model. The model with the smaller value of the model selection criteria is preferred.↩︎ Maximum Likelihood Estimation is a statistical method for estimating the coefficients of a model. This method determines the parameters that maximize the probability (likelihood) of the sample data.↩︎ The variable is called statistically significant if it is so extreme that such a result would be expected to arise simply by chance only in rare circumstances (with the probability equal to p-value). Generally, the regressor is thought to be significant if p-value is lower than 5%.↩︎ Missing observations are treated as additive outliers and interpolated accordingly.↩︎ Description based on the ESS Guidelines on Seasonal Adjustment (2015).↩︎ ESS Guidelines on Seasonal Adjustment (2015).↩︎ "],["r-packages.html", "Chapter 13 R packages 13.1 Available functions 13.2 Interaction with GUI", " Chapter 13 R packages 13.1 Available functions 13.1.1 Seasonal Adjustment 13.2 Interaction with GUI "],["plug-ins-for-jdemetra.html", "Chapter 14 Plug-ins for JDemetra+ 14.1 Main functions", " Chapter 14 Plug-ins for JDemetra+ 14.1 Main functions "],["production.html", "Chapter 15 Production", " Chapter 15 Production "],["tool-selection-issues.html", "Chapter 16 Tool selection issues", " Chapter 16 Tool selection issues "],["spectral-analysis-principles-and-tools.html", "Chapter 17 Spectral Analysis Principles and Tools 17.1 Overview 17.2 Periodogram 17.3 Autoregressive spectrum estimation 17.4 Tukey Spectrum definition", " Chapter 17 Spectral Analysis Principles and Tools add : R code, rjd3toolkit references 17.1 Overview 17.2 Periodogram 17.2.1 Step 1 For any given frequency \\(\\omega\\) the sample periodogram is the sample analog of the sample spectrum. In general, the periodogram is used to identify the periodic components of unknown frequency in the time series. X-13ARIMA-SEATS and TRAMO-SEATS use this tool for detecting seasonality in raw time series and seasonally adjusted series. Apart from this it is applied for checking randomness of the residuals from the ARIMA model. To define the periodogram, first consider the vector of complex numbers66: \\[\\mathbf{x} = \\begin{bmatrix} x_{1} \\\\ x_{2} \\\\ . \\\\ . \\\\ . \\\\ x_{n} \\\\ \\end{bmatrix} \\in \\mathbb{C}^{n}\\] where \\(\\mathbb{C}^{n}\\) is the set of all column vectors with complex-valued components. The Fourier frequencies associated with the sample size \\[n\\] are defined as a set of values \\[ω_{j} = \\frac{2\\pi j}{n}\\], \\[j = - \\lbrack \\frac{n-1}{2}\\rbrack,\\ldots,\\lbrack\\frac{n}{2}\\rbrack\\], \\[-\\pi&lt; \\omega_{j} \\leq \\pi\\], \\[j\\in F_{n}\\], where \\({\\lbrack n\\rbrack}\\) denotes the largest integer less than or equal to \\(n\\). The Fourier frequencies, which are called harmonics, are given by integer multiples of the fundamental frequency \\(\\ \\frac{2\\pi}{n}\\). Now the \\(n\\) vectors \\[e_{j} = n^{- \\frac{1}{2}}\\left(e^{-i\\omega_{j}},e^{-i{2\\omega}_{j}}, \\ldots,e^{- inω_{j}}\\right)^{&#39;}\\] can be defined. Vectors \\[e_{1},\\ldots, e_{n}\\] are orthonormal in the sense that: \\[ {\\mathbf{e}_{j}^{*}\\mathbf{e}}_{k} = n^{- 1}\\sum_{r = 1}^{n}e^{ir(\\omega_{j} - \\omega_{k})} = \\left\\{ \\begin{matrix} 1,\\ if\\ j = k \\\\ 0,\\ if\\ j \\neq k \\\\ \\end{matrix} \\right.\\ \\] where \\[\\mathbf{e}_{j}^{*}\\] denotes the row vector, which \\[k^{th}\\] component is the complex conjugate of the \\[k^{th}\\] component of \\[\\mathbf{e}_{j}\\].67 These vectors are a basis of \\[F_{n}\\], so that any \\[\\mathbf{x}\\in\\mathbb{C}^{n}\\] can be expressed as a sum of \\[n\\] components: \\[ \\mathbf{x} = \\sum_{j = - \\lbrack\\frac{n - 1}{2}\\rbrack}^{\\lbrack\\frac{n}{2}\\rbrack}{a_{j}\\mathbf{e}_{j}} \\] where the coefficients \\[a_{j} = \\mathbf{e}_{j}^{*}\\mathbf{x}=n^{-\\frac{1}{2}}\\sum_{t = 1}^{n}x_{t}e^{-it\\omega_{j}}\\] are derived from \\[3\\] by multiplying the equation on the left by \\[\\mathbf{e}_{j}^{*}\\] and using \\[1\\]. The sequence of \\[\\{a_{j},j\\in F_{n}\\}\\] is referred as a discrete Fourier transform of \\(\\mathbf{x}\\mathbb{\\in C}^{n}\\) and the periodogram \\(I(\\omega_{j})\\) of \\(\\mathbf{x}\\) at Fourier frequency \\(\\omega_{j} = \\frac{2\\pi j}{n}\\) is defined as the square of the Fourier transform \\[\\{a_{j}\\}\\] of \\(\\mathbf{x}\\): \\[ {I\\left( \\omega_{j} \\right)\\mathbf{=}{\\left| a_{j} \\right|^{2}}_{\\ } = n^{- \\ 1}\\left| \\sum_{t = 1}^{n}x_{t}e^{- it\\omega_{j}} \\right|^{2}}_{\\mathbf{\\ }} \\] From \\[2\\] and \\[3\\] it can be shown that in fact the periodogram decomposes the total sum of squares \\(\\sum_{t = 1}^{n}\\left| x_{t} \\right|^{2}\\) into a sums of components associated with the Fourier frequencies \\[ω_{j}\\]: \\[ \\sum_{t=1}^{n}{\\left|x_{t}\\right|}^{2} = \\sum_{j = - \\lbrack\\frac{n - 1}{2}\\rbrack}^{\\lbrack\\frac{n}{2}\\rbrack}\\left|a_{j}\\right|^{2} = \\sum_{j = - \\lbrack\\frac{n - 1}{2}\\rbrack}^{\\lbrack\\frac{n}{2}\\rbrack}{I\\left( \\omega_{j} \\right)} \\] If \\(\\ \\mathbf{x\\ \\in}\\ {R}^{n}\\), \\(\\omega_{j}\\) and \\[{-\\omega}_{j}\\] are both in \\[\\lbrack- \\pi, -\\pi \\rbrack\\] and \\[a_{j}\\] is presented in its polar form (i.e.\\[a_{j} = r_{j}\\exp\\left( i\\theta_{j} \\right)\\]), where \\(r_{j}\\) is the modulus of \\[a_{j}\\], then \\[3\\] can be rewritten in the form: \\[ \\mathbf{x} = a_{0}\\mathbf{e}_{0} + \\sum_{j = 1}^{\\lbrack\\frac{n - 1}{2}\\rbrack}{ {2^{1/2}r}_{j}{(\\mathbf{c}}_{j}\\cos\\theta_{j}{- \\mathbf{s}}_{j}\\sin\\theta_{j}) + a_{n/2}\\mathbf{e}_{n/2}} \\] The orthonormal basis for \\[{R}^{n}\\] is \\[\\{\\mathbf{e}_{0},\\mathbf{c}_{1},\\mathbf{s}_{1},\\ldots,\\mathbf{c}_{\\lbrack\\frac{n - 1}{2}\\rbrack},\\mathbf{s}_{\\lbrack\\frac{n - 1}{2}\\rbrack},\\mathbf{e}_{\\frac{n}{2}(excluded\\ if\\ n\\ is\\ odd)}\\} \\], where: \\[\\mathbf{e}_{0}\\] is a vector composed of n elements equal to \\[n^{- 1/2}\\], which implies that \\[\\mathbf{a}_{0}\\mathbf{e}_{0} = {(n^{-1}\\sum_{t = 1}^{n}x_{t},\\ldots,n^{- 1}\\sum_{t=1}^{n}x_{t})}^{&#39;}\\]; \\[ \\mathbf{c}_{j}=\\left(\\frac{n}{2}\\right)^{- 1/2}{\\left(\\cos\\omega_{j},\\cos{2\\omega}_{j},\\ldots,\\cos{n\\omega_{j}}\\right)}^{&#39;}, for 1 \\leq j \\leq \\lbrack \\frac{(n - 1)}{2}\\rbrack \\] ; \\[ \\mathbf{s}_{j} = {\\left( \\frac{n}{2} \\right)}^{-1/2}{\\left(\\sin{\\omega_{j}},\\sin{2\\omega_{j}},\\ldots,\\sin{n\\omega_{j}}\\right)}^{&#39;},\\ for\\ 1 \\leq j \\leq \\lbrack \\frac{(n - 1)}{2} \\rbrack \\]; \\[ \\mathbf{e}_{n/2} = {\\left(- \\left(n^{-\\frac{1}{2}}\\right),n^{- \\frac{1}{2}},\\ldots,{-\\left(n\\right)}^{- \\frac{1}{2}}),n^{-\\frac{1}{2}}\\right)}^{&#39;} \\]. Equation \\[5\\] can be seen as an OLS regression of \\[x_{t}\\] on a constant and the trigonometric terms. As the vector of explanatory variables includes \\[n\\] elements, the number of explanatory variables in \\[5\\] is equal to the number of observations. HAMILTON, J.D. (1994) shows that the explanatory variables are linearly independent, which implies that an OLS regression yields a perfect fit (i.e. without an error term). The coefficients have the form of a simple OLS projection of the data on the orthonormal basis: \\[ {\\widehat{a}}_{0}=\\frac{1}{\\sqrt{n}}\\sum_{t=1}^{n}x_{t} \\] \\[7\\] \\[ {\\widehat{a}}_{n/2}=\\frac{1}{\\sqrt{n}}\\sum_{t=1}^{n}{(-1)}^{t}x_{t}\\left( \\text{only when n is even} \\right) \\] \\[8\\] \\[ {\\widehat{a}}_{0}=\\frac{1}{\\sqrt{n}}\\sum_{t=1}^{n}x_{t} \\] \\[9\\] \\[ {\\widehat{\\alpha}}_{j} = 2^{1/2}r_{j}\\cos{\\theta_{j}} = {\\left(\\frac{n}{2} \\right)}^{- 1/2}\\sum_{t = 1}^{n}x_{t}\\cos{\\left(t\\frac{2\\pi j}{n}\\right)}, j = 1,\\ldots,\\lbrack\\frac{n - 1}{2}\\rbrack \\] \\[10\\] \\[ {\\widehat{\\beta}}_{j} = 2^{1/2}r_{j}\\sin{\\theta_{j}} = {\\left( \\frac{n}{2} \\right)}^{-1/2}\\sum_{t = 1}^{n}x_{t}\\sin{\\left(t\\frac{2\\pi j}{n} \\right)}, j = 1,\\ldots,\\lbrack\\frac{n - 1}{2}\\rbrack \\] \\[11\\] With \\[5\\] the total sum of squares \\(\\sum_{t = 1}^{n}\\left| x_{t} \\right|^{2}\\) can be decomposed into \\[2 \\times \\lbrack\\frac{n - 1}{2}\\rbrack\\] components corresponding to \\[\\mathbf{c}_{j}\\] and \\[\\mathbf{s}_{j}\\], which are grouped to produce the “frequency \\[ω_{j}\\]” component for \\[1 \\geq j \\geq \\lbrack\\frac{n - 1}{2}\\rbrack\\]. As it is shown in the table below, the value of the periodogram at the frequency \\(\\omega_{j}\\) is the contribution of the\\(\\ j^{\\text{th}}\\ \\)harmonic to the total sum of squares \\(\\sum_{t = 1}^{n}\\left| x_{t} \\right|^{2}\\). Decomposition of sum of squares into components corresponding to the harmonics {: .table .table-style} |Frequency |Degrees of freedom |Sum of squares decomposition| |———————————————– |———————— |————————————————————-| |\\(\\omega_{0}\\)(mean) |1 |\\[{a_{0}^{2}}_{\\ }=n^{- 1}\\left( \\sum_{t=1}^{n}x_{t} \\right)^{2} = I\\left( 0 \\right)\\]| |\\[\\omega_{1}\\] |2 |\\[{2r_{1}^{2}}_{\\ } = 2{\\|a_{1}\\|}^{2} = 2I\\left( \\omega_{1} \\right)\\]| |\\[\\vdots\\] |\\[\\vdots\\] |\\[\\vdots\\]| |\\[\\omega_{k}\\] |2 |\\[{2r_{k}^{2}}_{\\ } = 2{\\|a_{k}\\|}^{2} = 2I\\left( \\omega_{k} \\right)\\]| |\\[\\vdots\\] |\\[\\vdots\\] |\\[\\vdots\\]| |\\(\\omega_{n/2} = \\pi\\) (excluded if \\(n\\) is odd) |1 |\\[a_{n/2}^{2} = I\\left( \\pi \\right)\\]| |Total |\\[\\mathbf{n}\\] |\\[\\sum_{\\mathbf{t = 1}}^{\\mathbf{n}}\\mathbf{x}_{\\mathbf{t}}^{\\mathbf{2}}\\]| Source: DE ANTONIO, D., and PALATE, J. (2015). Obviously, if series were random then each component \\(I\\left( \\omega_{j} \\right)\\ \\)would have the same expectation. On the contrary, when the series contains a systematic sine component having a frequency \\(j\\) and amplitude \\(A\\) then the sum of squares \\(I\\left( \\omega_{j} \\right)\\) increases with \\(A\\). In practice, it is unlikely that the frequency \\(j\\) of an unknown systematic sine component would exacly match any of the frequencies, for which peridogram have been calcuated. Therefore, the periodogram would show an increase in intensities in the immediate vicinity of \\(j\\).68 Note that in JDemetra+ the periodogram object corresponds exactly to the contribution to the sum of squares of the standardised data, since the series are divided by their standard deviation for computational reasons. Using the decomposition presented in table above the periodogram can be expressed as: \\[ I\\left( \\omega_{j} \\right)\\mathbf{=}\\begin{matrix} r_{j}^{2} = \\frac{1}{2}{(\\alpha}_{j}^{2} + \\beta_{j}^{2}) = \\ {\\frac{1}{n}\\left( \\sum_{t = 1}^{n}{x_{t}\\cos{\\left( {t\\frac{2\\pi j}{n}}_{\\ } \\right)\\ }} \\right)}^{2} + \\frac{1}{n}\\left( \\sum_{t = 1}^{n}{x_{t}\\sin\\left( t\\frac{2\\pi j}{n} \\right)_{\\ }} \\right)^{2} \\\\ \\end{matrix} \\] \\[12\\] where \\(j = 0,\\ldots,\\left\\lbrack \\frac{n}{2} \\right\\rbrack\\). Since \\(\\mathbf{x} - \\overline{\\mathbf{x}}\\) are generated by an orthonormal basis, and \\(\\overline{\\mathbf{x}}\\mathbf{=}a_{0}\\mathbf{e}_{0}\\) \\[5\\] can be rearranged to show that the sum of squares is equal to the sum of the squared coefficients: \\[ \\mathbf{x} - a_{0}\\mathbf{e}_{0} =\\sum_{j=1}^{\\lbrack(n - 1)/2\\rbrack}\\left(\\alpha_{j}\\mathbf{c}_{j}+\\beta_{j}\\mathbf{s}_{j}\\right) + a_{n/2}\\mathbf{e}_{n/2} \\]. \\[13\\] Thus the sample variance of \\[x_{t}\\] can be expressed as: \\[ n^{- 1}\\sum_{t=1}^{n}{\\left(x_{t}-\\overline{x}\\right)}^{2}=n^{-1}\\left(\\sum_{k=1}^{\\lbrack(n - 1)/2\\rbrack}2{r_{j}}^{2} +{a_{n/2}}^{2}\\right) \\], \\[14\\] where \\(a_{n/2}^{2}\\) is excluded if \\(n\\) is odd. The term \\[2{r_{j}}^{2}\\] in \\[14\\] is then the contribution of the \\(j^{\\text{th}}\\) harmonic to the variance and \\[14\\] shows then how the total variance is partitioned. The periodogram ordinate \\(I\\left( \\omega_{j} \\right)\\) and the autocovariance coefficient \\(\\gamma(k)\\) are both quadratic forms of \\[x_{t}\\]. It can be shown that the periodogram and autocovarinace function are related and the periodogram can be written in terms of the sample autocovariance function for any non-zero Fourier frequency \\[ω_{j}\\] :69 \\[ I\\left( \\omega_{j} \\right) = \\sum_{\\left| k \\right| &lt; n}^{\\ }{\\widehat{\\gamma}\\left( k \\right)}_{\\ }e^{- ik\\omega_{j}} = {\\widehat{\\gamma}\\left( 0 \\right)}_{\\ } + 2\\sum_{k = 1}^{n - 1}{\\widehat{\\gamma}\\left( k \\right)\\cos{(k\\omega_{j})}}_{\\ } \\] and for the zero frequency \\(\\ I\\left( 0 \\right) = n\\left| \\overline{x} \\right|^{2}\\). Once comparing \\[15\\] with an expression for the spectral density of a stationary process: \\[ f\\left( \\omega_{\\ } \\right) = \\frac{1}{2\\pi}\\sum_{k &lt; - \\infty}^{\\infty}{\\gamma\\left( k \\right)}_{\\ }e^{- ik\\omega_{\\ }} = \\frac{1}{2\\pi}\\left\\lbrack {\\gamma\\left( 0 \\right)}_{\\ } + 2\\left(\\sum_{k = 1}^{\\infty}{\\gamma\\left( k \\right)\\cos{(k\\omega_{\\ })}} \\right) \\right\\rbrack \\] it can be noticed that the periodogram is a sample analog of the population spectrum. In fact, it can be shown that the periodogram is asymptotically unbiased but inconsistent estimator of the population spectum \\(f(\\omega)\\).[^75] Therefore, the periodogram is a wildly fluctuatating, with high variance, estimate of the spectrum. However, the consistent estimator can be achieved by applying the different linear smoothing filters to the periodogram, called lag-window estimators. The lag-window estimators implemented in JDemetra+ includes square, Welch, Tukey, Barlett, Hanning and Parzen. They are described in DE ANTONIO, D., and PALATE, J. (2015). Alternatively, the model-based consistent estimation procedure, resulting in autoregressive spectrum estimator, can be applied. 17.2.2 Step 2 The periodogram \\[ I(\\omega_j)\\] of \\[ \\mathbf{X} \\in \\mathbb{C}^n \\] is defined as the squared of the Fourier transform \\[ I(\\omega_{j})=a_{j}^{2}=n^{-1}\\left| \\sum_{t=1}^{n}\\mathbf{X_t} e^{-it\\omega_j} \\right|^{2}, \\] where the Fourier frequencies \\[ \\omega_{j} \\] are given by multiples of the fundamental frequency \\[ \\frac{2\\pi}{n} \\]: \\[ \\omega_{j}= \\frac{2\\pi j}{n}, -\\pi &lt; \\omega_{j} \\leq \\pi \\] An orthonormal basis in \\[ \\mathbb{R}^n \\]: \\[ \\left\\{ e_0, ~~~~~~c_1, s_1, ~~~~~\\ldots~~~~~\\ , ~~~~c_{[(n-1)/2]}, s_{[(n-1)/2]}~~~~,~~~~~~ e_{n/2} \\right\\}, \\] where \\[ e_{n/2} \\] is excluded if \\[ n \\] is odd, can be used to project the data and obtain the spectral decomposition Thus, the periodogram is given by the projection coefficients and represents the contribution of the jth harmonic to the total sum of squares, as illustrated by Brockwell and Davis (1991): Source Degrees of freedom \\[~~~~\\] Sum of squares decomposition Frequency \\[ \\omega_{0} \\] 1 \\[ a_{0}^{2}= n^{-1}(\\sum_{t=1}^{n}x_t )^2 =I(0)\\] Frequency \\[ \\omega_{1} \\] 2 \\[ 2 r^{2}_{1} = 2 \\left\\| a_{1} \\right\\|^{2} = 2 I(\\omega_{1}) \\] \\[ \\vdots \\] \\[ \\vdots \\] \\[ \\vdots \\] Frequency \\[ \\omega_{k} \\] 2 \\[ 2 r^{2}_{k} = 2 \\left\\| a_{k} \\right\\|^{2} = 2 I(\\omega_{k}) \\] \\[ \\vdots \\] \\[ \\vdots \\] \\[ \\vdots \\] Frequency \\[ \\omega_{n/2}=\\pi \\] 1 \\[ a_{n/2}^{2} = I(\\pi) \\] (excluded if \\[ n \\] is odd) \\[ ========= \\] \\[ ====== \\] \\[ ============ \\] Total n \\[ \\sum_{t=1}^{n}\\mathbf{X^2_t} \\] \\[~~~~\\] In JDemetra+, the periodogram of \\[ \\mathbf{X} \\in \\mathbb{R}^n \\] is computed for the standardized time series. This scenario is designed for advanced users interested in an in-depth analysis of time series in the frequency domain using three spectral graphs. Those graphs can also be used as a complementary analysis for a better understanding of the results obtained with some of the tests described above. Economic time series are usually presented in a time domain (X-axis). However, for analytical purposes it is convenient to convert the series to a frequency domain due to the fact that any stationary time series can be expressed as a combination of cosine (or sine) functions. These functions are characterized with different periods (amount of time to complete a full cycle) and amplitudes (maximum/minimum value during the cycle). The tool used for the analysis of a time series in a frequency domain is called a spectrum. The peaks in the spectrum indicate the presence of cyclical movements with periodicity between two months and one year. A seasonal series should have peaks at the seasonal frequencies. Calendar adjusted data are not expected to have peak at with a calendar frequency. The periodicity of the phenomenon at frequency f is \\(\\frac{2\\pi}{f}\\). It means that for a monthly time series the seasonal frequencies \\(\\frac{\\pi}{6},\\ \\frac{\\pi}{3},\\ \\frac{\\pi}{2},\\ \\frac{2\\pi}{3},\\ \\frac{5\\pi}{6}\\ \\) and \\(\\pi\\) correspond to 1, 2, 3, 4, 5 and 6 cycles per year. For example, the frequency \\(\\frac{\\pi}{3}\\) corresponds to a periodicity of 6 months (2 cycles per year are completed). For the quarterly series there are two seasonal frequencies: \\(\\frac{\\pi}{2}\\) (one cycle per year) and \\(\\pi\\) (two cycles per year). A peak at the zero frequency always corresponds to the trend component of the series. Seasonal frequencies are marked as grey vertical lines, while violet vertical lines represent the trading-days frequencies. The trading day frequency is 0.348 and derives from the fact that a daily component which repeats every seven days goes through 4.348 cycles in a month of average length 30.4375 days. It is therefore seen to advance 0.348 cycles per month when the data are obtained at twelve equally spaced times in 365.25 days (the average length of a year). The interpretation of the spectral graph is rather straightforward. When the values of a spectral graph for low frequencies (i.e. one year and more) are large in relation to its other values it means that the long-term movements dominate in the series. When the values of a spectral graph for high frequencies (i.e. below one year) are large in relation to its other values it means that the series are rather trendless and contains a lot of noise. When the values of a spectral graph are distributed randomly around a constant without any visible peaks, then it is highly probable that the series is a random process. The presence of seasonality in a time series is manifested in a spectral graph by the peaks on the seasonal frequencies. A time series \\[x_{t}\\] with stationary covariance, mean \\(μ\\) and \\[k^{th}\\] autocovariance \\(E\\left( \\left( x_{t} - \\mu \\right)\\left( x_{t - k} - \\mu \\right) \\right) = \\gamma(k)\\$ can be described as a weighted sum of periodic trigonometric functions: sin\\)(t)$ and cos\\((\\omega t)\\), where \\(\\omega\\) denotes frequency. Spectral analysis investigates this frequency domain representation of \\(x_{t}\\) to determine how important cycles of different frequencies are in accounting for the behaviour of \\(x_{t}\\). Assuming that the autocovariances \\(\\gamma(k)\\) are absolutely summable (\\(\\sum_{k = - \\infty}^{\\infty}\\left| \\gamma(k) \\right| &lt; \\infty\\)), the autocovariance generating function, which summarises these autocovariances through a scalar valued function, is given by equation \\[1\\]70. \\(acgf(z) = \\sum_{k = - \\infty}^{\\infty}{z^{k}\\gamma(k)}\\), where \\(z\\) denotes complex scalar. Once the equation \\[1\\] is divided by \\(\\pi\\) and evaluated at some \\(z{= e}^{- i\\omega} = cos\\omega - isin\\omega\\), where \\(i = \\sqrt{- 1}\\) and \\(\\omega\\) is a real scalar,\\(\\ - \\infty &lt; \\ \\omega &lt; \\infty\\), the result of this transformation is called a population spectrum \\(f\\left( \\omega \\right)\\ \\)for \\(\\ x_{t}\\), given in equation \\[2\\]71. \\[f\\left( \\omega \\right) = \\frac{1}{\\pi}\\sum_{k = - \\infty}^{\\infty}{e^{- ik\\omega}\\gamma(k)}\\] Therefore, the analysis of the population spectrum in the frequency domain is equivalent to the examination of the autocovariance function in the time domain analysis; however it provides an alternative way of inspecting the process. Because \\(f\\left( \\omega \\right)\\text{dω}\\) is interpreted as a contribution to the variance of components with frequencies in the range \\((\\omega,\\ \\omega + d\\omega)\\), a peak in the spectrum indicates an important contribution to the variance at frequencies near the value that corresponds to this peak. As \\(e^{- i\\omega} = cos\\omega - isin\\omega,\\ \\)the spectrum can be also expressed as in equation \\[3\\]. \\[f\\left( \\omega \\right) = \\frac{1}{\\pi}\\sum_{k = - \\infty}^{\\infty}{(cos\\omega k - isin\\omega k)\\gamma(k)}\\] Since \\(\\gamma(k) = \\gamma( - k)\\) (i.e. \\(\\gamma(k)\\ \\)is an even function of \\(k\\)) and \\(\\sin{( - x)}\\ = \\operatorname{-sin}x\\), \\[3\\] can be presented as equation \\[f\\left( \\omega \\right) = \\frac{1}{\\pi}\\left\\lbrack \\ \\gamma(0) + 2\\sum_{k = 1}^{\\infty}{\\ \\gamma(k)}cos\\text{ωk} \\right\\rbrack\\], This implies that if autocovariances are absolutely summable the population spectrum exists and is a continuous, real-valued function of \\(\\omega\\). Due to the properties of trigonometric functions \\(\\left( \\cos\\left( - \\omega k \\right) = \\cos\\left( \\text{ωk} \\right) \\right.\\ \\ \\)and \\(\\left. \\ \\cos\\left( \\omega + 2\\pi j)k = cos(\\omega k \\right) \\right)\\ \\)the spectrum is a periodic, even function of \\(\\omega\\), symmetric around \\(\\omega = 0\\). Therefore, the analysis of the spectrum can be reduced to the interval \\(( - \\pi,\\pi).\\) The spectrum is nonnegative for all \\(\\omega \\in ( - \\pi,\\pi)\\). The shortest cycle that can be distinguished in a time series lasts two periods. The frequency which corresponds to this cycle is \\(\\omega = \\pi\\) and is called the Nyquist frequency. The frequency of the longest cycles that can be observed in the time series with \\(n\\) observations is \\(\\omega = \\frac{2\\pi}{n}\\) and is called the fundamental (Fourier) frequency. Note that if \\[x_{t}\\] is a white noise process with zero mean and variance \\[\\sigma^{2}\\], then for all \\[\\left| k \\right| &gt; 0\\] \\[\\gamma\\left(k\\right)=0\\] and the spectrum of \\[x_{t}\\] is constant (\\[f\\left(\\omega\\right)= \\frac{\\sigma^{2}}{\\pi}\\]) since each frequency in the specrum contributes equally to the variance of the process72. The aim of spectral analysis is to determine how important cycles of different frequencies are in accounting for the behaviour of a time series73. Since spectral analysis can be used to detect the presence of periodic components, it is a natural diagnostic tool for detecting trading day effects as well as seasonal effects74. Among the tools used for spectral analysis are the autoregressive spectrum and the periodogram. The explanations given in the subsections of this node derive mainly from DE ANTONIO, D., and PALATE, J. (2015) and BROCKWELL, P.J., and DAVIS, R.A. (2006). The estimator of the spectral density at frequency \\[\\lambda \\in [0,\\pi]\\] will be given by the assumption that the series will follow an AR(p) process with large \\[p\\]. The spectral density of such model, with an innovation variance \\[ var(x_{t})=\\sigma^2_x \\], is expressed as follows: \\[ 10\\times log_{10} f_x(\\lambda)=10\\times log_{10} \\frac{\\sigma^2_x}{2\\pi \\left|\\phi(e^{i\\lambda}) \\right|^2 }=10\\times log_{10} \\frac{\\sigma^2_x}{2\\pi \\left|1-\\sum_{k=1}^{p}\\phi_k e^{i k \\lambda}) \\right|^2 } \\] where \\[ \\phi_k \\] denotes the AR(k) coefficient, and \\[ e^{-ik\\lambda}=cos⁡(-ik\\lambda)+i sin⁡(-ik\\lambda)\\]. Soukup and Findely (1999) suggest the use of p=30, which in practice much larger than the order that would result from the AIC criterion. The minimum number of observations needed to compute the spectrum is set to n=80 for monthly data (or n=60) for quarterly series. In turn, the maximum number of observations considered for the estimation is n=121. This choice offers enough resolution, being able to identify a maximum of 30 peaks in a plot of 61 frequencies: by choosing \\[ \\lambda_j=\\pi j/60 \\],for \\[ j=0,1,…,60 \\], we are able to calculate our density estimates at exact seasonal frequencies (1, 2, 3, 4, 5 and 6 cycles per year). Note that \\[x\\] cycles per year can be converted into cycles per month by simply dividing by twelve, \\[x/12\\], and to radians by applying the transformation \\[2\\pi(x/12)\\]. The traditional trading day frequency corresponding to 0.348 cycles per month is used in place of the closest frequency \\[\\pi j/60\\]. Thus, we replace \\[\\pi 42/60\\] by \\[\\lambda_{42}=0.348\\times 2 \\pi = 2.1865 \\]. The frequencies neighbouring \\[ \\lambda_{42}\\] are set to \\[ \\lambda_{41}= 2.1865-1/60 \\] and \\[\\lambda_{43}= 2.1865+1/60\\]. The periodogram below illustrates the proximity of this trading day frequency \\[\\lambda_{42}\\] (red shade) and the frequency corresponding to 4 cycles per year \\[\\lambda_{40}=2.0944\\]. This proximity is precisely what poses the identification problems: the AR spectrum boils down to a smoothed version of the periodogram and the contribution of the of the trading day frequency may be obscured by the leakage resulting from the potential seasonal peak at \\[\\lambda_{40}\\], and vice-versa. Text Periodogram with seasonal (grey) and calendar (red) frequencies highlighted JDemetra+ allows the user to modify the number of lags of this estimator and to change the number of observations used to determine the AR parameters. These two options can improve the resolution of this estimator. artest 17.3 Autoregressive spectrum estimation BROCKWELL, P.J., and DAVIS, R.A. (2006) point out that for any real-valued stationary process \\(\\left\\(x_{t}\\right\\)\\) with continuous spectral density \\(f\\left\\(\\omega\\right\\)\\) it is possible to find both \\(AR(p)\\) and \\(MA(q)\\) processes which spectral densities are arbitrarily close to \\(f\\left\\(\\omega\\right)\\). For this reason, in some sense, \\(\\left\\(x_{t}\\right\\)\\) can be approximated by either \\(AR(p)\\) or \\(MA(q)\\) process. This fact is a basis of one of the methods of achieving a consistent estimator of the spectrum, which is called an autoregressive spectrum estimation. It is based on the approximation of the stochastic process \\(\\left\\(x_{t}\\right\\)\\) by an autoregressive process of sufficiently high order \\(p\\): \\[ x_{t} = \\mu + \\left( \\phi_{1}B + \\ldots + \\phi_{p}B^{p} \\right)x_{t} + \\varepsilon_{t} \\] where \\[\\varepsilon_{t}\\] is a white-noise variable with mean zero and a constant variance. The autoregressive spectrum estimator for the series \\(x_{t}\\) is defined as: 75 \\[ \\widehat{s}\\left( \\omega \\right) = 10\\operatorname{\\times}{\\log_{10}\\frac{\\sigma_{x}^{2}}{2\\pi{|1 - \\sum_{k = 1}^{p}{\\widehat{\\phi}}_{k}e^{- ik\\omega}|}^{2}}} \\] where: \\(\\omega\\)– frequency, \\(0 \\leq \\omega \\leq \\pi\\); \\(\\sigma_{x}^{2}\\) – the innovation variance of the sample residuals; \\[{\\widehat{\\phi}}_{k}\\] – \\(\\text{AR}\\left\\(k\\right\\)\\) coefficient estimates of the linear regression of \\(x_{t} - \\overline{x}\\) on \\(x_{t - k} - \\overline{x}\\), \\(1 \\leq k \\leq p\\). The autoregressive spectrum estimator is used in the visual spectral analysis tool for detecting significant peaks in the spectrum. The criterion of visual significance, implemented in JDemetra+, is based on the range \\({\\widehat{s}}^{\\max} - {\\widehat{s}}^{\\min}\\) of the \\(\\widehat{s}\\left( \\omega \\right)\\) values, where \\({\\widehat{s}}^{\\max} = \\max_{k}\\widehat{s}\\left( \\omega_{k} \\right)\\); \\({\\widehat{s}}^{\\min} = \\min_{k}\\widehat{s}\\left( \\omega_{k} \\right);\\) and \\(\\widehat{s}\\left( \\omega_{k} \\right)\\ \\)is \\(k^{\\text{th}}\\) value of autoregressive spectrum estimator. The particular value is considered to be visually significant if, at a trading day or at a seasonal frequency \\(\\omega_{k}\\) (other than the seasonal frequency \\(\\omega_{60} = \\pi\\)), \\(\\widehat{s}\\left( \\omega_{k} \\right)\\ \\)is above the median of the plotted values of \\(\\widehat{s}\\left( \\omega_{k} \\right)\\) and is larger than both neighbouring values \\(\\widehat{s}\\left( \\omega_{k - 1} \\right)\\) and \\(\\widehat{s}\\left( \\omega_{k + 1} \\right)\\) by at least \\(\\frac{6}{52}\\) times the range \\({\\widehat{s}}^{\\max} - {\\widehat{s}}^{\\min}\\). Following the suggestion of SOUKUP, R.J., and FINDLEY, D.F. (1999), JDemetra+ uses an autoregressive model spectral estimator of model order 30. This order yields high resolution of strong components, meaning peaks that are sharply defined in the plot of \\(\\widehat{s}\\left( \\omega \\right)\\) with 61 frequencies. The minimum number of observations needed to compute the spectrum is set to \\(n =\\) 80 for monthly data and to \\(n =\\) 60 for quarterly series while the maximum number of observations considered for the estimation is 121. Consequently, with these settings it is possible to identify up to 30 peaks in the plot of 61 frequencies. By choosing \\(\\omega_{k} = \\frac{\\text{πk}}{60}\\) for $k = $0,1,…,60 the density estimates are calculated at exact seasonal frequencies (1, 2, 3, 4, 5 and 6 cycles per year). The model order can also be selected based on the AIC criterion (in practice it is much lower than 30). A lower order produces the smoother spectrum, but the contrast between the spectral amplitudes at the trading day frequencies and neighbouring frequencies is weaker, and therefore not as suitable for automatic detection. SOUKUP, R.J., and FINDLEY, D.F. (1999) also explain that the periodogram can be used in the visual significance test as it has as good as those of the AR(30) spectrum abilities to detect trading day effect, but also has a greater false alarm rate76. 17.3.0.1 Use The test can be applied directly to any series by selecting the option Statistical Methods &gt;&gt; Seasonal Adjustment &gt;&gt; Tools &gt;&gt; Seasonality Tests. This is an example of how results are displayed for the case of a monthly series: JDemetra+ considers critical values for \\[ \\alpha=1\\%\\] (code “A”) and \\[ \\alpha=5\\%\\] (code “a”) at each one of the seasonal frequencies represented in the table below, e.g. frequencies \\(\\frac{\\pi}{6},\\ \\frac{\\pi}{3},\\ \\frac{\\pi}{2},\\ \\frac{2\\pi}{3}\\text{ and } \\frac{5\\pi}{6}\\ \\) corresponding to 1, 2, 3, 4, 5 and 6 cycles per year in this example, since we are dealing with monthly data. The codes “t” and “T” correpond to the so-called Tukey spectrum, so ignore them for the moment. The seasonal and trading day frequencies by time series frequency Number of months per full period Seasonal frequency Trading day frequency (radians) 12 \\(\\frac{\\pi}{6},\\frac{\\pi}{3},\\ \\frac{\\pi}{2},\\frac{2\\pi}{3},\\ \\frac{5\\pi}{6},\\ \\pi\\) \\(d\\), 2.714 6 \\(\\frac{\\pi}{3},\\frac{2\\pi}{3}\\), \\(\\pi\\) \\[d\\] 4 \\(\\frac{\\pi}{2}\\), \\(\\pi\\) \\(d\\), 1.292, 1.850, 2.128 3 \\[\\pi\\] \\[d\\] 2 \\[\\pi\\] \\[d\\] Currently, only seasonal frequencies are tested, but the program allows you to manually plot the AR spectrum and focus your attention on both seasonal and trading day frequencies. Agustin Maravall has conducted a simulation experiment to calculate \\[ CV(\\lambda_{42}) \\] (trading day frequency) and proposes to set for all \\[j\\] equal to the critical value associated to the trading frequency, but this is currently not part of the current automatic testing procedure of JDemetra+. 17.3.0.2 References Soukup, R.J., and D.F. Findley (1999) On the Spectrum Diagnosis used by X12-ARIMA to Indicate the Presence of Trading Day Effects After Modeling or Adjustment. In Proceedengs of the American Statistical Association. Business and Economic Statistics Section, 144-149, Alexandria, VA. 17.4 Tukey Spectrum definition The Tukey spectrum belongs to the class of lag-window estimators. A lag window estimator of the spectral density \\[ f(\\omega)=\\frac{1}{2\\pi}\\sum_{k&lt;-\\infty}^{\\infty}\\gamma(k)e^{i k \\omega} \\] is defined as follows: \\[ \\hat{f}_{L}(\\omega)=\\frac{1}{2\\pi}\\sum_{\\left| h \\right| \\leq r } w(h/r)\\hat{\\gamma}(h)e^{i h \\omega} \\] where \\[\\hat{\\gamma}(.) \\] is the sample autocovariance function, \\[w(.)\\] is the lag window, and \\[r\\] is the truncation lag. \\[\\left| w(x)\\right| \\] is always less than or equal to one, \\[w(0)=1\\] and \\[w(x)=0\\] for \\[\\left| x \\right| &gt; 1\\]. The simple idea behind this formula is to down-weight the autocovariance function for high lags where \\[\\hat{\\gamma}(h)\\] is more unreliable. This estimator requires choosing \\[r\\] as a function of the sample size such that \\[r/n \\rightarrow 0 \\] and \\[r\\rightarrow \\infty \\] when \\[ n \\rightarrow \\infty \\] . These conditions guarantee that the estimator converges to the true density. JDemetra+ implements the so-called Blackman-Tukey (or Tukey-Hanning) estimator, which is given by \\[w(h/r)=0.5(1+cos(\\pi h/r))\\] if \\[\\left| h/r \\right| \\leq 1\\] and \\[0\\] otherwise. The choice of large truncation lags \\[r\\] decreases the bias, of course, but it also increases the variance of the spectral estimate and decreases the bandwidth. JDemetra+ allows the user to modify all the parameters of this estimator, including the window function. 17.4.1 Theoretical spectrum of the ARIMA model link to graph display in GUI In the bottom part the panel the ARIMA model used by TRAMO is presented using symbolic notation \\((P,D,Q)(PB,DB,QB)\\). Estimated parameters’ coefficients (regular and seasonal AR and MA) are shown in closed form (i.e. using the backshift operator77 \\(B\\)). For each regular AR root (i.e. the solution of the characteristic equation) the argument and modulus are given. Text The details of ARIMA model used for modelling For each regular AR root the argument and modulus are also reported (if present, i.e. if \\(\\)) to inform to which time series component the regular roots would be assigned. BROCKWELL, P.J., and DAVIS, R.A. (2002).↩︎ For details see BROCKWELL, P.J., and DAVIS, R.A. (2006).↩︎ BOX, G.E.P., JENKINS, G.M., and REINSEL, G.C. (2007).↩︎ The proof is given in BROCKWELL, P.J., and DAVIS, R.A. (2006).↩︎ HAMILTON, J.D. (1994).↩︎ HAMILTON, J.D. (1994).↩︎ BROCKWELL, P.J., and DAVIS, R.A. (2002).↩︎ HAMILTON, J.D. (1994).↩︎ SOKUP, R.J., and FINDLEY, D. F. (1999). ## AR Spectrum definition↩︎ Definition from ‘X-12-ARIMA Reference Manual’ (2011).↩︎ The false alarm rate is defined as the fraction of the 50 replicates for which a visually significant spectral peak occurred at one of the trading day frequencies being considered in the designated output spectra (SOUKUP, R.J., and FINDLEY, D.F. (1999)).↩︎ GÓMEZ, V., and MARAVALL, A. (2001b).↩︎ "],["reg-arima-models.html", "Chapter 18 Reg-Arima models 18.1 Overview 18.2 RegARIMA model 18.3 Autocorrelation function 18.4 Estimation Of Arma models 18.5 Maximum likelihood estimation 18.6 Handling of missing observations in (Reg)ARIMA models 18.7 Tests on residuals", " Chapter 18 Reg-Arima models 18.1 Overview 18.2 RegARIMA model old page might be mixed, objective * in champ 4 on SA, inte pre-adj section: priciples, purpose, results * here all the technical non algo specific details * differences (none left ?) between Ts and ReGarima The primary aim of seasonal adjustment is to remove the unobservable seasonal component from the observed series. The decomposition routines implemented in the seasonal adjustment methods make specific assumptions concerning the input series. One of the crucial assumptions is that the input series is stochastic, i.e. it is clean of deterministic effects. Another important limitation derives from the symmetric linear filter used in TRAMO-SEATS and X-13ARIMA-SEATS. A symmetric linear filter cannot be applied to the first and last observations with the same set of weights as for the central observations78. Therefore, for the most recent observations these filters provide estimates that are subject to revisions. To overcome these constrains both seasonal adjustment methods discussed here include a modelling step that aims to analyse the time series development and provide a better input for decomposition purposes. The tool that is frequently used for this purpose is the ARIMA model, as discussed by BOX, G.E.P., and JENKINS, G.M. (1970). However, time series are often affected by the outliers, other deterministic effects and missing observations. The presence of these effects is not in line with the ARIMA model assumptions. The presence of outliers and other deterministic effects impede the identification of an optimal ARIMA model due to the important bias in the estimation of parameters of sample autocorrelation functions (both global and partial)79. Therefore, the original series need to be corrected for any deterministic effects and missing observations. This process is called linearisation and results in the stochastic series that can be modelled by ARIMA. For this purpose both TRAMO and RegARIMA use regression models with ARIMA errors. With these models TRAMO and RegARIMA also produce forecasts. \\(z_{t} = y_{t}\\mathbf{\\beta} + x_{t}\\), \\[1\\] where: \\(z_{t}\\) is the original series; \\(\\mathbf{\\beta} = (\\beta_{1},\\ldots,\\beta_{n})\\) – a vector of regression coefficients; \\(y_{t} = (y_{1t},\\ldots,y_{\\text{nt}})\\) – \\(\\text{n}\\) regression variables (the trading day variables, the leap year effect, outliers, the Easter effect, ramps, intervention variables, user-defined variables); \\(x_{t}\\ \\)– a disturbance that follows the general ARIMA process: \\(\\phi\\left( B \\right)\\delta\\left( B \\right)x_{t} = \\theta(B)a_{t}\\); \\(\\phi\\left( B \\right)\\),\\(\\ \\theta(B)\\) and \\(\\delta\\left( B \\right)\\) are the finite polynomials in \\(B\\); \\(a_{t}\\) is a white-noise variable with zero mean and a constant variance. The polynomial \\(\\phi\\left( B \\right)\\) is a stationary autoregressive (AR) polynomial in \\(B\\), which is a product of the stationary regular AR polynomial in \\(B\\) and the stationary seasonal polynomial in \\(\\text{B}^{s}\\):80 \\[\\phi\\left( B \\right) = \\phi_{p}\\left( B \\right)\\Phi_{p_{s}}\\left( B^{s} \\right) = (1 + \\phi_{1}B + \\ldots + \\phi_{p}B^{p})(1 + \\Phi_{1}B^{s} + \\ldots + \\Phi_{p_{s}}B^{p_{s}s})\\] \\[2\\] where: \\(\\text{p}\\) – the number of regular AR terms, (in JDemetra+\\(\\ \\ p \\leq 3)\\); \\(p_{s}\\) – the number of seasonal AR terms, (in JDemetra+ \\(p_{s} \\leq 1)\\); \\(\\text{s}\\) – the number of observations per year (frequency of the time series). The polynomial \\(\\theta(B)\\) is an invertible moving average (MA) polynomial in \\(B\\), which is a product of the invertible regular MA polynomial in \\(B\\) and the invertible seasonal MA polynomial in \\(\\text{B}\\): \\(\\theta\\left( B \\right) = \\theta_{q}\\left( B \\right)\\Theta_{q_{s}}\\left( B^{s} \\right) = (1 + \\theta_{1}B + \\ldots + \\theta_{q}B^{q})(1 + \\Theta_{1}B^{s} + \\ldots + \\Theta_{q_{s}}B^{q_{s}s})\\), \\[3\\] where: \\(q\\) – the number of regular MA terms, (\\(q \\leq 3)\\); \\(q_{s}\\) – the number of seasonal MA terms, (\\(q_{s} \\leq 1)\\). An MA polynomial (1+\\(\\theta_{1}B + \\ldots + \\theta_{q}B^{q})\\ \\)is invertible if all its roots lie outside the unit circle, i.e. their modulus is greater than 1. When invertible an MA process, \\(x_{t} = \\theta\\left( B \\right)a_{t},\\) can be expressed as An AR process of infinite order, \\(\\pi\\left( B \\right)x_{t} = a_{t}\\), where , \\(\\pi\\left( B \\right) = \\pi_{0} + \\pi_{1}B + \\ldots\\) 81 The polynomial \\(\\delta\\left( B \\right)\\) is the non-stationary AR polynomial in \\(B\\) (unit roots): \\(\\delta\\left( B \\right) = {(1 - B)}^{d}{(1 - B^{s})}^{d_{s}}\\), \\[4\\] where: \\(d\\) – regular differencing order, (\\(d \\leq 1)\\); \\(d_{s}\\ \\)– seasonal differencing order, (\\(d_{s} \\leq 1)\\). JDemetra+ uses notation: \\(P,\\ D,\\ Q,BP,\\ BD,\\ BQ\\) instead of, respectively:\\(\\ p,\\ d,q,p_{s},\\ d_{s},\\ q_{s}\\). Therefore, the structure of the ARIMA\\(\\ (p,\\ d,\\ q)(P,\\ D,\\ Q)\\) model is denoted in JDemetra+ as ARIMA \\((P,\\ D,\\ Q)(BP,\\ BD,\\ BQ)\\). Both TRAMO and X-12-ARIMA allows for an automatic identification of an ARIMA model extended for the regression variables. The procedure includes a test for logarithmic transformation (so-called the test for the log-level specification), selection of the ARIMA model structure and regressors. The estimated deterministic effects are removed from the time series to improve the estimation of the time series components. Forecasts produced by the ARIMA model provide an input for the linear filters used in the decomposition step. In summary, the application of an ARIMA model to the original series vastly improves the quality and stability of the estimated components. The details of the estimation procedure of the ARIMA model with regression variables are presented in the later in this section. Among the deterministic effects one can distinguish between the calendar effects and outliers. The calendar effects are discussed in the Calendar effects section. The impact of different types of outliers on time series is illustrated with several examples by FRANSES, P.H. (1998) For example, it was explained that additive outliers, which are described later in this section, yield large values of skewness and kurtosis, and hence failure of a normality test. They also increase the standard error of the estimation parameters. This effect is especially prominent when the size of the outlier is considerable. KAISER, R., and MARAVALL, A. (1999) express the impact of the outliers on the observed series as:82 \\(y_{t}^{*} = \\sum_{j = 1}^{k}\\xi_{j}\\left( B \\right)\\omega_{j}I_{t}^{\\left( \\tau_{j} \\right)} + y_{t}\\), \\[5\\] where: \\(y_{t}^{*}\\) – an observed time series; \\(y_{t}\\) – a series that follows the ARIMA model; \\(\\omega_{j}\\) – an initial impact of the outlier at time \\({t = \\tau}_{j}\\); \\(I_{t}^{\\left( \\tau_{j} \\right)}\\) – an indicator variable such that is 1 for \\({t = \\tau}_{j}\\) and 0 otherwise; \\(\\xi_{j}\\left( B \\right)\\) – an expression that determines the dynamics of the outlier occurring at time \\({t = \\tau}_{j}\\); \\(B\\) – a backshift operator (i.e. \\(B^{k}X_{t} = X_{t - k}\\)). The optimal choice of regression variables (and/or intervention variables) requires knowledge from the user about the time series being modelled83. On the contrary, outliers, frequently used in modelling seasonal economic time series, can be automatically detected by JDemetra+ (see Specifiations - TRAMO section for TRAMO and Specifiations - ARIMA section for RegARIMA). The procedure described in GÓMEZ, V., and MARAVALL, A. (2001a) identifies the ARIMA model structure in the presence of deterministic effects. Therefore, the number of identified outliers may depend on the ARIMA model estimated on the series. The types of outliers that can be automatically identified and estimated by JDemetra+ without any user intervention are: Additive Outlier (AO) – a point outlier which occurs at a given time \\(t_0\\). For the additive outlier \\(\\xi_{j}( B) = 1\\), which results in the regression variable: \\[ AO_{t}^{t_{0}} = \\begin{cases} 1 \\text{ for } t = t_{0} \\\\\\\\ 0 \\text{ for } t \\neq t_{0} \\end{cases} \\]; \\[6\\] Level shift (LS) – a variable for a constant level shift beginning at the given time \\(t_{0}\\). For the level shift \\(\\xi_{j}\\left( B \\right) = \\frac{1}{1 - B}\\), which results in the regression variable: \\[ LS_{t}^{t_{0}} = \\begin{cases} -1 \\text{ for } t &lt; t_{0} \\\\\\\\ 0 \\text{ for } t \\geq t_{0} \\end{cases} \\]; \\[7\\] Temporary change84 (TC) – a variable for an effect on the given time \\(t_{0}\\) that decays exponentially over the following periods. For the temporary change \\(\\xi_{j}\\left( B \\right) = \\frac{1}{1 - \\delta B}\\), which results in the regression variable: \\[ TC_{t}^{t_{0}} = \\begin{cases} 0 \\text{ for } t &lt; t_{0} \\\\\\\\ \\alpha^{t - t_{0}} \\text{ for } t \\geq t_{0} \\end{cases} \\]; \\[8\\] where \\(\\alpha\\) is a rate of decay back to the previous level \\((0 &lt; \\alpha &lt; 1)\\). Seasonal outliers (SO) – a variable that captures an abrupt change in the seasonal pattern on the given date \\(t_{0}\\ \\)and maintains the level of the series with a contrasting change spread over the remaining periods. It is modelled by the regression variable: \\[ SO_{t}^{t_{0}} = \\begin{cases} 0 \\text{ for } t &lt; t_{0} \\\\\\\\ 1 \\text{ for } t \\geq t_{0}, \\text{$t$ same month/quarter as $t_{0}$} \\\\\\\\ \\frac{- 1}{(s - 1)} \\text{ otherwise} \\end{cases} \\]; \\[9\\] where \\(s\\) is a frequency of the time series (\\(s = 12\\ \\)for a monthly time series, \\(s = 4\\ \\)for a quarterly time series). The shapes generated by the formulas given above are presented in the figure below. Text Pre-defined outliers built in to JDemetra+ Within the RegARIMA model it can be also tested if a series of level shifts cancels out to form a temporary level change effect, which is a permanent level shift spanned between two given dates. It is modelled by the regression variable: \\[ TLS_{t}^{ {(t}_{0}{,\\ t}_{1})} = \\begin{cases} 0 \\text{ for } t &lt; t_{0} \\\\\\\\ 1 \\text{ for } t_{1} \\geq t \\geq t_{0} \\\\\\\\ 0 \\text{ for } t &gt; t_{1} \\end{cases} \\]; \\[10\\] JDemetra+ also identifies other pre-defined regression variables, for which a necessary input, such as location and process that generates the variable, is provided by the user. This group includes: Ramp – a variable for a linear increase or decrease in the level of the series over a specified time interval \\(t_{0}\\) to\\(\\ t_{1}\\). It is modelled by a regression variable: \\[ RP_{t}^{ {(t}_{0}{,\\ t}_{1})} = \\begin{cases} -1 \\text{ for } t \\leq t_{0} \\\\\\\\ -\\frac{t - t_{0}}{t_{1} - t_{0}} - 1 \\text{ for } t_{0} &lt; t &lt; t_{1} \\\\\\\\ 0 \\text{ for } t \\geq t_{1} \\end{cases} \\]; \\[11\\] Intervention variables which are combinations of five basic structures85: dummy variables86; any possible sequence of ones and zeros; \\(\\frac{1}{(1 - \\delta B)}\\), \\((0 &lt; \\delta \\leq 1)\\); \\(\\frac{1}{(1 - \\delta_{s}B^{s})}\\), \\((0 &lt; \\delta_{s} \\leq 1)\\); \\(\\frac{1}{(1 - B)(1 - B^{s})}\\); where \\(s\\) is frequency of the time series (\\(s = 12\\ \\)for a monthly time series, \\(s = 4\\ \\)for a quarterly time series). The structures considered by intervention variables allow for generation of all pre-defined outliers described by \\[6\\] – \\[11\\], as well as some more sophisticated effects. An example can be a level shift effect reached after a sequence of damped overshootings and undershootings, presented in the figure below and denoted there as IV. Another example of an outlier that can be created with intervention variables is a pure seasonal outlier (\\(PSO)\\), which, in contrast to the seasonal outlier described above, does not affect the trend. The set of pure seasonal outliers is used to model the seasonal change of regime (\\(\\text{SCR}\\)) effect, which describes a sudden and sustained change in the seasonal pattern affecting from \\(t_{0}\\) (possibly) all seasons of the series. It is defined as: \\[ SCR_{i,j} = \\begin{cases} 1 \\text{ for } t \\epsilon j \\text{ and } t &lt; t_{0} \\\\\\\\ 0 \\text{ for } t \\notin j \\text{ or } t \\geq t_{0} \\\\\\\\ -1 \\text{ for } t \\epsilon s \\text{ and } t &lt; t_{0} \\end{cases} \\]; \\[12\\] where \\(j = 1,\\ldots s - 1\\). Text Examples of intervention variables 18.2.0.1 Automatic model identification procedure in TRAMO An algorithm for Automatic Model Identification in the Presence of Outliers (AMI) implemented in TRAMO is based on TSAY, C. (1986) and CHEN, B.-C., and LIU, L.-M. (1993) with some modifications (see GÓMEZ, V., and MARAVALL, A. (2001)). It iterates between two stages: the first is automatic outlier detection and correction and the second automatic model identification. The parameters of the AMI procedure are described in the Specifiations - ARIMA section. Unless the parameters are set by the user the program runs with the default values. The algorithm starts with the identification of the default model and pre-testing procedure, where on the first step a test for a log-level specification is performed. It is based on the maximum likelihood estimation of the parameter \\(\\lambda\\) in the Box-Cox transformation, which is a power transformation such that the transformed values of the time series \\(y\\ \\)are a monotonic function of the observations, i.e. \\[ y^{\\alpha} = \\begin{cases} \\frac{\\left( y_{i}^{\\alpha} - 1 \\right)}{\\lambda}, \\lambda \\neq 0 \\\\\\\\ \\log{y_{i}^{\\alpha}, \\lambda = 0} \\end{cases} \\]; First, two Airline models (i.e. ARIMA(0,1,1)(0,1,1)) with a mean) are fitted to the time series: one in logs (\\(\\lambda = 0\\)), other without logs (\\(\\lambda = 1\\)). The test compares the sum of squares of the model without logs with the sum of squares multiplied by the square of the geometric mean of the (regularly and seasonally) differenced series in the case of the model in logs. Logs are taken in the case this last function is the minimum87. By default, both TRAMO and X-12-ARIMA have a slight bias towards the log transformation. Next, the test for calendar effects is performed with regressions using the default model for the noise and, if the model is subsequently changed, the test is redone. For seasonal series the default model is the Airline model (ARIMA (0,1,1)(0,1,1)) while for the non-seasonal series ARIMA (0,1,1)(0,0,0) with a constant term is used. The default model, which is used as a benchmark model at some next steps of AMI, is determined by the result of the pre-test for possible presence of seasonality. Once these pre-tests have been completed, the original series is corrected for all pre-specified outliers and regression effects provided by the user, if any. Next, the order of the differencing polynomial \\(\\text{δ}\\left( B \\right)\\)­ that contains the unit roots is identified and it is decided whether to specify a mean for the series or not. The identification of the ARMA model, i.e. the order of \\(\\phi\\left( B \\right)\\ \\)and \\(\\text{θ}\\left( B \\right)\\ \\) ­is performed using the Hannan and Rissanen method by the means of minimising the Bayesian information criterion with some constraints aimed at increasing the parsimony and favouring balanced models88. This procedure produces the initial values of the ARIMA model parameters. The search is done sequentially: for the fixed regular polynomials, the seasonal ones are obtained, and vice versa. When the estimated roots of the AR and MA processes are close to each other, the order of the ARIMA model can be reduced. The identification and estimation of the model is carried out using Exact Maximum Likelihood (EML) or the Unconditional Least Squares method. If the calendar effects were identified in the default model, they are included in a new ARIMA model provided that these effects are significant for this model. The estimated residuals from the modified ARIMA model with fixed parameters and the median absolute deviation of the standard deviation of these residuals are used in the outlier detection procedure. For each observation, \\(t\\)-tests are computed for all types of outlier considered in the automatic procedure (AO, TC, LS, SO), following the procedure proposed in CHEN, B.-C., and LIU, L.-M. (1993). The program compares\\(\\text{t}\\)-statistics to a critical value determined by the series length. If there are outliers, for which the absolute \\(t\\)-values are greater than a critical value, the one with the greatest absolute\\(\\text{t}\\)-value is selected. After correcting for the identified outlier, the process is started again to test if there is another outlier. The procedure is repeated until for none of the potential outliers, the \\(t\\)-statistic exceeds the critical value. If outliers are detected, then a multiple regression is performed using the Kalman filter and the QR algorithm to avoid (as much as possible) masking effects (i.e. detecting spurious outliers) and to correct for the bias produced in the estimators sequentially obtained89. If there are outliers for which the absolute \\(t\\)-values are greater than the critical value, the one with the greatest absolute \\(t\\)-value is selected and the algorithm continues to the estimation of the ARMA model parameters. Otherwise, the algorithm stops90. The estimated residuals from the final ARIMA model are checked for adequacy against the estimated residuals produced by the balanced model. The final model identified by the AMI procedure must show some improvement over the default model in these residual diagnostics; otherwise, the program will accept the default model91. 18.2.0.2 Automatic model identification procedure in RegARIMA The original RegARIMA algorithm developed by the U.S. Census Bureau includes two automatic model selection procedures: automdl that is based on TRAMO and pickmdl that originates from X-11-ARIMA92. The algorithm implementation in JDemetra+ for RegARIMA follows the TRAMO logic. It is very similar to the TRAMO procedure presented in the previous section, but contains modifications to make use of the X-13ARIMA-SEATS estimation procedure, which is different from the one that TRAMO uses93. The examples of extensions that are specific to RegARIMA only are: special treatment of the leap year effect in the multiplicative model, automatic detection of the length of the Easter effect94, option to reduce a series of level shifts to the temporary level shift. In comparison with TRAMO, there are also differences in the values of the default parameters. Besides, by default, RegARIMA does not favour balanced models. The log/level test in RegARIMA is based on the Corrected Akaike Information Criterion (AICC). Similarly, the estimation of calendar effects is based on AICC. As a result, the model selected by RegARIMA can differ from the model that TRAMO would select, especially in the case of series contaminated with some deterministic effects and/or those, which are modelled with the mixed ARIMA models. 18.2.0.3 Model selection criteria Model selection criteria are statistical tools for selecting the optimal order of the ARIMA model. The basic idea behind all these criteria is to obtain as much explanatory power (measured by the value of the likelihood function) with only a few parameters. The model selection criteria essentially choose the model with the best fit, as measured by the likelihood function, and it is a subject to a penalty term, to prevent over-fitting that increases with the number of parameters in the model.95 Some of the most well-known information criteria are: Akaike Information Criterion (AIC)96, Corrected Akaike Information Criterion (AICC)97, HQ Information Criterion (HannanQuinn)98 and Schwarz-Bayes Information Criterion (BIC)99. The formulae for the model selection criteria used by JDemetra+ are: \\[ AIC_{N} = -2L_{N} + {2n}_{p} \\] \\[13\\] \\[ AICC_{N} = - 2L_{N} + 2n_{p}\\left( 1 - \\frac{n_{p} + 1}{N} \\right)^{- 1} \\] \\[14\\] \\[ HQ_{N} = - 2L_{N} + 2n_{p}\\text{ln(ln(N))} \\] \\[15\\] \\[ BIC_{N} = - 2L_{N} + n_{p}\\text{logN} \\] \\[16\\] where: \\(\\text{N}\\)– a number of observations in time series; \\(n_{p}\\) – a number of estimate parameters; \\(L_{N}\\) – a loglikelihood function. For each model selection criteria the model with the smaller value is preferred. As it can be shown that AIC is biased for small samples, it is often replaced by AICC. To choose the ARIMA model parameters the original RegARIMA uses AICC while TRAMO uses BIC with some constraints aimed at increasing the parsimony and favouring balanced models. The automatic model identification methods implemented in JDemetra+ mostly use BIC, however other criteria are used as well. It should be noted that the BIC criterion imposes a greater penalty term than AIC and HQ, so that BIC tends to select simpler models than those chosen by AIC. The difference between both criteria can be huge if \\(N\\) is large100. 18.2.0.4 Hannan-Rissanen algorithm The Hannan-Rissanen algorithm101 is a penalty function method based on the BIC criterion, where the estimates of the ARMA model parameters are computed by means of a linear regression. The Hannan-Rissanen procedure operates on a stationary transformation of the original series, therefore only AR and MA parameters are being identified. In the first step, a high-order\\(\\ AR(m)\\), where \\(m &gt; max(p,q)\\), model is fitted to the time series \\(X_{t}\\). The maximum order of \\(p\\) and \\(q\\) is given a-priori. In JDemetra+ it is equal to 3 for both \\(p\\) and \\(q\\). The residuals \\[\\widehat{a}_{k}\\] from this model are used to provide estimates of the innovations in the ARMA model \\(a_{t}\\): \\[a_{t} = X_{t} - \\sum_{k = 1}^{m}{\\widehat{a}_{k}X}_{t - k}\\] \\[17\\] In the second step the parameters \\(p\\) and \\(q\\) of the ARMA model are estimated using a least squares linear regression of \\(X_{t}\\) onto \\(X_{t - 1},\\ldots X_{t - p},a_{t - 1},\\ldots a_{t - q}\\) for a combination of values \\(p\\) and \\(q\\). The first step of the procedure is skipped if the ARMA model fitted in the second step includes only an autoregressive part. Finally, the Hannan-Rissanen algorithm selects a pair of \\(p\\) and \\(q\\) values for which \\(BIC_{p,q}\\) is the smallest. \\(BIC_{p,q}\\) is defined as: \\[\\text{BIC}_{p,q} = \\log\\left( \\sigma_{p,q}^{2} \\right) + \\frac{\\left( p + q \\right)\\log\\left( n - d \\right)}{n - d}\\] \\[18\\] where: \\(\\sigma_{p,q}^{2}\\) is the maximum likelihood estimator of \\(\\sigma^{2}\\); \\(n - d\\ \\)is the number of series in the (regularly and/or) seasonally differenced series. The advantage of the Hannan-Rissanen algorithm is the speed of computation in comparison with an exact likelihood estimation. 18.2.0.5 Initial values for ARIMA model estimation By default, the initial parameter value in X-13ARIMA-SEATS is 0.1 for all AR and MA parameters. For the majority of time series this default value seems to be appropriate. Introducing better initial values (as might be obtained, e.g., by first fitting the model using conditional likelihood) could slightly speed up a convergence. Users are allowed to introduce manually initial values for AR and MA parameters that are then used to start the iterative likelihood maximization. This is rarely necessary, and, in general, not recommended. A possible exception to this occurs if the initial estimates that are likely to be extremely accurate are already available, such as when one is re-estimating a model with a small amount of new data added to a time series. However, the main reason for specifying initial parameter values is to deal with convergence problems that may arise in difficult estimation situations102. 18.2.0.6 Cancellation of AR and MA factors A cancellation issue consists in cancelling some factors on both sides of the ARIMA model. This problem concerns the mixed ARIMA \\((p,d,q)(P,D,Q)\\) models (i.e. \\(p &gt; 0\\ \\)and\\(\\ q &gt; 0\\), or \\(P &gt; 0\\) and \\(Q &gt; 0\\)). For example, a cancellation problem occurs with ARMA (1,1) model, \\(\\left( 1 - \\phi B \\right)z_{t} = (1 - \\theta B)a_{t}\\) when \\(\\phi = \\theta\\ \\)as then model is simply of the form: \\(z_{t} = a_{t}\\). Such model causes problems with convergence of the nonlinear estimation. For this reason the X-13ARIMA-SEATS and TRAMO-SEATS programs deal with a cancellation problem by computing zeros of the AR and MA polynomials. As the cancellation does not need to be exact, the cancellation limit can be provided by the user103. 18.2.0.7 Least squares estimation by means of the QR decomposition We consider the regression model: \\(y = X\\beta + \\varepsilon\\) \\[19\\] The least squares problem consists in minimizing the quantity \\(\\left\\| X\\beta - y \\right\\|^{2}\\). Provided that the regression variables are independent, it is possible to find an orthogonal matrix \\(Q\\), so that \\(Q \\bullet X = \\left( \\frac{R}{0} \\right)\\) where \\(R\\) is upper triangular. We have now to minimize: \\[\\left\\| QX\\beta - Qy \\right\\|_{2}^{2} = \\left\\| \\left( \\frac{R}{0} \\right) \\ \\beta - Qy \\right\\|_{2}^{2} = \\left\\| R\\beta - a \\right\\|_{2}^{2} + \\left\\| b \\right\\|_{2}^{2}\\] \\[20\\] where \\(Qy_{0\\ldots x - 1} = a\\) and \\(Qy_{x\\ldots n - 1} = b\\). The minimum of the previous norm is obtained by setting \\(\\beta = R^{- 1}a\\). In that case \\(\\left\\| R\\ \\beta - a \\right\\|_{2}^{2} = 0\\). The residuals obtained by this procedure are then \\(b\\), as defined above. It should be noted that the \\(\\text{QR}\\) factorization is not unique, and that the final residuals also depend on the order of the regression variables (the columns of \\(X\\)). 18.3 Autocorrelation function The correlation is a measure of the strength and the direction of a linear relationship between two variables. For time series the correlation can refer to the relation between its observations, e.g. between the current observation and the observation lagged by a given number of units. In this case all observations come from one variable, so similarity between a given time series and a \\(k\\)-lagged version of itself over successive time intervals is called an autocorrelation. The autocorrelation coefficient at lag \\(k\\) is defined as: \\[\\rho\\left( k \\right) = \\frac{\\sum_{t = k + 1}^{n}\\left( x_{t} - \\overline{x} \\right)}{\\sum_{t = 1}^{n}\\left( x_{t} - \\overline{x} \\right)^{2}} \\], \\[1\\] where: \\(x_{t}\\) – time series; \\(n\\) – total number of observations; \\(\\overline{x}\\) – mean of the time series. The set of autocorrelation coefficients \\((k)\\) arranged as a function of \\(k\\) is the autocorrelation function (ACF). The graphical or numerical representation of the ACF is called an autocorrelogram. Text Autocorrelation function The autocorrelation function is a valuable tool for investigating properties of an empirical time series.104 The assessment of the order of an AR process simply from the sample ACF is not straightforward. While for a first-order process the theoretical ACF decreases exponentially and the sample function is expected to have the similar shape, for the higher-order processes the ACF maybe a mixture of damper exponential or sinusoidal functions, which makes the order of the AR process difficult to identify.105 JDemetra+ displays the values of autocorrelation function for the residuals from the ARIMA model (see section Residuals). The ACF graph (figure above), presents autocorrelation coefficients and the confidence intervals. If the autocorrelation coefficient is in the confidence interval, it is regarded as not statistically significant. Therefore, the user should focus on the values where the value of the ACF is outside the confidence interval. In JDemetra+ the confidence interval is indicated by two grey, horizontal, dotted lines. Partial autocorrelation function The partial autocorrelation is a tool for the identification and estimation of the ARIMA model. It is defined as the amount of correlation between two variables that is not explained by their mutual correlations with a given set of other variables. Partial autocorrelation at lag \\(k\\) is defined as the autocorrelation between \\(x_{t}\\) and \\(x_{t - k}\\) that is not accounted for by lags 1 through to \\(k\\)-1, which means that correlations with all the elements up to lag \\(k\\) are removed. Following this definition, a partial autocorrelation for lag 1 is equivalent to an autocorrelation. The partial autocorrelation function (PACF) is the set of partial autocorrelation coefficients \\((k)\\) arranged as a function of \\(k\\). This function can be used to detect the presence of an autoregressive process in time series and identify the order of this process. Theoretically, the number of significant lags determines the order of the autoregressive process. Text Partial autocorrelation function The PACF graph above, which is available from the Tools\\(\\ \\rightarrow \\ \\)Differencing menu presents partial autocorrelation coefficients and the confidence intervals (two grey, horizontal, dotted lines). If the partial autocorrelation coefficient is in the confidence interval, it is regarded as statistically insignificant. Therefore, the user should focus on the values, for which the absolute value of the PACF is outside the confidence interval. 18.4 Estimation Of Arma models The computation of exact likelihood requires the evaluation of two main quantities: the determinant of the covariance matrix and the sum of the squared residuals. The different algorithms for the computation of the likelihood of ARMA models provides efficient solutions for those two problems. The quantity \\[ y&#39; \\Omega^-1 y \\] is computed by defining a linear transformation of the observations such that \\[ y&#39; \\Omega^-1 y = \\left(y&#39; T&#39; \\right) \\left(T y \\right) \\] An obvious solution will be the use of the inverse of the Cholesky factor of the covariance matrix. However, any transformation \\[ T\\sim m \\times n \\] such that \\[ \\Omega^-1 = T&#39; T \\] might be considered. Note that m can be larger than n (which means that the transformed observations will not be independent). JD+ provides several routines for estimating the exact likelihood of ARMA models Algorithm Use Kalman filter Default Ansley Large regression models X12 Legacy Ljung-Box Deprecated 18.5 Maximum likelihood estimation 18.5.1 Likelihood of a multivariate normal distribution The pdf of a multivariate normal distribution is: \\[p\\left( y \\right) = \\left( 2 \\pi \\right)^{-\\frac{n}{2}} \\vert \\Sigma \\vert ^{-\\frac{1}{2}}e^{ {-\\frac{1}{2}y&#39; \\Sigma ^{-1} y} } \\] If we set \\[ y&#39; \\Sigma ^{-1} y=u&#39;u \\:\\: or \\:\\: L^{-1}y = u \\] the log-likelihood is: \\[ l \\left( \\theta | y \\right) =- \\frac{1}{2} \\left(n \\log{2 \\pi}+ \\log{|\\Sigma |} +u&#39;u\\right) \\] In most cases, we will use a covariance matrix with a (unknown) scaling factor: \\[ \\Sigma = \\sigma^2 \\Omega \\] If we set \\[ L^{-1}y = e , \\quad LL&#39; = \\Omega\\] the log-likelihood can then be written: \\[ l \\left(\\theta, \\sigma | y \\right ) = - \\frac{1}{2} \\left(n \\log{2 \\pi}+ n \\log {\\sigma^2} + \\log{|\\Omega |} + \\frac{1}{\\sigma ^2} e&#39;e \\right) \\] The scaling factor can be concentrated out of the likelihood. Its estimator is \\[ \\hat{\\sigma} ^2 = \\frac{e&#39; e}{n} \\] so that : \\[ l_c \\left( \\theta | y \\right ) = - \\frac{1}{2} \\left(n \\log{2 \\pi} + n\\log{\\frac{e&#39;e}{n}} + \\log{|\\Omega |} + n \\right) \\] or \\[ l_c \\left( \\theta | y \\right ) = - \\frac{n}{2} \\left(\\log{2 \\pi}+ 1 - \\log {n} + \\log{e&#39;e} + \\log{|\\Omega |^\\frac{1}{n}}\\right) \\] Maximizing $ l_c $ is equivalent to minimizing the deviance \\[ d \\left( y | \\theta\\right ) = e&#39;e |\\Omega |^\\frac{1}{n} = v&#39;v, \\quad where \\quad v = \\sqrt{ |\\Omega |^\\frac{1}{n} }\\: e\\] This last formulation will be used in optimization procedures based on sums of squares (Levenberg-Marquardt and similar algorithms). 18.5.2 Linear model with gaussian noises The likelihood is often computed on a linear model \\[ y=X\\beta + \\mu \\quad \\mu \\sim N\\left(0, \\sigma^2\\Omega\\right) \\] The log-likelihood is then \\[ l \\left(\\theta,\\beta , \\sigma | y \\right ) = - \\frac{1}{2} \\left(n \\log{2 \\pi}+ n \\log {\\sigma^2} + \\log{|\\Omega |} + \\frac{1}{\\sigma ^2} \\left(y-X\\beta \\right)&#39;\\Omega^{-1}\\left(y-X\\beta \\right) \\right) \\] The maximum likelihood estimator of \\(\\beta\\) is \\[ \\hat{\\beta} = \\left( X&#39;\\Omega^{-1}X\\right)^{-1}X&#39;\\Omega^{-1}y \\] which is normally distributed with variance \\[ \\sigma^2 \\left( X&#39;\\Omega^{-1}X\\right)^{-1} \\] The formulae of the likelihood are still valid, using \\[ e=L^{-1} \\left(y-X\\hat\\beta \\right) \\] 18.5.3 Implementation Those representations of the concentrated likelihood are defined in the interfaces demetra.likelihood.ILikelihood and demetra.likelihood.IConcentratedLikelihood 18.5.3.1 Correspondance between the elements of the likelihood (see formulae) and the methods of the classes \\(n\\) : dim() \\(e&#39;e\\) : ssq() \\(e\\) : e() \\(\\log \\|\\Omega\\|\\) : logDeterminant() \\(v\\) : v() \\(\\|\\Omega\\|^{\\frac{1}{n}}\\) : factor() 18.5.4 Missing values Missing values are not taken into account in the likelihood. More especially, when they are estimated by means of additive outliers, all the different elements of the likelihood (dimension, determinantal term, coefficients…) should be adjusted to remove their effect. 18.5.5 Perfect collinearity in X In the case of perfect collinearity in the linear model, the dimensions of the coefficients and of the related matrices are not modified. However, information related to the redundant variables is set to 0. 18.5.6 References Gomez V. and Maravall A. (1994): “Estimation, Prediction, and Interpolation for Nonstationary Series With the Kalman Filter”, Journal of the American Statistical Association, vol. 89, n° 426, 611-624. 18.5.7 X-13 implementation Estimation of the exact likelihood of an ARMA model We suppose that \\[ y_t, \\quad 1 \\le t \\le n \\] follows an ARMA model. The X12 implementation computes the exact likelihood in two main steps 18.5.7.1 Overview We consider the transformation \\[ z_t = \\begin{pmatrix} z_{1t} \\\\ z_{2t} \\end{pmatrix} = \\begin{cases} y_t, &amp; 1 \\le t \\le p \\\\ \\Phi\\left(B\\right) y_t, &amp; p \\lt t \\le n\\end{cases}\\] It is obvious that \\[ p\\left(y_t\\right) = p\\left(z_t\\right) \\] We will estimate \\[ p\\left(z_t\\right) = p\\left(z_{2t}\\right) p\\left(z_{1t}\\right | z_{2t} )\\] 18.5.7.2 Step 1: likelihood of a pure moving average process \\[z_{2t}\\] is a pure moving average process. Its exact likelihood is estimated as follows (see [1] for details). We list below the different steps of the algorithm. Compute the conditional least squares residuals by the recursion: \\[ a_{0t} = \\begin{cases} 0,&amp; -q \\lt t \\leq 0 \\\\ z_t-\\theta_1 a_{0t-1}- \\cdots --\\theta_q a_{0t-q},&amp; 0 \\lt t \\leq n \\end{cases} \\] Compute the Pi-Weights of the model. They defines the (n+q x q) matrix \\[ G = \\begin{pmatrix} 1 &amp; 0 &amp; \\cdots &amp; 0 \\\\ \\pi_1 &amp; 1 &amp; \\cdots &amp; 0 \\\\ \\pi_2 &amp; \\pi_1 &amp; \\cdots &amp; \\vdots\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ \\pi_{n+q-1} &amp; \\pi_{n+q-2} &amp; \\cdots &amp; \\pi_{n} \\end{pmatrix} \\] Compute by recursion \\[ G&#39;a \\quad and \\quad G&#39;G \\] Compute by Cholesky decomposition \\[ G&#39;G \\hat z_* = G&#39;a \\quad and \\quad |G&#39;G| \\] Obtain by recursion the exact likelihood residuals \\[ \\Theta\\left(B\\right)\\begin{pmatrix}\\hat a_* \\\\ \\hat a_t \\end{pmatrix} = \\begin{pmatrix}z_* \\\\ z_t \\end{pmatrix} \\] The processing defines the linear transformation \\[ T z_t = \\begin{pmatrix}\\hat a_* \\\\ \\hat a_t \\end{pmatrix} \\] and the searched determinant. 18.5.7.3 Step 2: conditional distribution of the initial observations \\[ p\\left(z_{1t}\\right | z_{2t} ) \\] is easily obtained by considering the join distribution of \\[ \\left( z_{1t}, z_{2t} \\right) \\sim N \\left( 0, \\begin{pmatrix} \\Sigma_{11} &amp;&amp; \\Sigma{12} \\\\ \\Sigma_{21} &amp;&amp; \\Sigma{22} \\end{pmatrix}\\right) \\ \\] It is distributed as \\[ N\\left( \\Sigma_{12} \\Sigma_{22}^{-1}v, \\Sigma_{11}-\\Sigma_{12} \\Sigma_{22}^{-1} \\Sigma_{21} \\right)\\] \\[= N\\left( \\Sigma_{12} T&#39;Tv, \\Sigma_{11}-\\Sigma_{12} T&#39;T\\Sigma_{21} \\right)\\] \\[ = N\\left( U&#39;Tv, \\Sigma_{11}-U&#39;U \\right)\\] where * v is obtained by applying the auto-regressive polynomial on the observations * T is the linear transformation defined in step 1 * \\[ U = T \\Sigma_{21} \\] 18.5.7.3.1 References [1] Otto M. C., Bell W.R., Burman J.P. (1987), “An Iterative GLS Approach to Maximum Likelihood Estimation of Regression Models with Arima Errors”, Bureau of The Census, SRD Research Report CENSUS/SRD/RR_87/34. 18.6 Handling of missing observations in (Reg)ARIMA models 18.6.1 Skipping approach 18.6.2 Additive outlier approach 18.6.3 References GOMEZ V. , MARAVALL A. AND PEÑA D. (1999): “Missing observations in ARIMA models: Skipping approach versus additive outlier approach”, Journal of econometrics 88, 341-363. 18.7 Tests on residuals 18.7.1 Autocorrelation 18.7.1.1 The Durbin-Watson statistic is defined by106: \\[ d = \\frac{\\sum_{t = 2}^{N}\\left( {\\widehat{a}}_{t} - {\\widehat{a}}_{t - 1} \\right)^{2}}{\\sum_{t = 1}^{N}{\\widehat{a}}_{t}^{2}} \\] where: \\({\\widehat{a}}_{t}\\) : residual from the model. Since \\[\\sum_{t = 2}^{N}\\left( {\\widehat{a}}_{t} - {\\widehat{a}}_{t - 1} \\right)^{2} \\cong \\ \\]2\\[\\sum_{t = 1}^{N}{\\widehat{a}}_{t}^{2} - 2\\sum_{t = 2}^{N}{ {\\widehat{a}}_{t}{\\widehat{a}}_{t - 1}}\\], then the approximation \\(d \\cong 2(1 - r_{z,1})\\), where \\[r_{z,1} = \\frac{\\sum_{t = 1}^{N}{ {\\widehat{a}}_{t}{\\widehat{a}}_{t - 1}}}{\\sum_{t = 1}^{N}{\\widehat{a}}_{t}^{2}}\\] is the autocorrelation coefficient of the residuals at lag 1, is true. The Durbin-Watson statistics is between 0 and 4. When the model provides an adequate description of the data, then \\(r_{z,1}\\) should be close to 0 and therefore the Durbin-Watson statistics is close to 2. When the Durbin–Watson statistic is substantially less than 2, there is evidence of positive serial correlation, while when it is substantially greater than 2 it indicates that the successive error terms are, on average, much different in value from one another, i.e., negatively correlated. More formally, to test for a positive autocorrelation at significance level \\(\\alpha\\), the Durbin-Watson statistics is compared to the lower (\\(d_{L,\\alpha}\\ )\\ \\)and upper (\\(d_{U,\\alpha})\\) critical values: If \\(d &lt; d_{L,\\alpha}\\) there is statistical evidence that the error terms are positively autocorrelated. If \\(d &gt; d_{U,\\alpha}\\) there is no statistical evidence that the error terms are positively autocorrelated. If \\(d_{L,\\alpha}\\) \\(&lt; d &lt; d_{U,\\alpha}\\) the test is inconclusive. Positive serial correlation is serial correlation in which a positive error for one observation increases the chances of a positive error for another observation. To test for negative autocorrelation at significance\\(\\ \\alpha\\), the test statistic \\((4 - d)\\) is compared to the lower (\\(d_{L,\\alpha}\\ )\\ \\)and upper (\\(d_{U,\\alpha})\\) critical values: If \\(\\left( 4 - d \\right) &lt; d_{L,\\alpha}\\) there is statistical evidence that the error terms are negatively autocorrelated. If \\(\\left( 4 - d \\right) &gt; d_{U,\\alpha}\\) there is no statistical evidence that the error terms are negatively autocorrelated. If \\(d_{U,\\alpha} &lt; \\left( 4 - d \\right) &lt; d_{U,\\alpha}\\) the test is inconclusive. 18.7.1.2 Ljung-Box test (described twice: merge) The Ljung-Box Q-statistics are given by: \\[ \\text{LB}\\left( k \\right) = n \\times (n + 2) \\times \\sum_{k = 1}^{K}\\frac{\\rho_{a,k}^{2}}{n - k} \\], \\[1\\] where: \\[\\rho_{a,k}^{2}\\] is the autocorrelation coefficient at lag \\(k\\) of the residuals \\[{\\widehat{a}}_{t}\\]. \\(n\\) is the number of terms in differenced series; \\[K\\] is the maximum lag being considered, set in JDemetra+ to \\(24\\) (monthly series) or \\(8\\) (quarterly series). If the residuals are random (which is the case for residuals from a well specified model), they will be distributed as \\(\\chi_{(K - m)}^{2}\\), where \\(m\\) is the number of parameters in the model which has been fitted to the data. The Ljung-Box and Box-Pierce tests sometimes fail to reject a poorly fitting model. Therefore, care should be taken not to accept a model on a basis of their results. For the description of autocorrelation concept see section Autocorrelation function and partial autocorrelation function. The Ljung-Box test checks the “overall” randomnes of a time series using a given number of autocorrelations. It tests wether any of a group of autocorrelations of a time series are significantly different from 0. 18.7.1.2.1 Algorithm We consider the autocorrelations \\(\\hat\\gamma_l, \\cdots, \\hat\\gamma_{l\\cdot k}\\). Typically, \\(l=1\\) when testing the independence of the series or \\(l=freq\\) when testing seasonality. The value of the test is defined by \\[ lb=n \\left(n+2\\right)\\sum_{i=1}^k\\frac{\\hat\\gamma_{i \\cdot l}^2}{n-i \\cdot l}\\] It is asymptotically distributed as a \\(\\chi \\left(k\\right)\\) 18.7.1.2.2 Impementation in GUI 18.7.1.2.3 Impementation in R 18.7.1.2.4 Java Library This test is implemented in the class demetra.stats.tests.LjungBoxTest int N=100; DataBlock sample=DataBlock.make(N); Random rnd=new Random(); LjungBoxTest lb=new LjungBoxTest(sample); StatisticalTest test = lb .lag(3) .autoCorrelationsCount(10) .build(); 18.7.1.3 Box-Pierce Test (described twice: merge) The Box-Pierce Q-statistics are given by: \\[\\text{BP}\\left( k \\right) = n\\sum_{k = 1}^{K}\\rho_{a,k}^{2} \\], \\[1\\] where: \\(\\rho_{a,k}^{2}\\) is the autocorrelation coefficient at lag \\(k\\) of the residuals \\({\\widehat{a}}_{t}\\). \\(n\\) is the number of terms in differenced series; \\(K\\) is the maximum lag being considered, set in JDemetra+ to \\(24\\) (monthly series) or \\(8\\) (quarterly series). If the residuals are random (which is the case for residuals from a well specified model), they will be distributed as \\(\\chi_{(K - m)}^{2}\\) degrees of freedom, where \\(m\\) is the number of parameters in the model which has been fitted to the data. The Ljung-Box and Box-Pierce tests sometimes fail to reject a poorly fitting model. Therefore, care should be taken not to accept a model on a basis of their results. For the description of autocorrelation concept see section Autocorrelation function and partial autocorrelation function. Explain difference with Ljung-Box test The Box-Pierce test checks the “overall” randomnes of a time series using a given number of autocorrelations. It tests wether any of a group of autocorrelations of a time series are significantly different from 0. 18.7.1.3.1 Statistic We consider the autocorrelations \\(\\hat\\gamma_l, \\cdots, \\hat\\gamma_{l\\cdot k}\\). Typically, \\(l=1\\) when testing the independence of the series or \\(l=freq\\) when testing seasonality. The value of the test is defined by \\[ bp=n \\sum_{i=1}^k\\hat\\gamma_{i \\cdot l}^2\\] It is asymptotically distributed as a \\(\\chi \\left(k\\right)\\) 18.7.1.3.2 Impementation in GUI 18.7.1.3.3 Impementation in R 18.7.1.3.4 Java Library This test is implemented in the class demetra.stats.tests.BoxPierceTest int N=100; DataBlock sample=DataBlock.make(N); Random rnd=new Random(); sample.set(rnd::nextDouble); BoxPierceTest bp=new BoxPierceTest(sample); StatisticalTest test = bp .lag(3) .autoCorrelationsCount(10) .build(); 18.7.2 Normality 18.7.2.1 Doornik-Hansen test The Doornik-Hansen test for multivariate normality (DOORNIK, J.A., and HANSEN, H. (2008)) is based on the skewness and kurtosis of multivariate data that is transformed to ensure independence. It is more powerful than the Shapiro-Wilk test for most tested multivariate distributions107. The skewness and kurtosis are defined, respectively, as: \\[s = \\frac{m_{3}}{\\sqrt{m_{2}}^{3}}\\] and \\(k = \\frac{m_{4}}{m_{2}^{2}},\\ \\)where: \\(m_{i} = \\frac{1}{n}\\sum_{i = 1}^{n}{(x_{i}}{- \\overline{x})}^{i}\\) \\(\\overline{x} = \\frac{1}{n}\\sum_{i = 1}^{n}x_{i}\\) and \\(n\\) is a number of (non-missing) residuals. The Doornik-Hansen test statistic derives from SHENTON, L.R., and BOWMAN, K.O. (1977) and uses transformed versions of skewness and kurtosis. The transformation for the skewness \\(s\\) into\\(\\text{z}_{1}\\) is as in D'AGOSTINO, R.B. (1970): \\[ \\beta = \\frac{3(n^{2} + 27n - 70)(n + 1)(n + 3)}{(n - 2)(n + 5)(n + 7)(n + 9)} \\] \\[ \\omega^{2} = - 1 + \\sqrt{2(\\beta - 1)} \\] \\[ \\delta = \\frac{1}{\\sqrt{\\log{(\\omega}^{2})}} \\] \\[ y = s\\sqrt{\\frac{(\\omega^{2} - 1)(n + 1)(n + 3)}{12(n - 2)}} \\] \\[ z_{1} = \\delta log(y + \\sqrt{y^{2} - 1}) \\] The kurtosis \\(k\\) is transformed from a gamma distribution to \\(\\chi^{2}\\), which is then transformed into standard normal \\(z_{2}\\) using the Wilson-Hilferty cubed root transformation: \\[ \\delta = (n - 3)(n + 1)(n^{2} + 15n - 4) \\] \\[ a = \\frac{(n - 2)(n + 5)(n + 7)(n^{2} + 27n - 70)}{6\\delta} \\] \\[ c = \\frac{(n - 7)(n + 5)(n + 7)(n^{2} + 2n - 5)}{6\\delta} \\] \\[ l= \\frac{(n + 5)(n + 7)({n^{3} + 37n}^{2} + 11n - 313)}{12\\delta} \\] \\[ \\alpha = a + c \\times s^{2} \\] \\[ \\chi = 2l(k - 1 - s^{2}) \\] \\[ z_{2} = \\sqrt{9\\alpha}\\left( \\frac{1}{9\\alpha} - 1 + \\sqrt[3]{\\frac{\\chi}{2\\alpha}} \\right) \\] Finally, the Doornik-Hansen test statistic is defined as the sum of squared transformations of the skewness and kurtosis. Approximately, the test statistic follows a \\(\\chi^{2}\\)distribution, i.e.: \\[ DH = z_{1}^{2} + z_{2}^{2}\\sim\\chi^{2}(2) \\] 18.7.2.2 D’agostino ? 18.7.2.3 Jarque-Bera DAGUM, E.B. (1980).↩︎ GÓMEZ, V., and MARAVALL, A. (2001b).↩︎ The notation used by TRAMO for the polynomials is different from the one commonly used in the literature, for example in HAMILTON, J.D. (1994) the AR polynomial is denoted as .↩︎ BOX G.E.P., JENKINS, G.M., and REINSEL, G.C. (2007).↩︎ KAISER, R., and MARAVALL, A. (1999).↩︎ ‘X-13ARIMA-SEATS Reference Manual’ (2015).↩︎ In the TRAMO-SEATS method this type of outlier is called a transitory change.↩︎ GÓMEZ, V., and MARAVALL, A. (1997).↩︎ Dummy variable is the variable that takes the values 0 or 1 to indicate the absence or presence of some effect.↩︎ GÓMEZ, V., and MARAVALL, A. (2010).↩︎ Parsimonious models are those which have a great deal of explanatory power using a relatively small number of parameters. Balanced models are models for which the order of the combined AR and differencing operators is equal to the order of the combined MA operator (see GÓMEZ, V., and MARAVALL, A. (1997)). A model is said to be more balanced than a competing model if the absolute difference between the total orders of the AR plus differencing and MA operators is smaller for one model than another. For description of the Hannan-Rissanen algorithm see he respective section above, a well as HANNAN, E.J., and RISSANEN, J. (1982), GÓMEZ, V., and MARAVALL, A. (2001b).↩︎ GÓMEZ, V., and MARAVALL, A. (2001b).↩︎ MARAVALL, A. (2000).↩︎ ‘X-13ARIMA-SEATS Reference Manual’ (2015).↩︎ DAGUM, E.B. (1988).↩︎ ‘X-13ARIMA-SEATS Reference Manual’ (2015).↩︎ The pre-tested options are: one, eight, and fifteen days before Easter.↩︎ CHATFIELD, C. (2004).↩︎ AKAIKE, H. (1973).↩︎ HURVICH, C.M., and TSAI, C. (1989).↩︎ HANNAN, E.J., and QUINN, B.G. (1979).↩︎ SCHWARZ, G. (1978).↩︎ PEÑA, D. (2001).↩︎ HANNAN, E.J., and RISSANEN, J. (1982), NEWBOLD, D., and BOS, T. (1982).↩︎ ‘X-13ARIMA-SEATS Reference Manual’ (2015).↩︎ ‘X-13ARIMA-SEATS Reference Manual’ (2015).↩︎ MAKRIDAKIS, S., WHEELWRIGHT, S.C., and HYNDMAN, R.J. (1998).↩︎ CHATFIELD, C. (2004).↩︎ CHATFIELD, C. (2004).↩︎ The description of the test derives from DOORNIK, J.A., and HANSEN, H. (2008).↩︎ "],["moving-average-based-decomposition.html", "Chapter 19 Moving average based decomposition", " Chapter 19 Moving average based decomposition "],["arima-model-based-decomposition.html", "Chapter 20 Arima Model based decomposition", " Chapter 20 Arima Model based decomposition "],["state-space-framework.html", "Chapter 21 State Space Framework", " Chapter 21 State Space Framework "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
