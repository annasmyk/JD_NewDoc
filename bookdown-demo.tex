% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{book}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={JDemetra+ online documentation},
  pdfauthor={Anna Smyk},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{apalike}

\title{JDemetra+ online documentation}
\author{Anna Smyk}
\date{2022-05-13}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{jdemetra-software}{%
\chapter{JDemetra+ Software}\label{jdemetra-software}}

\hypertarget{structure-of-this-book}{%
\section{Structure of this book}\label{structure-of-this-book}}

\hypertarget{functions}{%
\subsection{Functions}\label{functions}}

what can be done

\hypertarget{tools-to-access-the-functions}{%
\subsection{Tools to access the functions}\label{tools-to-access-the-functions}}

how it can be done

\hypertarget{underlying-statistical-methods}{%
\subsection{Underlying Statistical Methods}\label{underlying-statistical-methods}}

why (roughly) is it done this way

\hypertarget{audience}{%
\section{Audience}\label{audience}}

\hypertarget{how-jdemetra-came-to-be}{%
\section{How Jdemetra+ came to be}\label{how-jdemetra-came-to-be}}

history of the project

\hypertarget{quick-start-with}{%
\chapter{Quick start with\ldots{}}\label{quick-start-with}}

\hypertarget{seasonal-adjustment}{%
\section{Seasonal Adjustment}\label{seasonal-adjustment}}

\hypertarget{seasonal-adjustment-of-high-frequency-data}{%
\section{Seasonal Adjustment of High-Frequency Data}\label{seasonal-adjustment-of-high-frequency-data}}

\hypertarget{use-of-jd-algorithms-in-r}{%
\section{Use of JD+ algorithms in R}\label{use-of-jd-algorithms-in-r}}

\hypertarget{use-of-jd-graphical-interface}{%
\section{Use of JD+ graphical interface}\label{use-of-jd-graphical-interface}}

\hypertarget{main-functions-overview}{%
\chapter{Main functions overview}\label{main-functions-overview}}

\hypertarget{seasonal-adjustment-algorithms}{%
\section{Seasonal adjustment algorithms}\label{seasonal-adjustment-algorithms}}

\hypertarget{x-13}{%
\subsection{X-13}\label{x-13}}

Estimation of the exact likelihood of an ARMA model

We suppose that
\[ y_t, \quad 1 \le t \le n \]
follows an ARMA model.

The X12 implementation computes the exact likelihood in two main steps

\hypertarget{overview}{%
\subsubsection{Overview}\label{overview}}

We consider the transformation

\[ z_t = \begin{pmatrix} z_{1t} \\ z_{2t} \end{pmatrix} = \begin{cases} y_t, & 1 \le t \le p \\ \Phi\left(B\right) y_t, & p \lt t \le n\end{cases}\]

It is obvious that
\[ p\left(y_t\right) = p\left(z_t\right) \]

We will estimate
\[ p\left(z_t\right) = p\left(z_{2t}\right)  p\left(z_{1t}\right | z_{2t} )\]

\hypertarget{step-1-likelihood-of-a-pure-moving-average-process}{%
\subsubsection{Step 1: likelihood of a pure moving average process}\label{step-1-likelihood-of-a-pure-moving-average-process}}

\[z_{2t}\]
is a pure moving average process. Its exact likelihood is estimated as follows (see {[}1{]} for details).
We list below the different steps of the algorithm.

\begin{itemize}
\item
  Compute the conditional least squares residuals by the recursion:
  \[ a_{0t} = \begin{cases} 0,& -q \lt t \leq 0 \\ z_t-\theta_1 a_{0t-1}- \cdots --\theta_q a_{0t-q},&  0 \lt t \leq n  \end{cases} \]
\item
  Compute the Pi-Weights of the model. They defines the (n+q x q) matrix
\end{itemize}

\[ G = \begin{pmatrix} 1 & 0 & \cdots & 0 \\ \pi_1 & 1 & \cdots & 0 \\ \pi_2 & \pi_1 & \cdots & \vdots\\ \vdots & \vdots & \vdots & \vdots \\ \pi_{n+q-1} & \pi_{n+q-2} & \cdots & \pi_{n} \end{pmatrix} \]

\begin{itemize}
\item
  Compute by recursion
  \[ G'a \quad and \quad G'G \]
\item
  Compute by Cholesky decomposition
  \[ G'G \hat z_* = G'a \quad and \quad |G'G| \]
\item
  Obtain by recursion the exact likelihood residuals
  \[ \Theta\left(B\right)\begin{pmatrix}\hat a_* \\ \hat a_t \end{pmatrix}  = \begin{pmatrix}z_* \\ z_t \end{pmatrix} \]
\end{itemize}

The processing defines the linear transformation
\[ T z_t = \begin{pmatrix}\hat a_* \\ \hat a_t \end{pmatrix} \] and the searched determinant.

\hypertarget{step-2-conditional-distribution-of-the-initial-observations}{%
\subsubsection{Step 2: conditional distribution of the initial observations}\label{step-2-conditional-distribution-of-the-initial-observations}}

\[ p\left(z_{1t}\right | z_{2t} ) \]
is easily obtained by considering the join distribution of
\[ \left( z_{1t}, z_{2t} \right) \sim N \left( 0, \begin{pmatrix} \Sigma_{11} && \Sigma{12} \\ \Sigma_{21} && \Sigma{22} \end{pmatrix}\right) \ \]

It is distributed as
\[ N\left( \Sigma_{12} \Sigma_{22}^{-1}v, \Sigma_{11}-\Sigma_{12} \Sigma_{22}^{-1} \Sigma_{21} \right)\]

\[= N\left( \Sigma_{12} T'Tv, \Sigma_{11}-\Sigma_{12} T'T\Sigma_{21} \right)\]

\[  = N\left( U'Tv, \Sigma_{11}-U'U \right)\]

where
* v is obtained by applying the auto-regressive polynomial on the observations
* T is the linear transformation defined in step 1
*
\[ U = T \Sigma_{21} \]

\hypertarget{references}{%
\paragraph{References}\label{references}}

{[}1{]} \textbf{\emph{Otto M. C., Bell W.R., Burman J.P.}} (1987), ``An Iterative GLS Approach to Maximum Likelihood Estimation of Regression Models with Arima Errors'', Bureau of The Census, SRD Research Report CENSUS/SRD/RR\_87/34.

\hypertarget{tramo-seats}{%
\subsection{Tramo-Seats}\label{tramo-seats}}

links : to praadj, decomp, tools, methods

TRAMO-SEATS is a model-based seasonal adjustment method developed by
Victor Gómez (Ministerio de Hacienda), and Agustin Maravall (Banco de
España). It consists of two linked programs: TRAMO and SEATS. TRAMO
(Time Series Regression with ARIMA Noise, Missing Observations, and
Outliers) performs estimation, forecasting, and interpolation of
regression models with missing observations and ARIMA errors, in the
presence of possibly several types of outlier. SEATS (Signal
Extraction in ARIMA Time Series) performs an ARIMA-based decomposition
of an observed time series into unobserved components. Information about
the TRAMO-SEATS method available in this section derives directly from
papers by Victor Gómez and Agustin Maravall; the most important ones
are: GÓMEZ, V., and MARAVALL, A. (1996), GÓMEZ, V., and MARAVALL, A.
(2001a, b) and MARAVALL, A. (2009). More information about the
TRAMO-SEATS method, TRAMO-SEATS software (DOS version and TSW+ -- Tramo
Seats Windows software and several interfaces) and its documentation as
well as papers on methodology and application of the programs, can be
found in the dedicated section of the
Banco de España website.

\hypertarget{trend-cycle-estimation}{%
\section{Trend-cycle estimation}\label{trend-cycle-estimation}}

\hypertarget{seasonal-adjustment-1}{%
\chapter{Seasonal Adjustment}\label{seasonal-adjustment-1}}

\hypertarget{motivation}{%
\section{Motivation}\label{motivation}}

\hypertarget{unobserved-components}{%
\section{Unobserved Components}\label{unobserved-components}}

\hypertarget{seasonality-tests}{%
\section{Seasonality tests}\label{seasonality-tests}}

\hypertarget{overview-1}{%
\subsection{Overview}\label{overview-1}}

\hypertarget{f-test-on-seasonal-dummies}{%
\subsection{F-test on seasonal dummies}\label{f-test-on-seasonal-dummies}}

The F-test on seasonal dummies checks for the presence of deterministic
seasonality. The model used here uses seasonal dummies (mean effect and
11 seasonal dummies for monthly data, mean effect and 3 for quarterly
data) to describe the (possibly transformed) time series behaviour. The
test statistic checks if the seasonal dummies are jointly statistically
not significant. When this hypothesis is rejected, it is assumed that
the deterministic seasonality is present and the test results are
displayed in green.

This test refers to Model-Based \(\chi^{2}\ \)and F-tests for Fixed
Seasonal Effects proposed by LYTRAS, D.P., FELDPAUSCH, R.M., and BELL,
W.R. (2007) that is based on the estimates of the regression dummy
variables and the corresponding t-statistics of the RegARIMA model, in
which the ARIMA part of the model has a form (0,1,1)(0,0,0). The
consequences of a misspecification of a model are discussed in LYTRAS,
D.P., FELDPAUSCH, R.M., and BELL, W.R. (2007).

For a monthly time series the RegARIMA model structure is as follows:

\[\left( 1 - B \right)\left( y_{t} - \beta_{1}M_{1,t} - \ldots - \beta_{11}M_{11,t} - \gamma X_{t} \right) = \mu + (1 - B)a_{t}
\], \[1\]

where:

\[
M_{j,t} =
\begin{cases}
1 & \text{ in month } j = 1, \ldots, 11 \\
- 1 & \text{ in December}\\
0 & \text{ otherwise}
\end{cases} \text{ - dummy variables;}
\]

\(y_{t}\) -- the original time series;

\(B\) -- a backshift operator;

\(X_{t}\) -- other regression variables used in the model (e.g.~outliers,
calendar effects, user-defined regression variables, intervention
variables);

\(\mu\) -- a mean effect;

\(a_{t}\) -- a white-noise variable with mean zero and a constant
variance.

In the case of a quarterly series the estimated model has a form:

\[\left( 1 - B \right)\left( y_{t} - \beta_{1}M_{1,t} - \ldots - \beta_{3}M_{3,t} - \gamma X_{t} \right) = \mu + (1 - B)a_{t}\], \[2\]

where:

\[
M_{j,t} =
\begin{cases}
1 & \text{ in quarter} j = 1, \ldots, 3 \\
- 1 & \text{ in the fourth quarter}\\
0 & \text{ otherwise}
\end{cases} \text{ - dummy variables;}
\]

One can use the individual t-statistics to assess whether seasonality
for a given month is significant, or a chi-squared test statistic if the
null hypothesis is that the parameters are collectively all zero. The
chi-squared test statistic is
\({\widehat{\chi}}^{2} = {\widehat{\beta}}^{'}{\lbrack Var(\widehat{\beta})}^{\ })^{- 1}\rbrack{\widehat{\beta}}^{\ }\)
in this case compared to critical values from a
\(\chi^{2}\left( \text{df} \right)\)-distribution, with degrees of freedom
\(df = 11\ \)(monthly series) or \(df = 3\) (quarterly series). Since the
\({Var(\widehat{\beta})}^{\ }\) computed using the estimated variance of
\(\alpha_{t}\) may be very different from the actual variance in small
samples, this test is corrected using the proposed
\(\text{F}\) statistic:

\[
  F = \frac{ {\widehat{\chi}}^{2}}{s - 1} \times \frac{n - d - k}{n - d}
  \]\emph{,} \[3\]

where \(n\) is the sample size, \(d\) is the degree of differencing, s is
time series frequency (12 for a monthly series, 4 for a quarterly
series) and \(k\) is the total number of regressors in the RegARIMA model
(including the seasonal dummies \(\text{M}_{j,t}\) and the intercept).

This statistic follows a \[F_{s - 1,n - d - k}\] distribution under the
null hypothesis.

\hypertarget{qs-test-on-autocorrelation-at-seasonal-lags}{%
\subsection{QS Test on autocorrelation at seasonal lags}\label{qs-test-on-autocorrelation-at-seasonal-lags}}

The QS test is a variant of the \href{../theory/Tests_LB.html}{Ljung-Box} test computed on seasonal lags, where we only consider positive auto-correlations

More exactly,

\[ QS=n \left(n+2\right)\sum_{i=1}^k\frac{\left[ \max  \left(0, \hat\gamma_{i \cdot l}\right)\right]^2}{n-i \cdot l}\]

where \[k=2\], so only the first and second seasonal lags are considered. Thus, the test would checks the correlation between the
actual observation and the observations lagged by one and two years. Note that \[l=12\] when dealing with monthly observations,
so we consider the autocovariances \[\hat\gamma_{12}\] and \[\hat\gamma_{24}\] alone. In turn, \[k=4\] in the case of quarterly data.

Under H0, which states that the data are independently distributed, the statistics follows a \[\chi \left(k\right)\] distribution. However,
the elimination of negative correlations makes it a bad approximation. The p-values would be given
by \(P(\chi^{2}\left( k \right) > Q)\) for \(k = 2\). As \({P(\chi}^{2}(2)) > 0.05 = 5.99146\) and
\({P(\chi}^{2}(2)) > 0.01 = 9.21034\), \(QS > 5.99146\) and \(QS > 9.21034\)
would suggest rejecting the null hypothesis at \(95\%\) and \(99\%\)
significance levels, respecively.

\hypertarget{modification}{%
\paragraph{Modification}\label{modification}}

Maravall (2012) proposes approximate the correct distribution (p-values) of the QS statistic using simulation techniques. Using 1000K replications of sample size 240,
the correct critical values would be 3.83 and 7.09 with confidence levels of \(95\%\) and \(99\%\), respectively (lower than the 5.99146 and 9.21034 shown above). For
each of the simulated series,
he obtains the distribution by assuming \(QS=0\) when \[\hat\gamma_{12}\], so in practice this test will detect seasonality only when
any of these conditions hold:
- Statistically significant positive autocorrelation at lag 12
- Nonnegative sample autocorrelation at lag 12 and statistically significant positive autocorrelation at lag 24

\hypertarget{implementation}{%
\subsubsection{Implementation}\label{implementation}}

\hypertarget{in-the-graphical-user-interface-gui}{%
\paragraph{In the graphical user interface (GUI)}\label{in-the-graphical-user-interface-gui}}

The test can be applied directly to any series by selecting the
option \emph{Statistical Methods \textgreater\textgreater{} Seasonal Adjustment \textgreater\textgreater{} Tools \textgreater\textgreater{} Seasonality Tests}. This is
an example of how results are displayed for the case of a monthly series:

\begin{figure}
\centering
\includegraphics{All_images/qs.png}
\caption{qs}
\end{figure}

The test can be applied to the input series before any seasonal adjustment method has been applied. It can also be applied to the seasonally
adjusted series or to the irreguar component.

\hypertarget{via-r-package-rjd3toolkit-blank}{%
\paragraph{Via R package: RJD3toolkit (blank)}\label{via-r-package-rjd3toolkit-blank}}

\hypertarget{java-library}{%
\paragraph{Java Library}\label{java-library}}

This test is implemented in the class \texttt{ec.satoolkit.diagnostics.QsTest}

\hypertarget{references-1}{%
\paragraph{References}\label{references-1}}

\begin{itemize}
\tightlist
\item
  LJUNG G. M. and G. E. P. BOX (1978). ``On a Measure of a Lack of Fit in Time Series Models''. Biometrika 65 (2): 297--303. \url{doi:10.1093/biomet/65.2.297}
\item
  MARAVALL, A. (2011). ''Seasonality Tests and Automatic Model Identification in Tramo-Seats''. Manuscript
\item
  MARAVALL, A. (2012). ``Update of Seasonality Tests and Automatic Model Identification in TRAMO-SEATS''. Bank of Spain (November 2012)
\end{itemize}

\hypertarget{qs-test-for-seasonality-bis-solve-this}{%
\subsection{QS Test for seasonality (BIS : solve this)}\label{qs-test-for-seasonality-bis-solve-this}}

More exactly,

\[ qs=n \left(n+2\right)\sum_{i=1}^k\frac{\left[ \max  \left(0, \hat\gamma_{i \cdot l}\right)\right]^2}{n-i \cdot l}\]

The current implementation still considers that the statistics is distributed as a
\[\chi \left(k\right)\]
even if it is obvioulsly incorrect.

\hypertarget{kurskall-wallis}{%
\subsection{Kurskall-Wallis}\label{kurskall-wallis}}

The Kruskal-Wallis test is a non-parametric test used for testing whether samples originate from the same distribution.
The parametric equivalent of the Kruskal-Wallis test is the one-way analysis of variance (ANOVA).
When rejecting the null hypothesis of the Kruskal-Wallis test, then at least one sample stochastically dominates at least one other sample.
The test does not identify where this stochastic dominance occurs or for how many pairs of groups stochastic dominance obtains.
The null hypothesis states that all months (or quarters, respectively) have the same mean.
Under this hypothesis the test statistic follows a \[ \chi^2 \] distribution.
When this hypothesis is rejected, it is assumed that time series values differ significantly between periods and the test results are displayed in green

The test is typically applied to \[ k  \] groups of data \[ \left\{x_{i}\right\}_{j} \]. Each group \[ j=1,…,k \] is composed of \[ n_j \] observations,
which are indexed by \[ i=1,…,n_j \]. Each month (or quarter) groups all the observations available for a certain number of years.

As opposed to the notation used in the \href{../theory/Tests_Friedman.html}{Friedman test}, number of observations here is not necessarily equal for each group.
The ranking of each data point, represented by variable \[ r_{ij} \]., is now defined different than in Friedman test,
since it considers all observables \[ N=n_1+ \dots + n_g \], thereby ignoring group membership.

The test statistic is given by

\[
Q=\frac{SS_t}{SS_e}
\]

where \[ SS_t=(N-1)\sum_{j=1}^{g}n_i(\bar{r}_{.j}-\bar{r})^2 \] and \[ SS_e=\sum_{j=1}^{g}\sum_{i=1}^{n_j}(r_{ij}-\bar{r})^2 \]
- \[ n_j \] is the number of observations in group \[ j  \]
- \[ \bar{r}_{.j} \] is the average of the absolute ranks of the data in group \[ j  \]
- The average rank is \[ \bar{r} =\frac{1}{2}(N+1) \]

Under the null hypothesis that all groups are generated from the same distribution, the test statistic Q is approximated by a chi-squared distribution.
Thus, the p-value is given by \[ P( \chi^2_{g-1}>Q) \]. This approximation can be misleading if some of the groups are very small (i.e.~less than five elements). If the statistic is not significant, then there is no evidence of stochastic dominance between the samples.
However, if the test is significant then at least one sample \href{http://en.wikipedia.org/wiki/Stochastic_dominance}{stochastically
dominates} another sample.

\hypertarget{use}{%
\subsubsection{Use}\label{use}}

The test can be applied directly to any series by selecting the
option \emph{Statistical Methods \textgreater\textgreater{} Seasonal Adjustment \textgreater\textgreater{} Tools \textgreater\textgreater{} Seasonality Tests}. This is
an example of how results are displayed for the case of a monthly series:

\begin{figure}
\centering
\includegraphics{All_images/kw.png}
\caption{kwResults}
\end{figure}

The test can be applied to the input series before any seasonal adjustment method has been applied. It can also be applied to the seasonally adjusted series or to the irreguar component.

\hypertarget{implementation-1}{%
\subsubsection{Implementation}\label{implementation-1}}

This test is implemented in the class \texttt{ec.satoolkit.diagnostics.KruskallWallisTest}

\hypertarget{references-2}{%
\subsubsection{References}\label{references-2}}

\begin{itemize}
\tightlist
\item
  Kruskal; Wallis (1952). ``Use of ranks in one-criterion variance analysis''. Journal of the American Statistical Association 47 (260): 583--621. \url{doi:10.1080/01621459.1952.10483441}.
\end{itemize}

\hypertarget{friedman-test-stable-seasonality-test}{%
\subsection{Friedman test (stable seasonality test)}\label{friedman-test-stable-seasonality-test}}

The Friedman test is a non-parametric method for testing that samples are drawn from the same population or from populations with equal medians.
The significance of the month (or quarter) effect is tested. The Friedman test requires no distributional assumptions. It uses the rankings of the observations.
If the null hypothesis of no stable seasonality is rejected at the 0.10\% significance level then the series is considered to be seasonal
and the test's outcome is displayed in green.

The test statistic is constructed as follows. Consider first the matrix of data \[ \left\{x_{ij}\right\}_{n \times k} \] with \[ n \] rows (the blocks,
i.e.~number of years in the sample), \[ k \] columns (the treatments, i.e.~either 12 months or 4 quarters, depending on the frequency of the data).\\
The data matrix needs to be replaced by a new matrix \[ \left\{r_{ij}\right\}_{n \times k} \], where the entry \[ r_{ij} \] is the rank of \[ x_{ij} \]
within block \[ i \] .

The test statistic is given by

\[
Q=\frac{SS_t}{SS_e}
\]

where \[ SS_t=n \sum_{j=1}^{k}(\bar{r}_{.j}-\bar{r})^2 \] and \[ SS_e=\frac{1}{n(k-1)} \sum_{i=1}^{n}\sum_{j=1}^{k}(r_{ij}-\bar{r})^2 \]
It represents the variance of the average ranking across treatments j relative to the total.

Under the hypothesis of no seasonality, all months can be equally treated. For the sake of completeness:
- \[ \bar{r}_{.j} \] is the average ranks of each treatment (month) j within each block (year)
- The average rank is given by \[ \bar{r}= \frac{1}{nk}\sum_{i=1}^{n}\sum_{j=1}^{k}(r_{ij})\]

For large \[ n \] or \[ k \] , i.e.~n \textgreater{} 15 or k \textgreater{} 4, the probability distribution of \[ Q \] can be approximated by that of
a chi-squared distribution. Thus, the p-value is given by \[ P( \chi^2_{k-1}>Q) \] .

\hypertarget{use-1}{%
\subsubsection{Use}\label{use-1}}

The test can be applied directly to any series by selecting the
option \emph{Statistical Methods \textgreater\textgreater{} Seasonal Adjustment \textgreater\textgreater{} Tools \textgreater\textgreater{} Seasonality Tests}. This is
an example of how results are displayed for the case of a monthly series:

\begin{figure}
\centering
\includegraphics{All_images/friedman.png}
\caption{friedman}
\end{figure}

If the null hypothesis of no stable seasonality is rejected at the 1\% significance level, then the series is
considered to be seasonal and the outcome of the test is displayed in green.

The test can be applied to the input series before any seasonal adjustment method has been
applied. It can also be applied to the seasonally adjusted series or to the irreguar component. In the case of
X-13ARIMA-SEATS, the test is applied to the preliminary estimate
of the unmodified Seasonal-Irregular component\footnote{The unmodified Seasonal-Irregular component corresponds to the
  Seasonal-Irregular factors with the extreme values.} (time series shown
in Table B3). In this estimate, the number of observations is lower than
in the final estimate of the unmodified Seasonal-Irregular component.
Thus, the number of degrees of freedom in the stable
seasonality test is lower than the number of degrees of freedom in the
test for the \href{../theory/Tests_presence_stability.html}{presence of seasonality assuming stability}. For
example, X-13ARIMA-SEATS uses a centred moving average of order 12 to
calculate the preliminary estimation of trend. Consequently, the first
six and last six points in the series are not computed at this stage of
calculation. The preliminary estimation of the trend is then used for
the calculation of the preliminary estimation of the unmodified Seasonal-Irregular.

\hypertarget{related-tests}{%
\subsubsection{Related tests}\label{related-tests}}

\begin{itemize}
\tightlist
\item
  When using this kind of design for a binary response, one instead uses the Cochran's Q test.
\item
  Kendall's W is a normalization of the Friedman statistic between 0 and 1.
\item
  The Wilcoxon signed-rank test is a nonparametric test of non-independent data from only two groups.
\end{itemize}

\hypertarget{implementation-2}{%
\subsubsection{Implementation}\label{implementation-2}}

This test is implemented in the class \texttt{ec.satoolkit.diagnostics.FriedmanTest}

\hypertarget{references-3}{%
\subsubsection{References}\label{references-3}}

\begin{itemize}
\item
  Friedman, Milton (December 1937). ``The use of ranks to avoid the assumption of normality implicit in the analysis of variance''. Journal of the American Statistical Association (American Statistical Association) 32 (200): 675--701. \url{doi:10.2307/2279372}. JSTOR 2279372.
\item
  Friedman, Milton (March 1939). ``A correction: The use of ranks to avoid the assumption of normality implicit in the analysis of variance''. Journal of the American Statistical Association (American Statistical Association) 34 (205): 109. \url{doi:10.2307/2279169}. JSTOR 2279169.
\item
  Friedman, Milton (March 1940). ``A comparison of alternative tests of significance for the problem of m rankings''. The Annals of Mathematical Statistics 11 (1): 86--92. \url{doi:10.1214/aoms/1177731944}. JSTOR 2235971.
\end{itemize}

\hypertarget{stable-seasonality-test-missing}{%
\subsection{Stable seasonality test (missing)}\label{stable-seasonality-test-missing}}

\hypertarget{moving-seasonality-test}{%
\subsection{Moving seasonality test}\label{moving-seasonality-test}}

The evolutive seasonality test is based on a two-way analysis of
variance model. The model uses the values from complete years only.
Depending on the decomposition type for the Seasonal -- Irregular
component it uses \[1\] (in the case of a multiplicative model) or
\[2\] (in the case of an additive model):

\[
  \left|\text{SI}_{\text{ij}} - 1 \right| = X_{\text{ij}} = b_{i} + m_{j} + e_{\text{ij}}
  \], \[1\]

\[
  \left| \text{SI}_{\text{ij}} \right| = X_{\text{ij}} = b_{i} + m_{j} + e_{\text{ij}}
  \], \[2\]

where:

\(m_{j}\) -- the monthly or quarterly effect for \(j\)-th period,
\(j = (1,\ldots,k)\), where \(k = 12\) for a monthly series and \(k = 4\) for
a quarterly series;

\(b_{j}\) -- the annual effect \(i\), \((i = 1,\ldots,N)\) where \(N\) is the
number of complete years;

\(e_{\text{ij}}\) -- the residual effect.

The test is based on the following decomposition:

\[S^{2} = S_{A}^{2} + S_{B}^{2} + S_{R}^{2},\] \[3\]

where:

\[
S^{2} = \sum_{j = 1}^{k}{\sum_{i = 1}^{N}\left( {\overline{X}}_{\text{ij}} - {\overline{X}}_{\bullet \bullet} \right)^{2}}\ 
\] --the total sum of squares;

\[
S_{A}^{2} = N\sum_{j = 1}^{k}\left( {\overline{X}}_{\bullet j} - {\overline{X}}_{\bullet \bullet} \right)^{2}
\] -- the inter-month (inter-quarter, respectively) sum of squares, which
mainly measures the magnitude of the seasonality;

\[
S_{B}^{2} = k\sum_{i = 1}^{N}\left( {\overline{X}}_{i \bullet} - {\overline{X}}_{\bullet \bullet} \right)^{2}
\] -- the inter-year sum of squares, which mainly measures the year-to-year
movement of seasonality;

\[
S_{R}^{2} = \sum_{i = 1}^{N}{\sum_{j = 1}^{k}\left( {\overline{X}}_{\text{ij}} - {\overline{X}}_{i \bullet} - {\overline{X}}_{\bullet j} - {\overline{X}}_{\bullet \bullet} \right)^{2}}
\] -- the residual sum of squares.

The null hypothesis \(H_{0}\ \)is that \(b_{1} = b_{2} = ... = b_{N}\) which
means that there is no change in seasonality over the years. This
hypothesis is verified by the following test statistic:

\[
   F_{M} = \frac{\frac{S_{B}^{2}}{(n - 1)}}{\frac{S_{R}^{2}}{(n - 1)(k - 1)}}
   \], \[4\]

which follows an \(F\)-distribution with \(k - 1\) and \(n - k\) degrees of
freedom.

\hypertarget{identifiable-seasonality}{%
\subsection{Identifiable seasonality}\label{identifiable-seasonality}}

This test combines the values of the \(F\)-statistic of the parametric
test for stable seasonality and the values of the moving seasonality
test, which was described above.

The test statistic is:

\[
  T = \left( \frac{\frac{7}{F_{S}} + \frac{3F_{M}}{F_{S}}}{2} \right)^{\frac{1}{2}}
  \], \[1\]

where \(F_{S}\) is a stable seasonality test statistic and \(F_{M}\) is
moving seasonality test statistic. The test checks if the stable
seasonality is not dominated by moving seasonality. In such a case the
seasonality is regarded as identifiable. This test statistic is used in
the combined seasonality tests (see section \href{../theory/Tests_combined.html}{Combined seasonality test}. The detailed description of the test is available in LOTHIAN, J., and MORRY, M. (1978).

\hypertarget{combined-seasonality-test}{%
\subsection{Combined seasonality test}\label{combined-seasonality-test}}

This test combines the Kruskal-Wallis test along with test for the
presence of seasonality assuming stability (\(F_{S}\)), and evaluative
seasonality test for detecting the presence of identifiable seasonality
(\(F_{M}\)). Those three tests are calculated using the final unmodified
SI component. The main purpose of the combined seasonality test is to
check whether the seasonality of the series is identifiable. For
example, the identification of the seasonal pattern is problematic if
the process is dominated by highly moving seasonality\footnote{DAGUM, E.B. (1987).}. The testing
procedure is shown in the figure below.

\begin{figure}
\centering
\includegraphics{All_images/UG_A_image18.png}
\caption{Text}
\end{figure}

\textbf{Combined seasonality test, source: LADIRAY, D., QUENNEVILLE, B. (2001)}

\hypertarget{spectral-analysis}{%
\subsection{Spectral analysis}\label{spectral-analysis}}

In order to decide whether a series has a seasonal component that is
predictable (stable) enough, these tests use visual criteria and formal
tests for the periodogram. The periodogram is calculated using complete
years, so that the set of Fourier frequencies contains exactly all
seasonal frequencies\footnote{For definition of the periodogram and Fourier frequencies see the \href{../theory/spectral_periodogram.html}{Periodogram} section.}.

The tests rely on two basic principles:

\begin{itemize}
\item
  The peaks associated with seasonal frequencies should be larger than
  \textgreater{} the median spectrum for all frequencies and;
\item
  The peaks should exceed the spectrum of the two adjacent values by
  \textgreater{} more than a critical value.
\end{itemize}

\begin{quote}
JDemetra+ performs this test on the original series. If these two
requirements are met, the test results are displayed in green. The
statistical significance of each of the seasonal peaks (i.e.
frequencies
\(\frac{\pi}{6},\ \frac{\pi}{3},\ \frac{\pi}{2},\ \frac{2\pi}{3}\text{ and } \frac{5\pi}{6}\ \)corresponding
to 1, 2, 3, 4 and 5 cycles per year) is also displayed. The seasonal
and trading days frequencies depends on the frequency of time series.
They are shown in the table below. The symbol \(d\) denotes a default
frequency and is described below the table.
\end{quote}

\textbf{The seasonal and trading day frequencies by time series
frequency}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2390}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.5346}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2264}}@{}}
\toprule
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Number of months per full period}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Seasonal frequency}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Trading day frequency (radians)}
\end{minipage} \\
\midrule
\endhead
12 & \(\frac{\pi}{6},\frac{\pi}{3},\ \frac{\pi}{2},\frac{2\pi}{3},\ \frac{5\pi}{6},\ \pi\) & \(d\), 2.714 \\
6 & \(\frac{\pi}{3},\frac{2\pi}{3}\), \(\pi\) & \(d\) \\
4 & \(\frac{\pi}{2}\), \(\pi\) & \(d\), 1.292, 1.850, 2.128 \\
3 & \(\pi\) & \(d\) \\
2 & \(\pi\) & \(d\) \\
\bottomrule
\end{longtable}

The calendar (trading day or working day) effects, related to the
variation in the number of different days of the week per period, can
induce periodic patterns in the data that can be similar to those
resulting from pure seasonal effects. From the theoretical point of
view, trading day variability is mainly due to the fact that the average
number of days in the months or quarters is not equal to a multiple of
\(7\) (the average number of days of a month in the year of \(365.25\ \)days
is equal to \(\frac{365.25}{12} = 30.4375\) days). This effect occurs
\(\frac{365.25}{12} \times \frac{1}{7} = 4.3482\) times per month: one
time for each one of the four complete weeks of each month, and a
residual of \(0.3482\) cycles per month, i.e.
\(0.3482 \times 2\pi = 2.1878\ radians\). This turns out to be a
fundamental frequency for the effects associated with monthly data. In
JDemetra+ the fundamental frequency corresponding to \(0.3482\) cycles per
month is used in place of the closest frequency\(\ \frac{\text{πk}}{60}\).
Thus, the quantity \(\frac{\pi \times 42}{60}\) is replaced
by \[\omega_{42} = 0.3482 \times 2\pi = 2.1878\]. The frequencies
neighbouring \(\omega_{42}\), i.e.~\[\omega_{41}\] and \[\omega_{43}\] are set
to, respectively, \[2.1865 - \frac{1}{60}\] and \[2.1865 + \frac{1}{60}\].

The default frequencies (\(d)\ \)for calendar effect are: 2.188 (monthly
series) and 0.280 (quarterly series). They are computed as:

\(\omega_{\text{ce}} = \frac{2\pi}{7}\left( n - 7 \times \left\lbrack \frac{n}{7} \right\rbrack \right)\), \[1\]
where:

\(n = \frac{365.25}{s}\), \(s = 4\) for quarterly series and \(s = 12\) for
monthly series.

Other frequencies that correspond to trading day frequencies are: 2.714
(monthly series) and 1.292, 1.850, 2.128 (quarterly series).

In particular, the calendar frequency in monthly data (marked in red on the figure below) is very close to the seasonal frequency corresponding to 4
cycles per year \(\text{ω}_{40} = \frac{2}{3}\pi = 2.0944\).

\begin{figure}
\centering
\includegraphics{All_images/UG_A_image19.png}
\caption{Text}
\end{figure}

\textbf{Periodogram with seasonal (grey) and calendar (red)
frequencies highlighted}

This implies that it may be hard to disentangle both effects using the
frequency domain techniques.

\hypertarget{periodogram}{%
\subsubsection{Periodogram}\label{periodogram}}

The periodogram \[ I(\omega_j)\] of \[ \mathbf{X} \in \mathbb{C}^n \] is defined as the squared of the Fourier transform

\[
I(\omega_{j})=a_{j}^{2}=n^{-1}\left| \sum_{t=1}^{n}\mathbf{X_t} e^{-it\omega_j} \right|^{2},
\]

where the Fourier frequencies \[ \omega_{j} \] are given by multiples of the fundamental frequency \[ \frac{2\pi}{n} \]:

\[
\omega_{j}= \frac{2\pi j}{n}, -\pi < \omega_{j} \leq \pi 
\]

An orthonormal basis in \[ \mathbb{R}^n \]:

\[ 
\left\{ e_0, ~~~~~~c_1, s_1, ~~~~~\ldots~~~~~\ , ~~~~c_{[(n-1)/2]}, s_{[(n-1)/2]}~~~~,~~~~~~ e_{n/2}  \right\},
\]
where \[ e_{n/2} \] is excluded if \[ n \] is odd,\\
can be used to project the data and obtain the spectral decomposition

Thus, the periodogram is given by the projection coefficients and represents the contribution of the jth
harmonic to the total sum of squares, as illustrated by Brockwell and Davis (1991):

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2500}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4167}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule
\begin{minipage}[b]{\linewidth}\raggedright
Source
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Degrees of freedom
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\[~~~~\] Sum of squares decomposition
\end{minipage} \\
\midrule
\endhead
Frequency \( \omega_{0} \) & 1 & \( a_{0}^{2}= n^{-1}(\sum_{t=1}^{n}x_t )^2 =I(0)\) \\
Frequency \( \omega_{1} \) & 2 & \( 2 r^{2}_{1} = 2 \left\| a_{1} \right\|^{2} = 2 I(\omega_{1}) \) \\
\( \vdots \) & \( \vdots \) & \( \vdots \) \\
Frequency \( \omega_{k} \) & 2 & \( 2 r^{2}_{k} = 2 \left\| a_{k} \right\|^{2} = 2 I(\omega_{k}) \) \\
\( \vdots \) & \( \vdots \) & \( \vdots \) \\
Frequency \( \omega_{n/2}=\pi \) & 1 & \( a_{n/2}^{2} = I(\pi) \) \\
(excluded if \( n \) is odd) & & \\
\( ========= \) & \( ====== \) & \( ============ \) \\
Total & n & \( \sum_{t=1}^{n}\mathbf{X^2_t} \) \\
\bottomrule
\end{longtable}

\[~~~~\]

In JDemetra+, the periodogram of \[ \mathbf{X} \in \mathbb{R}^n \] is computed for the standardized time series.

\hypertarget{defining-a-f-test}{%
\subsubsection{Defining a F-test}\label{defining-a-f-test}}

Brockwell and Davis (1991, section 10.2) exploit the fact that the periodogram can be expressed as
the projection on the orthonormal basis defined above to derive a test. Thus, under the null hypothesis:

\begin{itemize}
\tightlist
\item
  \[ 2I(\omega_{k})= \| P_{\bar{sp}_{\left\{ c_{k},s_{k} \right\}}} \mathbf{X} \|^{2}  \sim \sigma^{2} \chi^{2}(2) \], for Fourier frequencies
  \[ 0 < \omega_{k}=2\pi k/n < \pi \]
\item
  \[ I(\pi)= \| P_{\bar{sp}_{\left\{ e_{n/2} \right\}}} \mathbf{X} \|^{2}  \sim \sigma^{2} \chi^{2}(1) \], for \[ \pi \]
\end{itemize}

Because \[ I(\omega_{k}) \] is independent from the projection error sum of squares, we can define our F-test statistic as follows:

\begin{itemize}
\tightlist
\item
  \[ \frac{ 2I(\omega_{k})}{\|\mathbf{X}-P_{\bar{sp}_{\left\{ e_0,c_{k},s_{k} \right\}}} \mathbf{X}\|^2} \frac{n-3}{2} \sim F(2,n-3) \], for Fourier frequencies
  \[ 0 < \omega_{k}=2\pi k/n < \pi \]
\item
  \[ \frac{ I(\pi)}{\|\mathbf{X}-P_{\bar{sp}_{\left\{ e_0,e_{n/2} \right\}}} \mathbf{X}\|^2} \frac{n-2}{1} \sim F(1,n-2)\], for \[ \pi \]
\end{itemize}

where
- \[ \|\mathbf{X}-P_{\bar{sp}_{\left\{ e_0,c_{k},s_{k} \right\}}} \mathbf{X}\|^2  = \sum_{i=1}^{n}\mathbf{X^2_i}-I(0)-2I(\omega_{k}) \sim \sigma^{2} \chi^{2}(n-3)\] for Fourier frequencies
\[ 0 < \omega_{k}=2\pi k/n < \pi \]
- \[ \|\mathbf{X}-P_{\bar{sp}_{\left\{ e_0,e_{n/2} \right\}}} \mathbf{X}\|^2 = \sum_{i=1}^{n}\mathbf{X^2_i}-I(0)-I(\pi) \sim \sigma^{2} \chi^{2}(n-2)  \] for \[ \pi \]

Thus, we reject the null if our F-test statistic computed at a given seasonal frequency (different from \[ \pi \]) is larger than \[ F_{1-α}(2,n-3)\].
If we consider \[ \pi  \], our test statistic follows a \[ F_{1-α}(1,n-2)\] distribution.

\hypertarget{seasonality-test}{%
\subsubsection{Seasonality test}\label{seasonality-test}}

The implementation of JDemetra+ considers simultaneously the whole set of seasonal frequencies (1, 2, 3, 4, 5 and 6 cycles per year). Thus, the resulting test-statistic is:

\[
 \frac{ 2I(\pi/6)+ 2I(\pi/3)+ 2I(2\pi/3)+ 2I(5\pi/6)+ \delta I(\pi)}{\left\|\mathbf{X}-P_{\bar{sp}_{\left\{ e_0,c_{1},s_{1},c_{2},s_{2},c_{3},s_{3},c_{4},s_{4},c_{5},s_{5}, \delta e_{n/2} \right\}}} \mathbf{X} \right\|^2} \frac{n-12}{11} \sim F(11-\delta,n-12+\delta) 
\]
where \[ \delta=1 \] if \[ n \] is even and 0 otherwise.

In small samples, the test performs better when the periodogram is evaluated as the exact seasonal frequencies. JDemetra+ modifies the sample size
to ensure the seasonal frequencies belong to the set of Fourier frequencies. This strategy provides a very simple and effective way to eliminate the leakage problem.

Example of how results are displayed:

\begin{figure}
\centering
\includegraphics{All_images/periodogram.png}
\caption{periodtest}
\end{figure}

\hypertarget{references-4}{%
\subsubsection{References}\label{references-4}}

Brockwell, P.J., and R.A. Davis (1991). Times Series: Theory and Methods. Springer Series in Statistics.

\hypertarget{tukey-spectrum-definition}{%
\subsubsection{Tukey Spectrum definition}\label{tukey-spectrum-definition}}

The Tukey spectrum belongs to the class of lag-window estimators. A lag window estimator of the spectral density
\[
f(\omega)=\frac{1}{2\pi}\sum_{k<-\infty}^{\infty}\gamma(k)e^{i k \omega}
\]
is defined as follows:
\[
\hat{f}_{L}(\omega)=\frac{1}{2\pi}\sum_{\left| h \right| \leq r } w(h/r)\hat{\gamma}(h)e^{i h \omega}
\]

where \[\hat{\gamma}(.) \] is the sample autocovariance function, \[w(.)\] is the lag window, and \[r\] is the
truncation lag. \[\left| w(x)\right| \] is always less than or equal to one, \[w(0)=1\] and \[w(x)=0\] for \[\left| x \right| > 1\]. The
simple idea behind this formula is to down-weight the autocovariance function for high lags where \[\hat{\gamma}(h)\] is more unreliable. This estimator
requires choosing \[r\] as a function of the sample size such that \[r/n \rightarrow 0 \] and \[r\rightarrow \infty \] when \[ n \rightarrow \infty \] .
These conditions guarantee that the estimator converges to the true density.

JDemetra+ implements the so-called Blackman-Tukey (or Tukey-Hanning) estimator, which is given
by \[w(h/r)=0.5(1+cos(\pi h/r))\] if \[\left| h/r \right| \leq 1\] and \[0\] otherwise.

The choice of large truncation lags \[r\] decreases the bias, of course, but it also increases the variance of the spectral estimate and decreases the bandwidth.

JDemetra+ allows the user to modify all the parameters of this estimator, including the window function.

\hypertarget{graphical-test}{%
\subsubsection{Graphical Test}\label{graphical-test}}

The current JDemetra+ implementation of the seasonality test is based on a \[F(d_{1},d_{2})\] approximation that has been originally proposed by Maravall (2012) for
TRAMO-SEATS. This test is has been designed for a Blackman-Tukey window based on a particular choices of the truncation lag \[r\] and sample size. Following
this approach, we determine visually significant peaks for a frequency \[\omega_{j}\] when

\[ 
\frac{2 f_{x}(\omega_{j})}{\left[ f_{x}(\omega_{j+1})+ f_{x}(\omega_{j-1}) \right]} \ge CV(\omega_{j}) 
\]

where \[ CV(\omega_{j})\] is the critical value of a \[F(d_{1},d_{2})\] distribution, where the degrees of freedom are determined using simulations. For
\[\omega_{j}= \pi\], we have a significant peak when \[\frac{f_{x}(\omega_{[n/2]})}{\left[ f_{x}(\omega_{[(n-1)/2]})\right]} \ge CV(\omega_{j}) \]

Two significant levels for this test are considered: \[\alpha=0.05\] (code ``t'') and \[\alpha=0.01\] (code ``T'').

As opposed to the \href{\%7B\%7B\%20site.baseurl\%20\%7D\%7D/pages/theory/Tests_ARspectrum.html}{AR spectrum}, which is computed on the basis of the
last \[120\] data points, we will use here all available
observations. Those critical values have been calculated given the recommended truncation lag \[r=79\] for a sample size within the interval \[n \in [80,119]\]
and \[r=112\] for \[n \in [120,300]\] . The \[F\] approximation is less accurate for sample sizes larger than \[300\]. For quarterly data, \[r=44 \], but there
are no recommendations regarding the required sample size

\hypertarget{use-2}{%
\subsubsection{Use}\label{use-2}}

The test can be applied directly to any series by selecting the
option \emph{Statistical Methods \textgreater\textgreater{} Seasonal Adjustment \textgreater\textgreater{} Tools \textgreater\textgreater{} Seasonality Tests}. This is
an example of how results are displayed for the case of a monthly series:

\begin{figure}
\centering
\includegraphics{All_images/spectrum.png}
\caption{tktest}
\end{figure}

JDemetra+ considers critical values for \[ \alpha=1\%\] (code ``T'') and \[ \alpha=5\%\] (code ``t'') at each one of the seasonal
frequencies represented in the table below, e.g.~frequencies \(\frac{\pi}{6},\ \frac{\pi}{3},\ \frac{\pi}{2},\ \frac{2\pi}{3}\text{ and } \frac{5\pi}{6}\ \) corresponding
to 1, 2, 3, 4, 5 and 6 cycles per year in this example, since we are dealing with monthly
data. The codes ``a'' and ``A'' correpond to the so-called
\href{\%7B\%7B\%20site.baseurl\%20\%7D\%7D/pages/theory/Tests_ARspectrum.html}{AR spectrum}, so ignore them for the moment.

\textbf{The seasonal and trading day frequencies by time series
frequency}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2390}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.5346}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2264}}@{}}
\toprule
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Number of months per full period}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Seasonal frequency}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Trading day frequency (radians)}
\end{minipage} \\
\midrule
\endhead
12 & \(\frac{\pi}{6},\frac{\pi}{3},\ \frac{\pi}{2},\frac{2\pi}{3},\ \frac{5\pi}{6},\ \pi\) & \(d\), 2.714 \\
6 & \(\frac{\pi}{3},\frac{2\pi}{3}\), \(\pi\) & \(d\) \\
4 & \(\frac{\pi}{2}\), \(\pi\) & \(d\), 1.292, 1.850, 2.128 \\
3 & \(\pi\) & \(d\) \\
2 & \(\pi\) & \(d\) \\
\bottomrule
\end{longtable}

Currently, only seasonal frequencies are tested, but the program allows you to manually plot the Tukey spectrum and focus your attention on both seasonal
and trading day frequencies.

\hypertarget{references-5}{%
\subsubsection{References}\label{references-5}}

\begin{itemize}
\tightlist
\item
  Tukey, J. (1949). The sampling theory of power spectrum estimates., Proceedings Symposium on Applications of Autocorrelation Analysis to Physical Problems, NAVEXOS-P-735, Office of Naval Research, Washington, 47-69
\end{itemize}

\hypertarget{calendar-correction}{%
\section{Calendar correction}\label{calendar-correction}}

\hypertarget{outliers-and-intervention-variables}{%
\section{Outliers and intervention variables}\label{outliers-and-intervention-variables}}

\hypertarget{pre-adjustment}{%
\section{Pre-adjustment}\label{pre-adjustment}}

\hypertarget{decomposition}{%
\section{Decomposition}\label{decomposition}}

\hypertarget{x-11-moving-average-based-decomposition}{%
\subsection{X-11 moving average based decomposition}\label{x-11-moving-average-based-decomposition}}

A complete documentation of the X-11 method is available in LADIRAY, D.,
and QUENNEVILLE, B. (2001). The X-11 program is the result of a long
tradition of non-parametric smoothing based on moving averages, which
are weighted averages of a moving span of a time series (see hereafter).
Moving averages have two important drawbacks:

\begin{itemize}
\item
  They are not resistant and might be deeply impacted by outliers;
\item
  The smoothing of the ends of the series cannot be done except with asymmetric moving averages which introduce phase-shifts and delays in the detection of turning points.
\end{itemize}

These drawbacks adversely affect the X-11 output and stimulate the
development of this method. To overcome these flaws first the series are
modelled with a RegARIMA model that calculates forecasts and estimates
the regression effects. Therefore, the seasonal adjustment process is
divided into two parts.

\begin{itemize}
\item
  In a first step, the RegARIMA model is used to clean the series from
  \textgreater{} non-linearities, mainly outliers and calendar effects. A global
  \textgreater{} ARIMA model is adjusted to the series in order to compute the
  \textgreater{} forecasts.
\item
  In a second step, an enhanced version of the X-11 algorithm is used
  \textgreater{} to compute the trend, the seasonal component and the irregular
  \textgreater{} component.
\end{itemize}

\begin{figure}
\centering
\includegraphics{All_images/UG_A_image13.png}
\caption{Text}
\end{figure}

\textbf{The flow diagram for seasonal adjustment with X-13ARIMA-SEATS using the X-11 algorithm.}

\hypertarget{moving-averages}{%
\subsubsection{Moving averages}\label{moving-averages}}

The moving average of coefficient \(\theta_{i}\) is
defined as:

\[M\left( X_{t} \right) = \sum_{k = - p}^{+ f}\theta_{k}X_{t + k}\] \[1\]

The value at time \(t\) of the series is therefore replaced by a weighted
average of \(p\) "past" values of the series, the current value, and \(f\)
"future" values of the series. The quantity \(p + f + 1\ \)is called the
moving average order. When \(p\) is equal to \(f\), that is, when
the number of points in the past is the same as the number of points in
the future, the moving average is said to be centred. If, in addition,
\(\theta_{- k} = \theta_{k}\) for any \(k\), the moving average \(M\)
is said to be symmetric. One of the simplest moving averages is the
symmetric moving average of order \(P = 2p + 1\) where all the weights are
equal to\(\ \frac{1}{P}\).

This moving average formula works well for all time series observations,
except for the first \(p\) values and last \(f\) values. Generally, with a
moving average of order \(p + f + 1\ \)calculated for instant
\(t\)nwith points \(p\) in the past and points \(f\) in the
future, it will be impossible to smooth out the first \(p\) values and the
last \(f\) values of the series because of lack of input to the moving
average formula.

In the X-11 method, symmetric moving averages play an important role as
they do not introduce any phase-shift in the smoothed series. But, to
avoid losing information at the series ends, they are either
supplemented by \emph{ad hoc} asymmetric moving averages or applied on the
series extended by forecasts.

For the estimation of the seasonal component, X-13ARIMA-SEATS uses
\(P \times Q\) composite moving averages, obtained by composing a simple
moving average of order \(P\), which coefficients are all equal to
\(\frac{1}{P}\), and a simple moving average of order \(Q\), which
coefficients are all equal to \(\frac{1}{Q}\).

The composite moving averages are widely used by the X-11 method. For an
initial estimation of trend X-11 method uses a \(2 \times 4\) moving
average in case of a quarterly time series while for a monthly time
series a \(2 \times 12\ \)moving average is applied. The \(2 \times 4\)
moving average is an average of order 5 with coefficients
\[\frac{1}{8}\left\{1, 2, 2, 2, 1\right\}\]. It eliminates frequency
\(\frac{\pi}{2}\) corresponding to period 4 and therefore it is suitable
for seasonal adjustment of the quarterly series with a constant
seasonality. The \(2 \times 12\) moving average, with coefficients
\[\frac{1}{24}\left\{1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1\right\} \]that
retains linear trends, eliminates order-\(12\) constant seasonality and
minimises the variance of the irregular component. The \(2 \times 4\) and
\(2 \times 12\) moving averages are also used in the X-11 method to
normalise the seasonal factors. The composite moving averages are also
used to extract the seasonal component. These, which are used in the
purely automatic run of the X-11 method (without any intervention from
the user) are \(3 \times 3\), \(3 \times 5\) and \(3 \times 9\).

In the estimation of the trend also Henderson moving averages are used.
These filters have been chosen for their smoothing properties. The
coefficients of a Henderson moving average of order \(2p + 1\) may be
calculated using the formula:

\(\theta_{i} = \frac{315\left\lbrack \left( n - 1 \right)^{2} - i^{2} \right\rbrack\left\lbrack n^{2} - i^{2} \right\rbrack\left\lbrack \left( n + 1 \right)^{2} - i^{2} \right\rbrack\left\lbrack {3n}^{2} - 16 - 11i^{2} \right\rbrack}{8n\left( n^{2} - 1 \right)\left( {4n}^{2} - 1 \right)\left( {4n}^{2} - 9 \right)\left( 4n^{2} - 25 \right)}\), \[2\]

where: \(n = p + 2\)\(n = p + 2\).

\hypertarget{the-basic-algorithm-of-the-x-11-method}{%
\subsubsection{The basic algorithm of the X-11 method}\label{the-basic-algorithm-of-the-x-11-method}}

The X-11 method is based on an iterative principle of estimation of the
different components using appropriate moving averages at each step of
the algorithm. The successive results are saved in tables. The list of
the X-11 tables displayed in JDemetra+ is included at the end of this
section.

The basic algorithm of the X-11 method will be presented for a monthly
time series \(X_{t}\) that is assumed to be decomposable into trend,
seasonality and irregular component according to an additive model
\(X_{t} = TC_{t} + S_{t} + I_{t}\).

A simple seasonal adjustment algorithm can be thought of in eight steps.
The steps presented below are designed for the monthly time series. In
the algorithm that is run for the quarterly time series the \(2 \times 4\)
moving average instead of the \(2 \times 12\) moving average is used.

\textbf{\emph{Step 1: Estimation of Trend by}} \(\mathbf{2 \times 12}\) \textbf{\emph{moving
average:}}

\(TC_{t}^{(1)} = M_{2 \times 12}(X_{t})\) \[3\]

\textbf{\emph{Step 2: Estimation of the Seasonal-Irregular component:}}

\(\left( S_{t} + I_{t} \right)^{(1)} = X_{t} - \text{TC}_{t}^{(1)}\) \[4\]

\textbf{\emph{Step 3: Estimation of the Seasonal component by}} \(\mathbf{3 \times 3}\)
\textbf{\emph{moving average over each month:}}

\(S_{t}^{(1)} - M_{3 \times 3}\left\lbrack \left( S_{t} + I_{t} \right)^{(1)} \right\rbrack\) \[5\]

The moving average used here is a \(3 \times 3\) moving average over
\(5\) terms, with coefficients
\[\frac{1}{9} \left\{1, 2, 3, 2, 1 \right\}\]. The seasonal component
is then centred using a \(2 \times 12\) moving average.

\[
   \widetilde{S}_{t}^{(1)} = S_{t}^{(1)} - M_{2 \times 12}\left( S_{t}^{(1)} \right)
  \] \[6\]

\textbf{\emph{Step 4: Estimation of the seasonally adjusted series:}}

\[
  SA_{t}^{\left( 1 \right)} = \left( \text{TC}_{t} + I_{t} \right)^{(1)} = X_{t} - {\widetilde{S}}_{t}^{(1)}
  \] \[7\]

This first estimation of the seasonally adjusted series must, by
construction, contain less seasonality. The X-11 method again executes
the algorithm presented above, changing the moving averages to take this
property into account.

\textbf{\emph{Step 5: Estimation of Trend by 13-term Henderson moving average:}}

\[
  TC_{t}^{(2)} = H_{13}\left( \text{SA}_{t}^{\left( 1 \right)} \right)
  \] \[8\]

Henderson moving averages, while they do not have special properties in
terms of eliminating seasonality (limited or none at this stage), have a
very good smoothing power and retain a local polynomial trend of degree
\(2\) and preserve a local polynomial trend of degree \(3\).

\textbf{\emph{Step 6: Estimation of the Seasonal-Irregular component:}}

\[
  \left( S_{t} + I_{t} \right)^{(2)} = X_{t} - \text{TC}_{t}^{(2)}
  \] \[9\]

\textbf{\emph{Step 7: Estimation of the Seasonal component by}} \(\mathbf{3 \times 5}\)
\textbf{\emph{moving average over each month:}}

\[S_{t}^{(2)} - M_{3 \times 3}\left\lbrack \left( S_{t} + I_{t} \right)^{(2)} \right\rbrack\] \[10\]

The moving average used here is a \(3 \times 5\) moving average over \(7\)
terms, of coefficients
\[\frac{1}{15} \left\{ 1,\ 2,\ 3,\ 3,\ 3,\ 2,\ 1 \right\}\] and retains
linear trends. The coefficients are then normalised such that their sum
over the whole \(12\)-month period is approximately cancelled out:

\[{ \widetilde{S}}_{t}^{(2)} = S_{t}^{(2)} - M_{2 \times 12}\left( S_{t}^{(2)} \right)\] \[11\]

\textbf{\emph{Step 8: Estimation of the seasonally adjusted series:}}

\[SA_{t}^{\left( 2 \right)} = \left(TC_{t} + I_{t} \right)^{(2)} = X_{t} - {\widetilde{S}}_{t}^{(2)}\] \[12\]

The whole difficulty lies, then, in the choice of the moving averages
used for the estimation of the trend in steps \(1\) and \(5\) on the one
hand, and for the estimation of the seasonal component in steps \(3\) and
\(5\). The course of the algorithm in the form that is implemented in
JDemetra+ is presented in the figure below. The adjustment for trading day
effects, which is present in the original X-11 program, is omitted here,
as since calendar correction is performed by the RegARIMA model,
JDemetra+ does not perform further adjustment for these effects in the
decomposition step.

\textbf{A workflow diagram for the X-11 algorithm based upon training material from the Deutsche Bundesbank}

\hypertarget{the-iterative-principle-of-x-11}{%
\paragraph{\texorpdfstring{\textbf{The iterative principle of X-11}}{The iterative principle of X-11}}\label{the-iterative-principle-of-x-11}}

To evaluate the different components of a series, while taking into
account the possible presence of extreme observations, X-11 will proceed
iteratively: estimation of components, search for disruptive effects in
the irregular component, estimation of components over a corrected
series, search for disruptive effects in the irregular component, and so
on.

The Census X-11 program presents four processing stages (A, B, C, and
D), plus 3 stages, E, F, and G, that propose statistics and charts and
are not part of the decomposition per se. In stages B, C and D the basic
algorithm is used as is indicated in the figure below.

\textbf{A workflow diagram for the X-11 algorithm implemented in
JDemetra+. Source: Based upon training material from the Deutsche
Bundesbank}

\begin{itemize}
\tightlist
\item
  \textbf{Part A: Pre-adjustments}
\end{itemize}

This part, which is not obligatory, corresponds in X-13ARIMA-SEATS to
the first cleaning of the series done using the RegARIMA facilities:
detection and estimation of outliers and calendar effects (trading day
and Easter), forecasts and backcasts{[}\^{}61{]} of the series. Based on these
results, the program calculates prior adjustment factors that are
applied to the raw series. The series thus corrected, Table B1 of the
printouts, then proceeds to part B.

\begin{itemize}
\tightlist
\item
  \textbf{Part B: First automatic correction of the series}
\end{itemize}

This stage consists of a first estimation and down-weighting of the
extreme observations and, if requested, a first estimation of the
calendar effects. This stage is performed by applying the basic
algorithm detailed earlier. These operations lead to Table B20,
adjustment values for extreme observations, used to correct the
unadjusted series and result in the series from Table C1.

\begin{itemize}
\tightlist
\item
  \textbf{Part C: Second automatic correction of the series}
\end{itemize}

Applying the basic algorithm once again, this part leads to a more
precise estimation of replacement values of the extreme observations
(Table C20). The series, finally "cleaned up", is shown in Table D1 of
the printouts.

\begin{itemize}
\tightlist
\item
  \textbf{Part D: Seasonal adjustment}
\end{itemize}

This part, at which our basic algorithm is applied for the last time, is
that of the seasonal adjustment, as it leads to final estimates:

\begin{itemize}
\item
  of the seasonal component (Table D10);
\item
  of the seasonally adjusted series (Table D11);
\item
  of the trend component (Table D12);
\item
  of the irregular component (Table D13).
\end{itemize}

\begin{itemize}
\tightlist
\item
  \textbf{Part E: Components modified for large extreme values}
\end{itemize}

Parts E includes:

\begin{itemize}
\item
  Components modified for large extreme values;
\item
  Comparison the annual totals of the raw time series and seasonally adjusted time series;
\item
  Changes in the final seasonally adjusted series;
\item
  Changes in the final trend;
\item
  Robust estimation of the final seasonally adjusted series.
\end{itemize}

The results from part E are used in part F to calculate the quality measures.

\begin{itemize}
\tightlist
\item
  \textbf{Part F: Seasonal adjustment quality measures}
\end{itemize}

Part F contains statistics for judging the quality of the seasonal
adjustment. JDemetra+ presents selected output for part F, i.e.:

\begin{itemize}
\item
  M and Q statistics;
\item
  Tables.
\end{itemize}

\begin{itemize}
\tightlist
\item
  \textbf{Part G: Graphics}
\end{itemize}

Part G presents spectra estimated for:

\begin{itemize}
\item
  Raw time series adjusted a priori (Table B1);
\item
  Seasonally adjusted time series modified for large extreme values
  (Table E2);
\item
  Final irregular component adjusted for large extreme values (Table
  E3).
\end{itemize}

Originally, graphics were displayed in character mode. In JDemetra+,
these graphics are replaced favourably by the usual graphics software.

\textbf{The Henderson moving average and the trend estimation}

In iteration B (Table B7), iteration C (Table C7) and iteration D (Table
D7 and Table D12) the trend component is extracted from an estimate of
the seasonally adjusted series using Henderson moving averages. The
length of the Henderson filter is chosen automatically by
X-13ARIMA-SEATS in a two-step procedure.

It is possible to specify the length of the Henderson moving average to
be used. X-13ARIMA-SEATS provides an automatic choice between a 9-term,
a 13-term or a 23-term moving average. The automatic choice of the order
of the moving average is based on the value of an indicator called
\(\frac{\overline{I}}{\overline{C}}\ \)ratio which compares the magnitude
of period-on-period movements in the irregular component with those in
the trend. The larger the ratio, the higher the order of the moving
average selected. Moreover, X-13ARIMA-SEATS allows the user to choose
manually any odd‑numbered Henderson moving average. The procedure used
in each part is very similar; the only differences are the number of
options available and the treatment of the observations in the both ends
of the series. The procedure below is applied for a monthly time series.

In order to calculate \(\frac{\overline{I}}{\overline{C}}\ \) ratio a
first decomposition of the SA series (seasonally adjusted) is computed
using a 13-term Henderson moving average.

For both the trend (\(C\)) and irregular (\(I\)) components, the average of
the absolute values for monthly growth rates (multiplicative model) or
for monthly growth (additive model) are computed. They are denoted as
\(\overline{C}\ \)and \(\overline{I}\), receptively, where
\(\overline{C} = \frac{1}{n - 1}\sum_{t = 2}^{n}\left| C_{t} - C_{t - 1} \right|\)
and
\(\overline{I} = \frac{1}{n - 1}\sum_{t = 2}^{n}\left| I_{t} - I_{t - 1} \right|\).

Then the value of \(\frac{\overline{I}}{\overline{C}}\ \) ratio is checked
and in iteration B:

\begin{itemize}
\item
  If the ratio is smaller than 1, a 9-term Henderson moving average is selected;
\item
  Otherwise, a 13-term Henderson moving average is selected.
\end{itemize}

Then the trend is computed by applying the selected Henderson filter to
the seasonally adjusted series from Table B6. The observations at the
beginning and at the end of the time series that cannot be computed by
means of symmetric Henderson filters are estimated by ad hoc asymmetric
moving averages.

In iterations C and D:

\begin{itemize}
\item
  If the ratio is smaller than 1, a 9-term Henderson moving average is selected;
\item
  If the ratio is greater than 3.5, a 23-term Henderson moving average is selected.
\item
  Otherwise, a 13-term Henderson moving average is selected.
\end{itemize}

The trend is computed by applying selected Henderson filter to the
seasonally adjusted series from Table C6, Table D7 or Table D12,
accordingly. At the both ends of the series, where a central Henderson
filter cannot be applied, the asymmetric ends weights for the 7 term
Henderson filter are used.

\hypertarget{choosing-the-composite-moving-averages-when-estimating-the-seasonal-component}{%
\paragraph{\texorpdfstring{\textbf{Choosing the composite moving averages when estimating the seasonal component}}{Choosing the composite moving averages when estimating the seasonal component}}\label{choosing-the-composite-moving-averages-when-estimating-the-seasonal-component}}

In iteration D, Table D10 shows an estimate of the seasonal factors
implemented on the basis of the modified SI (Seasonal -- Irregular)
factors estimated in Tables D4 and D9bis. This component will have to be
smoothed to estimate the seasonal component; depending on the importance
of the irregular in the SI component, we will have to use moving
averages of varying length as in the estimate of the Trend/Cycle where
the \(\frac{\overline{I}}{\overline{C}}\ \) ratio was used to select the
length of the Henderson moving average. The estimation includes several
steps.

\textbf{\emph{Step 1: Estimating the irregular and seasonal components}}

An estimate of the seasonal component is obtained by smoothing, month by
month and therefore column by column, Table D9bis using a simple 7-term
moving average, i.e.~of coefficients
\[\frac{1}{7} \left\{1,\ 1,\ 1,\ 1,\ 1,\ 1,\ 1\right\}\]. In order not to
lose three points at the beginning and end of each column, all columns
are completed as follows. Let us assume that the column that corresponds
to the month is composed of \(N\) values
\[
\left\{ x_{1},\ x_{2},\ x_{3},\ \ldots x_{N - 1},\ x_{N} \right\}.
\]
It
will be transformed into a series
\[\left\{ {x_{- 2},x_{- 1}{,x}_{0},x}_{1},\ x_{2},\ x_{3},\ \ldots x_{N - 1},\ x_{N},x_{N + 1},\ x_{N + 1},\ x_{N + 2},\ x_{N + 3} \right\}\\] with \[x_{- 2} = x_{- 1} = x_{0} = \frac{x_{1} + x_{2} + x_{3}}{3}\] and \[x_{N + 1} = x_{N + 2} = x_{N + 3} = \frac{x_{N} + x_{N - 1} + x_{N - 2}}{3}\].
We then have the required estimates: \(S = M_{7}(D9bis)\) and
\(I = D9bis - S\).

\textbf{\emph{Step 2: Calculating the Moving Seasonality Ratios}}

For each \(i^{\text{th}}\) month the mean annual changes for each
component is obtained by calculating
\[{\overline{S}}_{i} = \frac{1}{N_{i} - 1}\sum_{t = 2}^{N_{i}}\left| S_{i,t} - S_{i,t - 1} \right|\]
and
\[{\overline{I}}_{i} = \frac{1}{N_{i} - 1}\sum_{t = 2}^{N_{i}}\left| I_{i,t} - I_{i,t - 1} \right|\],
where \(N_{i}\) refers to the number of months \(\text{i}\)in the data,
and the moving seasonality ratio of month \(i\):
\[MSR_{i} = \frac{\ {\overline{I}}_{i}}{ {\overline{S}}_{i}}\].
These ratios are presented in \emph{Details} of the \emph{Quality Measures} node
under the \emph{Decomposition (X11)} section. These ratios are used to
compare the year-on-year changes in the irregular component with those
in the seasonal component. The idea is to obtain, for each month, an
indicator capable of selecting the appropriate moving average for the
removal of any noise and providing a good estimate of the seasonal
factor. The higher the ratio, the more erratic the series, and the
greater the order of the moving average should be used. As for the rest,
by default the program selects the same moving average for each month,
but the user can select different moving averages for each month.

\textbf{\emph{Step 3: Calculating the overall Moving Seasonality Ratio}}

The overall Moving Seasonality Ratio is calculated as follows:

\[\text{MSR}_{i} = \frac{\sum_{i}^{}{N_{i}\ }\ {\overline{I}}_{i}}{\sum_{i}^{}N_{i}{\overline{S}}_{i}}\] \[13\]

\textbf{\emph{Step 4: Selecting a moving average and estimating the seasonal
component}}

Depending on the value of the ratio, the program automatically selects a
moving average that is applied, column by column (i.e.~month by month)
to the Seasonal/Irregular component in Table D8 modified, for extreme
values, using values in Table D9.

The default selection procedure of a moving average is based on the
Moving Seasonality Ratio in the following way:

\begin{itemize}
\item
  If this ratio occurs within zone A (MSR \textless{} 2.5), a \(3 \times 3\) moving average is used; if it occurs within zone C (3.5 \textless{} MSR \textless{} 5.5), a \(3 \times 5\) moving average is selected; if it occurs within zone E (MSR~\textgreater~6.5), a \(3 \times 9\) moving average is used;
\item
  If the MSR occurs within zone B or D, one year of observations is removed from the end of the series, and the MSR is recalculated.
  \textgreater{} If the ratio again occurs within zones B or D, we start over
  \textgreater{} again, removing a maximum of five years of observations. If this
  \textgreater{} does not work, i.e.~if we are again within zones B or D, a
  \textgreater{} \(3 \times 5\) moving average is selected.
\end{itemize}

The chosen symmetric moving average corresponds, as the case may be 5
(\(3 \times 3\)), 7 \((3 \times 5)\ \)or 11 (\(3 \times 9\)\(3 \times 9)\)
terms, and therefore does not provide an estimate for the values of
seasonal factors in the first 2 (or 3 or 5) and the last 2 (or 3 or 5)
years. These are then calculated using associated asymmetric moving
averages.

\textbf{Moving average selection procedure, source: DAGUM, E. B.(1999)}

\hypertarget{identification-and-replacement-of-extreme-values}{%
\paragraph{\texorpdfstring{\textbf{Identification and replacement of extreme values}}{Identification and replacement of extreme values}}\label{identification-and-replacement-of-extreme-values}}

X-13ARIMA-SEATS detects and removes outliers in the RegARIMA part.
However, if there is a seasonal heteroscedasticity in a time series i.e.
the variance of the irregular component is different in different
calendar months. Examples for this effect could be the weather and
snow-dependent output of the construction sector in Germany during
winter, or changes in Christmas allowances in Germany and resulting from
this a transformation in retail trade turnover before Christmas. The
ARIMA model is not on its own able to cope with this characteristic. The
practical consequence is given by the detection of additional extreme
values by X-11. This may not be appropriate if the seasonal
heteroscedasticity is produced by political interventions or other
influences. The ARIMA models assume a constant variance and are
therefore not by themselves able to cope with this problem. Choosing
longer (in the case of diverging weather conditions in the winter time
for the construction sector) or shorter filters (in the case of a
changing pattern of retail trade turnover in the Christmas time) may be
reasonable in such cases. It may even be sensible to take into account
the possibility of period-specific (e.g.~month-specific) standard
deviations, which can be done by changing the default settings of the
\textbf{calendarsigma} parameter (see \href{../reference-manual/sa-spec-X13.html}{Specifications-X13} section). The value of the
\textbf{calendarsigma} parameter will have an impact on the method of
calculation of the moving standard deviation in the procedure for
extreme values detection presented below.

\textbf{\emph{Step 1: Estimating the seasonal component}}

The seasonal component is estimated by smoothing the SI component
separately for each period using a \(3 \times 3\) moving average, i.e.:

\[
  \frac{1}{9} \times \begin{Bmatrix}   
  1,0,0,0,0,0,0,0,0,0,0,0, \\           
  2,0,0,0,0,0,0,0,0,0,0,0, \\           
  3,0,0,0,0,0,0,0,0,0,0,0, \\           
  2,0,0,0,0,0,0,0,0,0,0,0, \\           
  1,0,0,0,0,0,0,0,0,0,0,0, \\           
  \end{Bmatrix}
  \] \[14\]

\textbf{\emph{Step 2: Normalizing the seasonal factors}}

The preliminary seasonal factors are normalized in such a way that for
one year their average is equal to zero (additive model) or to unity
(multiplicative model).

\textbf{\emph{Step 3: Estimating the irregular component}}

The initial normalized seasonal factors are removed from the
Seasonal-Irregular component to provide an estimate of the irregular
component.

\textbf{\emph{Step 4: Calculating a moving standard deviation}}

By default, a moving standard deviation of the irregular component is
calculated at five-year intervals. Each standard deviation is associated
with the central year used to calculate it. The values in the central
year, which in the absolute terms deviate from average by more than the
\textbf{Usigma} parameter
are marked as extreme values and assigned a zero weight. After excluding
the extreme values the moving standard deviation is calculated once
again.

\textbf{\emph{Step 5: Detecting extreme values and weighting the irregular}}

The default settings for assigning a weight to each value of irregular
component are:

\begin{itemize}
\item
  Values which are more than \textbf{Usigma} (2.5, by default) standard
  deviations away (in the absolute terms) from the 0 (additive) or 1
  (multiplicative) are assigned a zero weight;
\item
  Values which are less than 1.5 standard deviations away (in the
  absolute terms) from the 0 (additive) or 1 (multiplicative) are
  assigned a full weight (equal to one);
\item
  Values which lie between 1.5 and 2.5 standard deviations away (in the absolute terms) from the 0 (additive) or 1 (multiplicative) are assigned a weight that varies linearly between 0 and 1 depending on their position.
\end{itemize}

The default boundaries for the detection of the extreme values can be
changed with \textbf{LSigma} and \textbf{USigma} parameters

\textbf{\emph{Step 6: Adjusting extreme values of the seasonal-irregular
component}}

Values of the SI component are considered extreme when a weight less
than 1 is assigned to their irregular. Those values are replaced by a
weighted average of five values:

\begin{itemize}
\item
  The value itself with its weight;
\item
  The two preceding values, for the same period, having a full weight(if available);
\item
  The next two values, for the same period, having full a weight (if available).
\end{itemize}

When the four full-weight values are not available, then a simple
average of all the values available for the given period is taken.

This general algorithm is used with some modification in parts B and C
for detection and replacement of extreme values.

\hypertarget{x-11-tables}{%
\paragraph{\texorpdfstring{\textbf{X-11 tables}}{X-11 tables}}\label{x-11-tables}}

The list of tables produced by JDemetra+ is presented below. It is not
identical to the output produced by the original X-11 program.

\textbf{Part A -- Preliminary Estimation of Outliers and Calendar Effects.}

This part includes prior modifications to the original data made in the
RegARIMA part:

\begin{itemize}
\item
  Table A1 -- Original series;
\item
  Table A1a -- Forecast of Original Series;
\item
  Table A2 -- Leap year effect;
\item
  Table A6 -- Trading Day effect (1 or 6 variables);
\item
  Table A7 -- The Easter effect;
\item
  Table A8 -- Total Outlier Effect;
\item
  Table A8i -- Additive outlier effect;
\item
  Table A8t -- Level shift effect;
\item
  Table A8s -- Transitory effect;
\item
  Table A9 -- Effect of user-defined regression variables assigned to the seasonally adjusted series or for which the component has not been defined;
\item
  Table 9sa -- Effect of user-defined regression variables assigned to the seasonally adjusted series;
\item
  Table9u -- Effect of user-defined regression variables for which the component has not been defined.
\end{itemize}

\textbf{Part B -- Preliminary Estimation of the Time Series Components:}

\begin{itemize}
\item
  Table B1 -- Original series after adjustment by the RegARIMA model;
\item
  Table B2 -- Unmodified Trend (preliminary estimation using composite moving average);
\item
  Table B3 -- Unmodified Seasonal -- Irregular Component (preliminary estimation);
\item
  Table B4 -- Replacement Values for Extreme SI Values;
\item
  Table B5 -- Seasonal Component;
\item
  Table B6 -- Seasonally Adjusted Series;
\item
  Table B7 -- Trend (estimation using Henderson moving average);
\item
  Table B8 -- Unmodified Seasonal -- Irregular Component;
\item
  Table B9 -- Replacement Values for Extreme SI Values;
\item
  Table B10 -- Seasonal Component;
\item
  Table B11 -- Seasonally Adjusted Series;
\item
  Table B13 -- Irregular Component;
\item
  Table B17 -- Preliminary Weights for the Irregular;
\item
  Table B20 -- Adjustment Values for Extreme Irregulars.
\end{itemize}

\textbf{Part C -- Final Estimation of Extreme Values and Calendar Effects:}

\begin{itemize}
\item
  Table C1 -- Modified Raw Series;
\item
  Table C2 -- Trend (preliminary estimation using composite moving average);
\item
  Table C4 -- Modified Seasonal -- Irregular Component;
\item
  Table C5 -- Seasonal Component;
\item
  Table C6 -- Seasonally Adjusted Series;
\item
  Table C7 -- Trend (estimation using Henderson moving average);
\item
  Table C9 -- Seasonal -- Irregular Component;
\item
  Table C10 -- Seasonal Component;
\item
  Table C11 -- Seasonally Adjusted Series;
\item
  Table C13 -- Irregular Component;
\item
  Table C20 -- Adjustment Values for Extreme Irregulars.
\end{itemize}

\textbf{Part D -- Final Estimation of the Different Components:}

\begin{itemize}
\item
  Table D1 -- Modified Raw Series;
\item
  Table D2 -- Trend (preliminary estimation using composite moving average);
\item
  Table D4 -- Modified Seasonal -- Irregular Component;
\item
  Table D5 -- Seasonal Component;
\item
  Table D6 -- Seasonally Adjusted Series;
\item
  Table D7 -- Trend (estimation using Henderson moving average);
\item
  Table D8 -- Unmodified Seasonal -- Irregular Component;
\item
  Table D9 -- Replacement Values for Extreme SI Values;
\item
  Table D10 -- Final Seasonal Factors;
\item
  Table D10A -- Forecast of Final Seasonal Factors;
\item
  Table D11 -- Final Seasonally Adjusted Series;
\item
  Table D11A -- Forecast of Final Seasonally Adjusted Series;
\item
  Table D12 -- Final Trend (estimation using Henderson moving average);
\item
  Table D12A -- Forecast of Final Trend Component;
\end{itemize}

\begin{itemize}
\item
  Table D13 -- Final Irregular Component;
\item
  Table D16 -- Seasonal and Calendar Effects;
\item
  Table D16A -- Forecast of Seasonal and Calendar Component;
\item
  Table D18 -- Combined Calendar Effects Factors.
\end{itemize}

\textbf{Part E -- Components Modified for Large Extreme Values:}

\begin{itemize}
\item
  Table E1 -- Raw Series Modified for Large Extreme Values;
\item
  Table E2 -- SA Series Modified for Large Extreme Values;
\item
  Table E3 -- Final Irregular Component Adjusted for Large Extreme Values;
\item
  Table E11 -- Robust Estimation of the Final SA Series.
\end{itemize}

\textbf{Part F -- Quality indicators:}

\begin{itemize}
\item
  Table F2A -- Changes, in the absolute values, of the principal components;
\item
  Table F2B -- Relative contribution of components to changes in the raw series;
\item
  Table F2C -- Averages and standard deviations of changes as a function of the time lag;
\item
  Table F2D -- Average duration of run;
\item
  Table F2E -- I/C ratio for periods span;
\item
  Table F2F -- Relative contribution of components to the variance of the stationary part of the original series;
\item
  Table F2G -- Autocorrelogram of the irregular component.
\end{itemize}

\hypertarget{filter-length-choice}{%
\subsubsection{Filter length choice}\label{filter-length-choice}}

A seasonal filter is a weighted average of a moving span of fixed length
within a time series that can be used to remove a fixed seasonal pattern.
X-13ARIMA-SEATS uses several of these filters, according to the needs of
the different stages of the program. As only X-13ARIMA-SEATS allows the
user to manually select seasonal filters, this case study can be
applied only to the X-13ARIMA-SEATS specifications.

The automatic seasonal adjustment procedure uses the default
options to select the most appropriate moving average. However there are
occasions when the user will need to specify a different seasonal moving\\
average to that identified by the program. For example, if the SI values
do not closely follow the seasonal component, it may be appropriate to
use a shorter moving average. Also the presence of sudden breaks in
the seasonal pattern -- e.g.~due to changes in the methodology -- can
negatively impact on the automatic selection of the most appropriate
seasonal filter. In such cases the usage of short seasonal filters in
the selected months or quarters can be considered. Usually, a shorter
seasonal filter \((3 \times 1)\) allows seasonality to change very rapidly
over time. However, a very short seasonal filter should not normally be
used, as it might often lead to large revisions as new data becomes
available. If a short filter is to be used it will usually be limited to one
month/quarter with a known reason for wanting to capture
a rapidly changing seasonality.

In the standard situation one seasonal filter is applied to all
individual months/quarters. The estimation of seasonal movements is
therefore based on the sample windows of equal lengths for each
individual month/quarter (i.e.~for each month/quarter the seasonal
filter length or the number of years representing the major part of the
seasonal filter weights is identical). This approach relies on the
assumption that the number of past periods in which the conditions
causing seasonal behaviour are sufficiently homogenous is the same in
all months/quarters. However, this assumption does not always hold.
Seasonal causes may change in one month, while staying the same in
others{[}\^{}1{]}. For instance, seasonal heteroskedasticity might require
different filter lengths in different months or quarters.

Another interesting example is industrial production in Germany. It can
be influenced by school holidays, since many employees have
school-age children, which interrupt their working pattern during these
school holidays. Consequently, businesses may temporarily suspend or lower
production during these periods. Since school holidays do not occur at the same time
throughout Germany and their timing varies from year to year in the
individual federal states, the effect is not completely captured by
seasonal adjustment. And since school holidays are treated as usual
working days, these effects are not captured by calendar adjustment
either. The majority of school holidays in Germany can take place either
in July or in August. This yields higher variances in the irregular
component for these months compared to the rest of the year. Therefore,
in this case a longer seasonal filter is used for these months to account for
this.

Another example might be given by German retail trade. Due to changes in
the consumers' behaviour around Christmas -- possibly more gifts of money
-- the seasonal peak in December has become steadily less pronounced. To
account for this moving seasonality, shorter seasonal filters in
December than during the rest of the year need to be applied.

JDemetra+ offers the options to assign a different seasonal filter
length to each period (month or quarter). The program offers these
options in the \emph{single spec} mode as well as in the \emph{multispec} mode,
albeit they are available only in the \emph{Specifications} window, after a
document is created.

\hypertarget{model-based-decomposition}{%
\subsection{Model based decomposition}\label{model-based-decomposition}}

SEATS is a program for estimating unobserved components in a time
series. It follows the ARIMA-model-based (AMB) method, developed from
the work of CLEVELAND, W.P., and TIAO, G.C. (1976), BURMAN, J.P. (1980),
HILLMER, S.C., and TIAO, G.C. (1982), BELL, W.R., and HILLMER, S.C.
(1984) and MARAVALL, A., and PIERCE, D.A. (1987).

In JDemetra+ the input for the model based signal extraction procedure
is always provided by TRAMO and includes the original series \(y_{t}\),
the linearized series \(x_{t}\) (i.e.~the original series \(y_{t}\ \)with
the deterministic effects removed), the ARIMA model for the stochastic
(linearized) time series \(x_{t}\) and the deterministic effects (calendar
effects, outliers and other regression variable effects)\footnote{In the original software SEATS can be used either with TRAMO,
  operating on the input received from the latter, or alone, fitting
  an ARIMA model to the series.}. SEATS
decomposes the linearized series (and the ARIMA model) into trend,
seasonal, transitory and irregular components, provides forecasts for
these components, together with the associated standard errors, and
finally assign the deterministic effects to each component yielding the
\emph{final} components\footnote{GÓMEZ, V., and MARAVALL, A. (1998).}. The Minimum Mean Square Error (MMSE)
estimators of the components are computed with a Wiener-Kolmogorov
filter applied to the finite series extended with forecasts and
backcasts\footnote{GÓMEZ, V., and MARAVALL, A. (1997).}.

One of the fundamental assumptions made by SEATS is that the linearized
time series \(x_{t}\) follows the ARIMA model

\[\phi(B)\delta\left( B \right)x_{t} = \theta(B)a_{t}\] \[1\]

where:

\(B\) -- the backshift operator \((Bx_{t} = x_{t - 1})\);

\(\delta\left( B \right)\) -- a non-stationary autoregressive (AR)
polynomial in \(B\) (unit roots);

\(\theta\left( B \right)\) -- an invertible moving average (MA) polynomial
in \(B\) and in \(B^{S}\), which can be expressed in the multiplicative form

\(\left( 1 + \vartheta_{1}B + \ldots{+ \ \vartheta}{q}B^{q} \right)\left( \ 1 + \Theta{1}B^{s} + \ldots{+ \ \Theta}_{Q}B^{\text{sQ}} \right)\) ;

\(\phi(B)\) -- a stationary autoregressive (AR) polynomial in \(B\) and in
\(B^{S}\ \)containing regular and seasonal unit roots, with \emph{s}
representing the number of observations per year;

\(a_{t}\) -- a white-noise variable with the variance\(\ V(a)\).

It should be noted that the stochastic time series can be predicted
using its past observations and making an error. The variable \(a_{t}\),
which is assumed to be white noise, is the fundamental \emph{innovation} to
the series at time \emph{t}, that is the part that cannot be predicted based
on the past history of the series.

Denoting
\(\varphi\left( B \right) = \phi\left( B \right)\delta\left( B \right),\ \)
\[1\] can be written in a more concise form as

\[\varphi\left( B \right)x_{t} = \theta(B)a_{t}\], \[2\]

where \(\varphi\left( B \right)\) contains both the stationary and the
nonstationary roots.

\hypertarget{derivation-of-the-models-for-the-components}{%
\subsubsection{Derivation of the models for the components}\label{derivation-of-the-models-for-the-components}}

Let us consider the additive decomposition model

\[x_{t} = \sum_{i = 1}^{k}x_{\text{it}}\], \[3\]

where \emph{i} refers to the orthogonal components: trend, seasonal,
transitory or irregular. Apart from the irregular component, supposed to
be a white noise, it is assumed that each component follows the ARIMA
model which can be represented, using the notation of \[2\] , as:

\[\varphi_{i}\left( B \right)\ x_{\text{it}} = \theta_{i}(B)a_{\text{it}}\], \[4\]

where
\(\varphi_{i}\left( B \right) = \phi_{i}\left( B \right)\delta_{i}\left( B \right),\ \ x_{\text{it}}\)
is the \emph{i}-\emph{th} unobserved component, \(\varphi_{i}\left( B \right)\) and
\(\theta_{i}\left( B \right)\) are finite polynomials of order \(p_{i}\) and
\(q_{i}\), respectively, and \(a_{\text{it}},\) the disturbance associated
with such component, is a white noise process with zero mean and
constant variance \(V(a_{i})\) and \(a_{\text{it}}\) and
\(a_{\text{jt}}\ \)are not correlated for \(i \neq j\ \)and for any \(t\)..
These disturbances are functions of the innovations in the series and
are called "pseudo-innovations" in the literature concerning the AMB
decomposition as they refer to the components that are never observed
\footnote{GÓMEZ, V., and MARAVALL, A. (2001a).}. In the JDemetra+ documentation the term "innovations" is used to
refer to the "pseudo-innovations".

The following assumptions hold for \[4\] . For each \(\text{i}\) the
polynomials \(\phi_{i}\left( B \right)\), \(\delta_{i}\left( B \right)\) and
\(\theta_{i}(B)\) are prime and of finite order. The roots of
\(\delta_{i}\left( B \right)\) lies on the unit circle; those of
\(\phi_{i}\left( B \right)\) lie outside, while all the roots of
\(\theta_{i}\left( B \right)\ \)are on or outside the unit circle. This
means that nonstationary and noninvertible components are allowed. Since
different roots of the AR polynomial induce peaks in the spectrum\footnote{For description of the spectrum see section \href{../theory/spectral.html}{Spectral Analysis}.}
of the series at different frequencies, and given that different
components are associated with the spectral peaks for different
frequencies, it is assumed that for \(i \neq j\) the
polynomials\(\ \phi_{i}\left( B \right)\) and \(\phi_{j}\left( B \right)\)
do not share any common root (they are coprime). Finally, it is assumed
that the polynomials \(\theta_{i}\left( B \right),\ i = 1,\ldots,k\) are
prime share no unit root in common, guaranteeing the invertibility of
the overall series. In fact, since the unit root of
\(\theta_{i}\left( B \right)\) induce a spectral zero, when the
polynomials \(\theta_{i}\left( B \right),\ i = 1,\ldots,k\) share no unit
root in common, there is no frequency for which all component spectra
become zero\footnote{MARAVALL, A. (1995).}.

Since aggregation of ARIMA models yields ARIMA models, the series
\(x_{t}\ \)will also follow an ARIMA model, as in \[2\] , and
consequently the following identity can be derived:

\[\frac{\theta(B)}{\varphi(B)}a_{t} = \sum_{i = 1}^{k}{\frac{\theta_{i}(B)}{\varphi_{i}(B)}a_{\text{it}}}\]. \[5\]

In the ARIMA model based approach implemented in SEATS, the ARIMA model
identified and estimated for the observed series \(x_{t}\) is
decomposed to derive the models for the components. In particular, the
AR polynomials for the components, \(\varphi_{i}\left( B \right),\) are
easily derived through the factorization of the AR polynomial
\(\varphi\left( B \right)\):

\[\varphi\left( B \right) = \prod_{i = 1}^{k}{\varphi_{i}\left( B \right)}\], \[6\]

while the MA polynomials for the components, together with the
innovation variances \(V(a_{i})\), cannot simply be obtained through the
relationship:

\[\theta(B)a_{t} = \sum_{i = 1}^{k}{\varphi_{\text{ni}}\left( B \right)}\theta_{i}(B)a_{\text{it}}\], \[7\]

where \(\varphi_{\text{ni}}\left( B \right)\) is the product of all
\(\varphi_{j}\left( B \right),\ j = 1,\ldots,k\), except from
\(\varphi_{i}\left( B \right)\). Further assumptions are therefore needed
to cope with the underidentification problem: i) \(p_{i} \geq q_{i}\) and
ii) the canonical decomposition, i.e.~the decomposition that allocate
all additive white noise to the irregular component (yielding
noninvertible components except the irregular).

To understand how SEATS factorizes the AR polynomials, first a concept
of a root will be explored\footnote{Description based on KAISER, R., and MARAVALL, A. (2000) and
  MARAVALL, A. (2008c).}.

The equation \[2\] can be expressed as:

\[\psi^{- 1}(B)x_{t} = a_{t}(1 + \varphi_{1}B + \ldots\varphi_{p}B^{p})x_{t} =(1 + \theta_{1}B + \ldots\theta_{q}B^{q})a_{t}\], \[8\]

Let us now consider \[2\] in the inverted form:

\[\theta\left( B \right)y_{t} = \varphi(B)a_{t}\], \[9\]

If both sides of \[8\] are multiplied by \(x_{t - k}\) with \(k > q\),
and expectations are taken, the right hand side of the equation vanishes
and the left hand side becomes:

\[\varphi(B)\gamma_{k} = \gamma_{k} + \varphi_{1}\gamma_{k - 1} + \ldots\varphi_{p}\gamma_{k - p} = 0 \], \[10\]

where \(B\) operates on the subindex \(k\).

The autocorrelation function \(\gamma_{k}\) is a solution of \[10\] with
the characteristic equation:

\[z^{p} + \varphi_{1}z^{p - 1} + \ldots\varphi_{p - 1}z + \varphi_{p} = 0\]. \[11\]

If \(z_{1}\),\ldots,\(\ z_{p}\) are the roots of \[11\] , the solutions of
\[10\] can be expressed as:

\(\gamma_{k} = \sum_{i = 1}^{p}z_{i}^{k}\), \[12\]

and will converge to zero as \(k \rightarrow \infty\) when
\(\left| r_{i} \right| < 1,\ i = 1,\ldots,p\). From \[10\] and \[12\]
it can be noticed that \(z_{1} = B_{i}^{- 1}\), meaning that
\(z_{1}\),\ldots,\(\ z_{p}\) are the inverses of the roots \(B_{1},\ldots,B_{p}\)
of the polynomial \(\varphi(B)\). The convergence of \(\gamma_{k}\) implies
that the roots of the \(\varphi(B)\) are larger than 1 in modulus (lie
outside the unit circle). Therefore, from the equation

\[
  {\varphi(B)}^{- 1} = \frac{1}{(1 - z_{1})\ldots(1 - z_{1})}
  \] \[13\]

it can be derived that \({\varphi(B)}^{- 1}\) is convergent and all its
inverse roots are less than 1 in modulus.

Equation \[11\] has real and complex roots (solutions). Complex number
\(x = a + bi\), with \(a\) and \(\text{b}\) both real numbers, can be
represented as \(x = r\left( cos(\omega) + i\ sin(\omega \right))\), where
\(i\) is the imaginary unit\({\ (i}^{2} = - 1)\), \(r\) is the modulus of \(x\),
that is \(\ r = \left| x \right| = \sqrt{a^{2} + b^{2}}\) and \(\omega\) is
the argument (frequency). When roots are complex, they are always in
pairs of complex conjugates. The representation of the complex number
\(x = a + bi\) has a geometric interpretation in the complex plane
established by the real axis and the orthogonal imaginary axis.

\begin{figure}
\centering
\includegraphics{All_images/UG_A_image3.png}
\caption{Text}
\end{figure}

\textbf{Geometric representation of a complex number and of its conjugate}

Representing the roots of the characteristic equation \[11\] in the
complex plane enhances understanding how they are allocated to the
components. When the modulus \(r\) of the roots in \(\text{z}\) are greater
than 1 (i.e.~modulus of the roots in \(\varphi(B)\  < 1\)), the solution
of the characteristic equation has a systematic explosive process, which
means that the impact of the given impulse on the time series is more
and more pronounced in time. This behaviour is not in line with the
developments that can be identified in actual economic series.
Therefore, the models estimated by TRAMO-SEATS (and X-13ARIMA-SEATS)
have never inverse roots in \(B\) with modulus greater than 1.

The characteristic equations associated with the regular and the
seasonal differences have roots in \(\varphi(B)\) with modulus \(r = 1\).
They are called non-stationary roots and can be represented on the unit
circle. Let us consider the seasonal differencing operator applied to a
quarterly time series \((1 - B^{4})\). Its characteristic equation is
\({(z}^{4} - 1) = 0\) with solutions given by\(\ z = \sqrt[4]{1}\), i.e.
\(z_{1,2} = \pm 1\) and \(z_{3,4} = \pm i1\). The first two solutions are
real and the last two are complex conjugates. They are represented by
the black points on the unit circle on the figure below.

\begin{figure}
\centering
\includegraphics{All_images/UG_A_image4.png}
\caption{Text}
\end{figure}

\textbf{Unit roots on the unit circle}

For the seasonal differencing operator \((1 - B^{12})\) applied to the
monthly time series the characteristic equation \({\ (z}^{12} - 1) = 0\)
has twelve non-stationary solutions given by\(\ z = \sqrt[12]{1}:\) two
real and ten complex conjugates, represented by the white circles in
unit roots figure above.

The complex conjugates roots generate the periodic movements of the
type:

\[z_{t} = A^{t}\cos\left( \omega t + W \right).\] \[14\]

where:

\(A\) -- amplitude;

\(\omega\) -- angular frequency (in radians);

\(W\) -- phase (angle at \(t = 0)\).

The frequency \(f\), i.e.~the number of cycles per unit time, is
\(\frac{\omega}{2\pi}\). If it is multiplied by \emph{s}, the number of
observations per year, the number of cycles completed in one year is
derived. The period of function \[14\] , denoted by \(\tau\), is the
number of units of time (months/quarters) it takes for a full circle to
be completed.

For quarterly series the seasonal movements are produced by complex
conjugates roots with angular frequencies at \(\frac{\pi}{2}\) (one cycle
per year) and \(\pi\) (two cycles per year). The corresponding number of
cycles per year and the length of the movements are presented in the table below.

\textbf{Seasonal frequencies for a quarterly time series}

\{: .table .table-style\}
\textbar{} \textbf{Angular frequency (\(\omega\))} \textbar{} \textbf{Frequency (cycles per unit time) (\(f\))} \textbar{} \textbf{Cycles per year} \textbar{} \textbf{Length of the movement measured in quarters (\(\tau\))} \textbar{}
\textbar-----------------\textbar-----------------\textbar-----------------\textbar-----------------\textbar{}
\textbar{} \(\frac{\pi}{2}\) \textbar{} 0.25 \textbar{} 1 \textbar{} 4 \textbar{}
\textbar{} \(\pi\) \textbar{} 0.5 \textbar{} 2 \textbar{} 2 \textbar{}

For monthly time series the seasonal movements are produced by complex
conjugates roots at the angular frequencies:
\(\ \frac{\pi}{6},\frac{\pi}{3},\ \frac{\pi}{2},\ \frac{2\pi}{3},\ \frac{5\pi}{6}\ \)and
\(\pi\). The corresponding number of cycles per year and the length of the
movements are presented in the table below: Seasonal frequencies for a monthly
time series.

\textbf{Seasonal frequencies for a monthly time series}

\{: .table .table-style\}
\textbar{} \textbf{Angular frequency (\(\omega\))} \textbar{} \textbf{Frequency (cycles per unit time) (\(f\))} \textbar{} \textbf{Cycles per year} \textbar{} \textbf{Length of the movement measured in months (\(\tau\))} \textbar{}
\textbar-----------------\textbar-----------------\textbar-----------------\textbar-----------------\textbar{}
\textbar{} \(\frac{\pi}{6}\) \textbar{} 0.083 \textbar{} 1 \textbar{} 12 \textbar{}
\textbar{} \(\frac{\pi}{3}\) \textbar{} 0.167 \textbar{} 2 \textbar{} 6 \textbar{}
\textbar{} \(\frac{\pi}{2}\) \textbar{} 0.250 \textbar{} 3 \textbar{} 4 \textbar{}
\textbar{} \(\frac{2\pi}{3}\) \textbar{} 0.333 \textbar{} 4 \textbar{} 3 \textbar{}
\textbar{} \(\frac{5\pi}{6}\) \textbar{} 0.417 \textbar{} 5 \textbar{} 2.4 \textbar{}
\textbar{} \[\pi\] \textbar{} 0.500 \textbar{} 6 \textbar{} 2 \textbar{}

In JDemetra+ SEATS assigns the roots of the AR full polynomial to the
components according to their associated modulus and frequency,
i.e.:\footnote{For details see MARAVALL, A., CAPORELLO, G., PÉREZ, D., and
  LÓPEZ, R. (2014).}

\begin{itemize}
\item
  Roots of \(\left( 1 - B \right)^{d}\) are assigned to trend component.
\item
  Roots of
  \(\ \left( 1 - B^{s} \right)^{d_{s}} = {((1 - B)(1 + B + \ldots + B^{s - 1}))}^{d_{s}}\ \)are
  assigned to the trend component (root
  of\({\ \left( 1 - B \right)}^{d_{s}}\)) and to the seasonal component
  (roots of\({\ (1 + B + \ldots + B^{s - 1})}^{d_{s}}\)).
\item
  When the modulus of the inverse of a real positive root of
  \(\varphi(B)\) is greater than \(k\) or equal to \(k\), where \(k\) is the
  threshold value controlled by the \emph{Trend boundary} parameter(in the
  original SEATS it is controlled by \emph{rmod})\footnote{In JDemetra+ this argument is called \emph{Trend boundary}.}, then the root is
  assigned to the trend component. Otherwise it is assigned to the
  transitory component.
\item
  Real negative inverse roots of
  \(\text{ ϕ}_{p}\left( B \right)\ \)associated with the seasonal
  two-period cycle are assigned to the seasonal component if their
  modulus is greater than \emph{k}, where \(k\) is the threshold value
  controlled by the \emph{Seasonal boundary} and the \emph{Seas. boundary
  (unique)} parameters. Otherwise they are assigned to the transitory
  component.
\item
  Complex roots, for which the argument (angular frequency) is close
  enough to the seasonal frequency are assigned to the seasonal
  component. Closeness is controlled by the \emph{Seasonal tolerance} and
  \emph{Seasonal tolerance (unique)} parameters (in the original SEATS it
  is controlled by \emph{epsphi}). Otherwise they are assigned to the
  transitory component.
\item
  If \(d_{s}\ \)(seasonal differencing order) is present\(\ \)and
  \(\text{Bphi} < 0\) (\(\text{Bphi}\) is the estimate of the seasonal
  autoregressive parameter), the real positive inverse root is
  assigned to the trend component and the other (\(s - 1\)) inverse
  roots are assigned to the seasonal component. When \(d_{s} = 0\), the
  root is assigned to the seasonal when \(\text{Bphi} < - 0.2\) and/or
  the overall test for seasonality indicates presence of seasonality.
  Otherwise it goes to the transitory component. Also, when
  \(\text{Bphi} > 0\), roots are assigned to the transitory component.
\end{itemize}

For further details about JDemetra+ parameters see section \href{../reference-manual/sa-spec-tramo.html}{TramoSeats}.

It should be highlighted that when\(\ Q > P\), where \(Q\) and \(P\) denote
the orders of the polynomials \(\varphi\left( B \right)\) and \(\theta(B)\),
the SEATS decomposition yields a pure MA \((Q - P)\) component (hence
transitory). In this case the transitory component will appear even when
there is no AR factor allocated to it.

Once these rules are applied, the factorization of the AR polynomial
presented by \[2\] yields to the identification of the AR polynomials
for the components which contain, respectively, the AR roots associated
with the trend component, the seasonal component and the transitory
component.\footnote{The AR roots close to or at the trading day frequency generates a
  stochastic trading day component. A stochastic trading day component
  is always modelled as a stationary ARMA(2,2), where the AR part
  contains the roots close to the TD frequency, and the MA(2) is
  obtained from the model decomposition (MARAVALL, A., and PÉREZ, D.
  (2011)). This component, estimated by SEATS, is not implemented by
  the current version of JDemetra+.}

Then with the partial fraction expansion the spectrum of the final
components are obtained.

For example, the Airline model for a monthly time series:

\[(1 - B)(1 - B^{12})x_{t} = (1 + \theta_{1}B)(1 + \Theta_{1}B^{12})\ a_{t}\], \[15\]

is decomposed by SEATS into the model for the trend component:

\[(1 - B)(1 - B)c_{t} = (1 + \theta_{c,1}B + \theta_{c,2}B^{2})a_{c,t}\], \[16\]

and the model for the seasonal component:

\[\left( 1 + B + \ldots + B^{11} \right)s_{t} = \left( 1 + \theta_{s,1}B + \ldots + {\theta_{s,11}B}^{11} \right)a_{s,t},\] \[17\]

As a result, the Airline model is decomposed as follows:

\[\frac{(1 + \theta_{1}B)(1 + \Theta_{1}B^{12})}{(1 - B)(1 - B)}a_{t} = \frac{\left( 1 + \theta_{s,1}B + \ldots + {\theta_{s,11}B}^{11} \right)}{\left( 1 + B + \ldots + B^{11} \right)}a_{s,t} + \frac{(1 + \theta_{c,1}B + \theta_{c,2}B^{2})}{(1 - B)(1 - B)}a_{c,t} + u_{t}\]. \[18\]

The transitory component is not present in this case and the irregular
component is the white noise.

The partial fractions decomposition is performed in a frequency domain.
In essence, it consists in portioning of the pseudo-spectrum\footnote{The term pseudo-spectrum is used for a non-stationary time
  series, while the term spectrum is used for a stationary time
  series.}
of \(x_{t}\) into additive spectra of the components. When the AMB
decomposition of the ARIMA model results in the non-negative spectra for
all components, the decomposition is called admissible\footnote{If the ARIMA model estimated in TRAMO does not accept an
  admissible decomposition, SEATS replaces it with a decomposable
  approximation. The modified model is therefore used to decompose the
  series. There are also other rare situations when the ARIMA model
  chosen by TRAMO is changed by SEATS. It happens when, for example,
  the ARIMA models generate unstable seasonality or produce a
  senseless decomposition. Such examples are discussed by MARAVALL, A.
  (2009).}. In such
case an infinite number of admissible decompositions exists, i.e.
decompositions that yield the non-negative spectra of all components.
Therefore, the MA polynomials and the innovation variances cannot be yet
identified from the model of \(x_{t}\). As sketched above, to
solve this underidentification problem and identify a unique
decomposition, it is assumed that for each component the order of the MA
polynomial is no greater than the order of the AR polynomial and the
canonical solution of S.C. Hillmer and G.C. Tiao is applied\footnote{HILLMER, S.C., and TIAO, G.C. (1982).}, i.e.
all additive white noise is added to the irregular component As a
consequence all components derived from the canonical decomposition,
except from the irregular, have a spectral minimum of zero and are thus
noninvertible\footnote{GÓMEZ, V., and MARAVALL, A. (2001a).}. Given the stochastic features of the series, it can
be shown by that the canonical decomposition produces as stable as
possible trend and seasonal components since it maximizes the variance
of the irregular and minimizes the variance of the other
components\footnote{HILLMER, S.C., and TIAO, G.C. (1982).}. However, there is a price to be paid as canonical
components can produce larger revisions in the preliminary estimators of
the component\footnote{MARAVALL, A. (1986).} than any other admissible decomposition.

The figure below represents the pseudo-spectrum for the canonical trend and an
admissible trend.

\begin{figure}
\centering
\includegraphics{All_images/UG_A_image5.png}
\caption{Text}
\end{figure}

\textbf{A comparison of canonical trend and admissible trend}

A pseudo-spectrum is denoted by\(\ g_{i}(\omega)\), where \(\omega\)
represents the angular frequency. The pseudo-spectrum of \(x_{\text{it}}\)
is defined as the Fourier transform of ACGF of\(\ x_{t}\) which is
expressed as:

\[\frac{\psi_{i}\left( B \right)\psi_{i}\left( F \right)}{\delta_{i}\left( B \right)\delta_{i}\left( F \right)}V(a_{i})\], \[19\]

where:

\(\psi_{i}\left( F \right) = \frac{\theta_{i}\left( F \right)}{\phi_{i}\left( F \right)}\)

\(\psi_{i}\left( B \right) = \frac{\theta_{i}\left( B \right)}{\phi_{i}\left( B \right)}\)

\(B\) is the backward operator,

\(F\) is the forward operator.

A pseudo-spectrum for a monthly time series \(x_{t}\ \)is presented in
the figure below: The pseudo-spectrum for a monthly series. The frequency
\(\omega = 0\) is associated with the trend, frequencies in the range
\[$0 + \epsilon_{1},\ \frac{\pi}{6} - \epsilon_{2}$\] with
\(\left[0 + \epsilon_{1},\ \frac{\pi}{6} - \epsilon_{2}\right]\)
\(\epsilon_{1},\ \epsilon_{2} > 0\) and
\(\epsilon_{1} < \ \frac{\pi}{6} - \epsilon_{2}\ \) are usually associated
with the business-cycle and correspond to a period longer than a year
and bounded\footnote{Ibid.}. The frequencies in the range \[$\frac{\pi}{6},\ \pi$\]
are associated with the short term movements, whose cycle is completed
in less than a year. If a series contains an important periodic
component, its spectrum reveals a peak around the corresponding
frequency and in the ARIMA model it is captured by an AR root. In the
example below spectral peaks occur at the frequency \(\omega = 0\) and at
the seasonal frequencies ( \(\frac{\pi}{6}\),
\(\frac{2\pi}{6},\ \frac{3\pi}{6},\ \frac{4\pi}{6},\frac{5\pi}{6},\pi\)).
\footnote{KAISER, R., and MARAVALL, A. (2000).}

\begin{figure}
\centering
\includegraphics{All_images/UG_A_image6.png}
\caption{Text}
\end{figure}

\textbf{The pseudo-spectrum for a monthly series}

In the decomposition procedure, the pseudo-spectrum of the time series
\(x_{t}\) is divided into the spectra of its components (in the
example figure below, four components were obtained).

\begin{figure}
\centering
\includegraphics{All_images/UG_A_image7.png}
\caption{Text}
\end{figure}

\textbf{The pseudo-spectra for the components}

\hypertarget{estimation-of-the-components-with-the-wiener-kolmogorow-filter}{%
\subsubsection{Estimation of the components with the Wiener-Kolmogorow filter}\label{estimation-of-the-components-with-the-wiener-kolmogorow-filter}}

The various components are estimated using Wiener-Kolmogorow (WK)
filters. JDemetra+ includes three options to estimate the WK filter,
namely \emph{Burman}, \emph{KalmanSmoother} and \emph{MCElroyMatrix}\footnote{The choice of the estimation method is controlled by the \emph{Method}
  parameter, explained in the \href{../reference-manual/sa-spec-tramo.html}{SEATS specification} section.}. Here the
first of abovementioned options, proposed by BURMAN, J.P. (1980) will be
explained.

The estimation procedure and the properties of the WK filter are easier
to explain with a two-component model. Let the seasonally adjusted
series (\(s_{t}\)) be the signal of interest and the seasonal component
(\(n_{t}\)) be the remainder, "the noise". The series is given by the
model \[2\] and from \[4\] the models for theoretical components
are:

\[\varphi_{s}(B)s_{t} = \theta_{s}(B)a_{\text{st}}\] \[20\]

and

\[\varphi_{n}(B)n_{t} = \theta_{n}(B)a_{\text{nt}}\]. \[21\]

From \[6\] and \[7\] it is clear that
\(\varphi\left( B \right) = \varphi_{s}(B)\varphi_{n}(B)\) and
\(\theta\left( B \right)a_{t} = \theta_{s}(B)a_{\text{st}}+\theta_{n}(B)a_{\text{nt}}\).

As the time series components are never observed, their estimators have
to be used. Let us note \(X_{T}\) an infinite realization of the
time series \(x_{t}\). SEATS computes the Minimum Mean Square Error (MMSE)
estimator of \(s_{t}\), e.g.~the estimator \[\widehat{s}_{t}\] that
minimizes \[E\lbrack\left({s_{t}-{\widehat{s}}_{t})}^{2}|X_{T} \right)\rbrack\].
Under the normality assumption \[{\widehat{s}}_{t|T}\] is also equal to
the conditional expectation \[E\left(s_{t}|X_{T}\right)\], so it
can be presented as a linear function of the elements
in \[X_{T}\].\footnote{MARAVALL, A. (2008c).} WHITTLE (1963) shows that the MMSE estimator of
\[{\widehat{s}}_{t}\] is:

\[{\widehat{s}}_{t} = k_{s}\frac{\psi_{s}(B)\psi_{s}(F)}{\psi(B)\psi(F)}x_{t}\], \[22\]

where \[\psi(B)= \frac{\theta(B)}{\phi(B)}\],

\[F = B^{- 1}\] and \[k_{s}=\frac{V(a_{s})}{V(a)}\],

\[V(a_{s})\] is the variance of \[a_{st}\] and \[V(a)\] is
the variance of \[a_{t}\].

Expressing the \[\psi\left(B\right)\] polynomials as functions of the AR
and MA polynomials, after cancelation of roots, the estimator
of \[s_{t}\] can be expressed as:

\[{\widehat{s}}_{t} = k_{s}\frac{\theta_{s}\left(B\right)\theta_{s}\left(F\right)\varphi_{n}\left(B \right)\delta_{n}\left(B\right)\varphi_{n}\left(F\right)\delta_{n}\left(F\right)}{\theta\left(B\right)\theta\left(F \right)}x_{t}\], \[23\]

where:

\[\nu_{s}\left( B,F \right) = k_{s}\frac{\theta_{s}\left( B \right)\theta_{s}\left( F \right)\varphi_{n}\left( B \right)\delta_{n}\left( B \right)\varphi_{n}\left( F \right)\delta_{n}\left( F \right)}{\theta\left( B \right)\theta\left( F \right)}\] \[24\]

is a WK filter.

Equation \[24\] shows that the WK filter is two-sided (uses
observations both from the past and from the future), centered (the
number of points in the past is the same as in the future) and symmetric
(for any \(k\) the weight applied to \(x_{t - k}\) and \(x_{t + k}\) is the
same), which allows the phase effect to be avoided. Due to invertibility
of \(\theta\left( B \right)\) (and \(\theta\left( F \right)\)) the filter is
convergent in the past and in the future.

The estimator can be presented as

\[{\widehat{s}}_{t} = \nu_{i}\left(B,F\right)x_{t}\], \[25\]

where

\[\nu_{i}\left(B,F\right)=\nu_{0}+ \sum_{j = 1}^{\infty}\nu_{ij}(B^{j}+F^{j})\]

is the WK filter.

The example of the WK filters obtained for the pseudo-spectra of the
series illustrated above is shown on the figure below: WK filters for components.

\begin{figure}
\centering
\includegraphics{All_images/UG_A_image8.png}
\caption{Text}
\end{figure}

\textbf{WK filters for components}

The WK filter from \[24\] can also be expressed as a ratio of two
pseudo-autocovariance generating functions (p-ACGF). The p-ACGF function
summarizes the sequence of absolutely summable autocovariances of a
stationary process \(x_{t}\) (see section section \href{../theory/spectral.html}{Spectral Analysis}).

The ACGF function of an ARIMA process is expressed as:

\[acgf(B) = \frac{\theta\left( B \right)\theta\left( F \right)}{\phi\left( B \right)\delta\left( B \right)\phi\left( F \right)\delta\left( F \right)}V(a)\] \[26\]

And, the WK filter can be rewritten as:

\[\nu_{s}\left( B,F \right) = \frac{\gamma_{s}(B,F)}{\gamma(B,F)}\], \[27\]

where:

\[\gamma_{s}\left( B,F \right) = \frac{\theta_{s}\left( B \right)\theta_{s}\left( F \right)}{\phi_{s}\left( B \right)\delta_{s}\left( B \right)\phi_{s}\left( F \right)\delta_{s}\left( F \right)}V(a_{s})\]
is the p-ACGF of \[s_{t}\];

\(\gamma\left( B,F \right) = \frac{\theta\left( B \right)\theta\left( F \right)}{\phi\left( B \right)\delta\left( B \right)\phi\left( F \right)\delta\left( F \right)}V(a)\)
is the p-ACGF of \(x_{t}\).

From \[24\] it can be seen that the WK filter depends on both the
component and the series models. Consequently, the estimator of the
component and the WK filter reflect the characteristic of data and by
construction, the WK filter adapts itself to the series under
consideration. Therefore, the ARIMA model is of particular importance
for the SEATS method. Its misspecification results in an incorrect
decomposition.

This adaptability, if the model has been correctly determined, avoids
the dangers of under and overestimation with an ad-hoc filtering. For
example, for the series with a highly stochastic seasonal component the
filter adapts to the width of the seasonal peaks and the seasonally
adjusted series does not display any spurious seasonality\footnote{MARAVALL, A. (1995).}. Examples
of WK filters for stochastic and stable seasonal components are
presented on the figure below.

\begin{figure}
\centering
\includegraphics{All_images/UG_A_image9.png}
\caption{Text}
\end{figure}

\textbf{WK filters for stable and stochastic seasonal components}

The derivation of the components requires an infinite realization of
\(x_{t}\) in the direction of the past and of the future. However, the
convergence of the WK filter guarantees that, in practice, it could be
approximated by a truncated (finite) filter and, in most applications,
for large \[k\] the estimator for the central periods of the
series can be safely seen as generated by the WK filter\footnote{MARAVALL, A., and PLANAS, C. (1999).}:

\[{\widehat{s}}_{t}=\nu_{k}x_{t-k} + \ldots + \nu_{0}x_{t} + \ldots + \nu_{k}x_{t+k}\]. \[28\]

When \(T > 2L + 1\), where \(T\) is the last observed period, and \(L\) is an
a priori number that typically expands between 3 and 5 years, the
estimator expressed by \[23\] can be assumed as the final (historical)
estimator for the central observations of the series\footnote{MARAVALL, A. (1998).}. In practice,
the Wiener-Kolmogorov filter is applied to \(x_{t}\) extended with
forecasts and backcasts from the ARIMA model. The final or historical
estimator of \[{\widehat{s}}_{t}\], is obtained with a doubly infinite
filter, and therefore contains an error \[e_{st}\] called final
estimation error, which is equal \[e_{st}=s_{t}-{\widehat{s}}_{t}\].

In the frequency domain, the Wiener-Kolmogorov filter\(\ \nu(B,F)\) that
provides the final estimator of \(s_{t}\ \)is expressed as the ratio of
the \(s_{t}\ \)and \(x_{t}\) pseudo-spectra:

\[\widetilde{\nu}\left( \omega \right) = \frac{g_{s}(\omega)}{g_{x}(\omega)}\]. \[29\]

The function \(\widetilde{\nu}\left( \omega \right)\ \)is also referred as
the gain of the filter.\footnote{GÓMEZ, V., and MARAVALL, A. (2001a).} GÓMEZ, V., and MARAVALL, A. (2001a) show
that when for some frequency the signal (the seasonally adjusted series)
dominates the noise (seasonal fluctuations) the gain
\(\widetilde{\nu}\left( \omega \right)\) approaches 1. On the contrary,
when for some frequency the noise dominates the gain
\(\widetilde{\nu}\left( \omega \right)\ \)approaches 0.

The spectrum of the estimator of the seasonal component is expressed as:

\[g_{\widehat{s}}\left( \omega \right) = \left\lbrack \frac{g_{s}(\omega)}{g_{x}(\omega)} \right\rbrack^{2}g_{x}(\omega)\], \[30\]

where\(\ \left\lbrack \widetilde{\nu}\left( \omega \right) \right\rbrack^{2} = \left\lbrack \frac{g_{s}(\omega)}{g_{x}(\omega)} \right\rbrack^{2} = \left\lbrack \frac{g_{s}(\omega)}{g_{s}(\omega) + g_{n}(\omega)} \right\rbrack^{2} = \left\lbrack \frac{1}{1 + \frac{1}{r(\omega)}} \right\rbrack^{2}\)
is the squared gain of the filter and
\(r\left( \omega \right) = \frac{g_{s}(\omega)}{g_{n}(\omega)}\)
represents the signal-to-noise ratio.

For each \(\omega\), the MMSE estimation gives the signal-to-noise ratio.
If this ratio is high, then the contribution of that frequency to the
estimation of the signal will be also high. Assume that the trend is a
signal that needs to be extracted from a seasonal time series. Then
\(R\left( 0 \right) = 1\ \)and the frequency \(\omega = 0\) will only be
used for trend estimations. For seasonal frequencies
\(R\left( \omega \right) = 0,\) so that these frequencies are ignored in
computing the trend resulting in spectral zeros in
\(g_{\widehat{s}}\left( \omega \right)\). For this reason, unlike the
spectrum of the component, the component spectrum contains dips as it
can be seen on the figure below: Component spectrum and estimator
spectrum for trend.

\begin{figure}
\centering
\includegraphics{All_images/UG_A_image10.png}
\caption{Text}
\end{figure}

\textbf{Component spectrum and estimator spectrum for trend}

From the equation \[29\] it is clear that the squared gain of the
filter determines how the variance of the series contributes to the
variance of the seasonal component for the different frequencies. When
\(\widetilde{\nu}\left( \omega \right) = 1\), the full variation of
\(x_{t}\) for that frequency is passed to \[{\widehat{s}}_{t}\], while if
\[\widetilde{\nu}\left(\omega\right) = 0 \] the variation of \(x_{t}\) for
that frequency is fully ignored in the computation of
\[{\widehat{s}}_{t}\]. These two cases are well illustrated by the figure below
that shows the square gain of the WK filter for two series already
analysed in the figure above (Figure: WK filters for stable and stochastic seasonal components).

\begin{figure}
\centering
\includegraphics{All_images/UG_A_image11.png}
\caption{Text}
\end{figure}

\textbf{The squared gain of the WK filter for stable and stochastic seasonal components.}

Since \(r\left( \omega \right) \geq 0\), then
\(\widetilde{\nu}\left( \omega \right) \leq 1\) and from \[29\] it can
be derived that
\(g_{\widehat{s}}\left( \omega \right) = \widetilde{\nu}\left( \omega \right)g_{s}(\omega)\).
As a result, the estimator will always underestimate the component, i.e.
it will be always more stable that the component.\footnote{Ibid.}

Since
\(g_{\widehat{n}}\left( \omega \right) < g_{n}\left( \omega \right)\)
and\(\ g_{\widehat{s}}\left( \omega \right) < g_{s}\left( \omega \right)\)
the expression:
\(g_{x}\left( \omega \right) - \left\lbrack g_{\widehat{n}}\left( \omega \right) + g_{\widehat{s}}\left( \omega \right) \right\rbrack \geq 0\)
is the cross-spectrum. As it is positive, the MMSE yields correlated
estimators. This effect emerges since variance of estimator is smaller
than the variance of component. Nevertheless, if at least one
non-stationary component exists, cross-correlations estimated by
TRAMO-SEATS will tend to zero as cross-covariances between estimators of
the components are finite. In practice, the inconvenience caused by this
property will likely be of little relevance.

\textbf{Preliminary estimators for the components}

GÓMEZ, V., and MARAVALL, A. (2001a) point out that \emph{the properties of
the estimators have been derived for the final (or historical)
estimators. For a finite (long enough) realization, they can be assumed
to characterize the estimators for the central observations of the
series, but for periods close to the beginning of the end the filter
cannot be completed and some preliminary estimator has to be used}.
Indeed, the historical estimator shown in \[28\] is obtained for the
central periods of the series. However, when \(t\) approaches \(T\) (last
observation), the WK filter requires observations, which are not
available yet. For this reason a preliminary estimator needs to be used.

To introduce preliminary estimators let us consider a semi-finite
realization \(\lbrack x_{- \infty}\),\ldots{}\(\ x_{T}\){]}, where \(T\) is the last
observed period. The preliminary estimator of \[x_{\text{it}}\] obtained
at \(T\) \[(T - t = k \geq 0)\] can be expressed as

\[
{\widehat{x}}_{it|t + k}=\nu_{i}\left(B,F\right)x_{t|T}^{e}
\], \[31\]

where \[\nu_{i}\left(B,F \right)\] is the WK filter and \[x_{t|T}^{e}\] is
the extended series, such that \(x_{t|T}^{e} = x_{t}\) for \(t \leq T\) and
\[x_{t|T}^{e}={\widehat{x}}_{t|T}\] for \[t>T\], where
\[{\widehat{x}}_{t|T}\] denotes the forecast of \(x_{t}\) obtained at period
\(T\).

The future \(k\) values necessary to apply the filter are not yet
available and are replaced by their optimal forecasts from the ARIMA
model on \[x_{t}\]. When \[k=0\] the preliminary estimator becomes
the concurrent estimator. As the forecasts are linear functions of
present and past observations of \[x_{t}\], the preliminary
estimator \[{\widehat{x}}_{it}\] will be a truncated asymmetric
filter applied to \[x_{t}\] that generates a phase effect\footnote{KAISER, R., and MARAVALL, A. (2000).}.

When a new observation \[x_{T + 1}\] becomes available the forecast
\[{\widehat{x}}_{T + 1|T}\] is replaced by the observation and the
forecast \[{\widehat{x}}_{iT + j|T}\], \[j > 1\] are updated to
\[x_{T + j|T + 1}\] resulting in the revision error\footnote{MARAVALL, A. (1995).}. The total error
in the preliminary estimator \[d_{it|t + k}\] is expressed as a sum of the
final estimation error (\[e_{it}\]) and the revision error
(\[r_{it|t + k}\]), i.e.:

\[
d_{it|t + k} = x_{it}-{\widehat{x}}_{it|t + k} = \left(x_{it} - {\widehat{x}}_{it}\right) + \left(          {\widehat{x}}_{it} - {\widehat{x}}_{it|t + k} \right) = e_{it} + r_{it|t + k}
\], \[32\]

where:

\[x_{it}-i^{th}\] component;

\[{\widehat{x}}_{it|t + k}\]- the estimator of \[x_{it}\] when the
last observation is \[x_{t + k}\].

Therefore the preliminary estimator is subject not only to the final
error but also to a revision error, which are orthogonal to each
other\footnote{MARAVALL, A. (2009).}. The revision error decreases as \[k\] increases, until
it can be assumed equal to 0 for large enough \[k\].

It's worth remembering that SEATS estimates the unobservable components
of the time series so the "true" components are never observed.
Therefore, MARAVALL, A. (2009) stresses that \emph{the error in the
historical estimator is more of academic rather than practical interest.
In practice, interest centres on revisions. (\ldots) the revision standard
deviation will be an indicator of how far we can expect to be from the
optimal estimator that will be eventually attained, and the speed of
convergence of} \({\theta\left( B \right)\ }^{- 1}\) \emph{will dictate the
speed of convergence of the preliminary estimator to the historical
one.} The analysis of an error is therefore useful for making decision
concerning the revision policy, including the policy for revisions and
horizon of revisions.

\hypertarget{psie-weights}{%
\subsubsection{PsiE-weights}\label{psie-weights}}

The estimator of the component is calculated as
\[{\widehat{x}}_{it} = \nu_{s}\left(B,F\right)x_{t}\]. By replacing
\[x_{it}=\frac{\theta(B)}{\gamma(B)\delta(B)}a_{t}\], the
component estimator can be expressed as\footnote{The section is based on KAISER, R., and MARAVALL, A. (2000).}:

\[
  {\widehat{x}}_{it} = \xi_{s}\left(B,F\right)a_{t}
  \], \[33\]

where
\(\xi_{s}\left( B,F \right) = \ldots + \xi_{j}B^{j} + \ldots + \xi_{1}B + \xi_{0} + \xi_{- 1}F\ldots\xi_{- j}F^{j} + \ldots\).

This representation shows the estimator as a filter applied to the
innovation \[a_{t}\], rather than on the
series \[x_{t}\]\footnote{See section PsiE-weights. For further details see MARAVALL, A. (2008).}. Hence, the filter from \[32\] can be
divided into two components: the first one, i.e.
\[\ldots + \xi_{j}B^{j}+ \ldots+ \xi_{1}B + \xi_{0}\], applies to
prior and concurrent innovations, the second one, i.e.
\[\xi_{- 1}F + \ldots + \xi_{- j}F^{j}\] applies to future (i.e.
posterior to \[t\]) innovations. Consequently, \[\xi_{j}\]
determines the contribution of \[a_{t - j}\] to \[{\widehat{s}}_{t}\] while
\[\xi_{- j}\] determines the contribution of \[a_{t + j}\] to
\[{\widehat{s}}_{t}\]. Finally, the estimator of the component can be
expressed as:

\[
  {\widehat{x}}_{it} =\xi_{i}(B)^{-}a_{t} + \xi_{i}(F)^{+}a_{t + 1}
  \], \[34\]

where:

\(\xi_{i}{(B)}^{-}a_{t}\) is an effect of starting conditions, present and
past innovations in series;

\(\xi_{i}{(F)}^{+}a_{t + 1}\) is an effect of future innovations.

For the two cases already presented in figure \emph{WK filters for stable and stochastic seasonal components} and figure \emph{The squared gain of the WK filter for stable and stochastic seasonal components} above, the psi-weights are shown in the figure below.

\begin{figure}
\centering
\includegraphics{All_images/UG_A_image12.png}
\caption{Text}
\end{figure}

It can be shown that \[{\xi}_{- 1},\ldots,\xi_{- j}\] are convergent and
\[\xi_{j},\ldots, {\xi}_{1},\xi_{0}\] are divergent. From \[33\] , the
concurrent estimator is equal to
\[
{\widehat{x}}_{it|t} = E_{t}x_{it}=E_{t}{\widehat{x}}_{it} = {\xi}_{i}(B)^{-}a_{t}
\], \[35\]

so that the revision
\[
r_{it} = {\widehat{x}}_{it} - {\widehat{x}}_{it|t} = \xi_{i}(F)^{+}a_{t + 1}
\] \[36\]

is a zero-mean stationary MA process. As a result, historical and
preliminary estimators are cointegrated. From expression \[25\] the
relative size of the full revision and the speed of convergence can be
obtained.

\hypertarget{high-frequency-data-seasonal-adjustment}{%
\chapter{High Frequency data (seasonal adjustment)}\label{high-frequency-data-seasonal-adjustment}}

\hypertarget{motivation-1}{%
\section{Motivation}\label{motivation-1}}

\hypertarget{unobserved-components-1}{%
\section{Unobserved Components}\label{unobserved-components-1}}

\hypertarget{seasonality-tests-1}{%
\section{Seasonality tests}\label{seasonality-tests-1}}

\hypertarget{calendar-correction-1}{%
\section{Calendar correction}\label{calendar-correction-1}}

\hypertarget{outliers-and-intervention-variables-1}{%
\section{Outliers and intervention variables}\label{outliers-and-intervention-variables-1}}

\hypertarget{pre-adjustment-1}{%
\section{Pre-adjustment}\label{pre-adjustment-1}}

\hypertarget{decomposition-1}{%
\section{Decomposition}\label{decomposition-1}}

\hypertarget{generating-calendar-regressors-and-other-input-variables}{%
\chapter{Generating Calendar regressors and other input variables}\label{generating-calendar-regressors-and-other-input-variables}}

\hypertarget{motivation-2}{%
\section{Motivation}\label{motivation-2}}

\hypertarget{underlying-theory}{%
\section{Underlying theory}\label{underlying-theory}}

\hypertarget{available-tools}{%
\section{Available Tools}\label{available-tools}}

\hypertarget{gui}{%
\subsection{GUI}\label{gui}}

\hypertarget{rjd3modelling-package}{%
\subsection{Rjd3modelling package}\label{rjd3modelling-package}}

\hypertarget{outlier-detection}{%
\chapter{Outlier detection}\label{outlier-detection}}

\hypertarget{motivation-3}{%
\section{Motivation}\label{motivation-3}}

\hypertarget{with-reg-arima-models}{%
\section{With Reg Arima models}\label{with-reg-arima-models}}

\hypertarget{part-of-preadjustment}{%
\subsection{Part of preadjustment}\label{part-of-preadjustment}}

\hypertarget{specific-terror-tool}{%
\subsection{specific TERROR tool}\label{specific-terror-tool}}

\hypertarget{with-structural-models}{%
\section{With structural models}\label{with-structural-models}}

\hypertarget{modelling-time-series}{%
\chapter{Modelling Time Series}\label{modelling-time-series}}

\hypertarget{motivation-4}{%
\section{Motivation}\label{motivation-4}}

\hypertarget{benchmarking-and-temporal-disagreggation}{%
\chapter{Benchmarking and temporal disagreggation}\label{benchmarking-and-temporal-disagreggation}}

\hypertarget{benchmarking-overview}{%
\section{Benchmarking overview}\label{benchmarking-overview}}

Often one has two (or multiple) datasets of different frequency for the same
target variable. Sometimes, however, these datasets are not coherent in the
sense that they don't match up. Benchmarking{[}\^{}1{]} is a method todeal with
this situation. An aggregate of a higher-frequency measurement variables is not necessarily
equal to the corresponding lower-frequency less-aggregated measurement.
Moreover, the sources of data may have different reliability levels. Usually,
less frequent data are considered more trustworthy as they are
based on larger samples and compiled more precisely. The
more reliable measurements, hence often the less frequent, will serve as benchmark.

In seasonal adjustment methods benchmarking is the procedure that
ensures the consistency over the year between adjusted and
non-seasonally adjusted data. It should be noted that the
\href{https://ec.europa.eu/eurostat/documents/3859598/6830795/KS-GQ-15-001-EN-N.pdf/d8f1e5f5-251b-4a69-93e3-079031b74bd3}{ESS Guidelines on Seasonal Adjustment (2015)},
do not recommend benchmarking as it introduces a bias in the seasonally adjusted data.
The U.S. Census Bureau also points out that ``\emph{forcing the seasonal
adjustment totals to be the same as the original series annual totals
can degrade the quality of the seasonal adjustment, especially when the
seasonal pattern is undergoing change. It is not natural if trading day
adjustment is performed because the aggregate trading day effect over a
year is variable and moderately different from zero}''{[}\^{}2{]}. Nevertheless,
some users may need that the annual totals of the seasonally adjusted
series match the annual totals of the original, non-seasonally adjusted
series{[}\^{}3{]}.

According to the
\href{https://ec.europa.eu/eurostat/documents/3859598/6830795/KS-GQ-15-001-EN-N.pdf/d8f1e5f5-251b-4a69-93e3-079031b74bd3}{ESS Guidelines on Seasonal Adjustment (2015)}, the
only benefit of this approach is that there is consistency over the year
between adjusted and the non-seasonally adjusted data; this can be of
particular interest when low-frequency (e.g.~annual) benchmarking
figures officially exist (e.g.~National Accounts, Balance of Payments,
External Trade, etc.) and where users' needs for time consistency are
stronger.

\hypertarget{underlying-theory-1}{%
\section{Underlying Theory}\label{underlying-theory-1}}

Benchmarking\footnote{Description of the idea of benchmarking is based on DAGUM, B.E.,
  and CHOLETTE, P.A. (1994) and QUENNEVILLE, B. et all (2003).
  Detailed information can be found in: DAGUM, B.E., and CHOLETTE,
  P.A. (2006).} is a procedure widely used when for the same target
variable the two or more sources of data with different frequency are
available. Generally, the two sources of data rarely agree, as an
aggregate of higher-frequency measurements is not necessarily equal to
the less-aggregated measurement. Moreover, the sources of data may have
different reliability. Usually it is thought that less frequent data are
more trustworthy as they are based on larger samples and compiled more
precisely. The more reliable measurement is considered as a benchmark.

Benchmarking also occurs in the context of seasonal adjustment. Seasonal
adjustment causes discrepancies between the annual totals of the
seasonally unadjusted (raw) and the corresponding annual totals of the
seasonally adjusted series. Therefore, seasonally adjusted series are
benchmarked to the annual totals of the raw time series\footnote{DAGUM, B.E., and CHOLETTE, P.A. (2006).}. Therefore,
in such a case benchmarking means the procedure that ensures the
consistency over the year between adjusted and non-seasonally adjusted
data. It should be noted that the `\emph{ESS Guidelines on Seasonal
Adjustment}' (2015) do not recommend benchmarking as it introduces a
bias in the seasonally adjusted data. Also the U.S. Census Bureau points
out that: \emph{Forcing the seasonal adjustment totals to be the same as the
original series annual totals can degrade the quality of the seasonal
adjustment, especially when the seasonal pattern is undergoing change.
It is not natural if trading day adjustment is performed because the
aggregate trading day effect over a year is variable and moderately
different from zero}.\footnote{'\emph{X-12-ARIMA Reference Manual'} (2011).} Nevertheless, some users may prefer the
annual totals for the seasonally adjusted series to match the annual
totals for the original, non-seasonally adjusted series\footnote{HOOD, C.C.H. (2005).}. According
to the `\emph{ESS Guidelines on Seasonal Adjustment}' (2015), the only
benefit of this approach is that there is consistency over the year
between adjusted and non-seasonally adjusted data; this can be of
particular interest when low-frequency (e.g.~annual) benchmarking
figures officially exist (e.g.~National Accounts, Balance of Payments,
External Trade, etc.) where user needs for time consistency are
stronger.

The benchmarking procedure in JDemetra+ is available for a single
seasonally adjusted series and for an indirect seasonal adjustment of an
aggregated series. In the first case, univariate benchmarking ensures
consistency between the raw and seasonally adjusted series. In the
second case, the multivariate benchmarking aims for consistency between
the seasonally adjusted aggregate and its seasonally adjusted
components.

Given a set of initial time series \[\left\{ z_{i,t} \right\}_{i \in I}\],
the aim of the benchmarking procedure is to find the corresponding
\[\left\{ x_{i,t} \right\}_{i \in I}\] that respect temporal aggregation
constraints, represented by \[X_{i,T} = \sum_{t \in T}^{}x_{i,t}\] and
contemporaneous constraints given by
\[q_{k,t} = \sum_{j \in J_{k}}^{}{w_{\text{kj}}x_{j,t}}\] or, in matrix
form: \[q_{k,t} = w_{k}x_{t}\].

The underlying benchmarking method implemented in JDemetra+ is an
extension of Cholette's\footnote{CHOLETTE, P.A. (1979).} method, which generalises, amongst others,
the additive and the multiplicative Denton procedure as well as simple
proportional benchmarking.

The JDemetra+ solution uses the following routines that are described in
DURBIN, J., and KOOPMAN, S.J. (2001):

\begin{itemize}
\item
  The multivariate model is handled through its univariate
  transformation,
\item
  The smoothed states are computed by means of the disturbance
  smoother.
\end{itemize}

The performance of the resulting algorithm is highly dependent on the
number of variables involved in the model (\(\propto \ n^{3}\)). The other
components of the problem (number of constraints, frequency of the
series, and length of the series) are much less important
(\(\propto \ n\)).

From a theoretical point of view, it should be noted that this approach
may handle any set of linear restrictions (equalities), endogenous
(between variables) or exogenous (related to external values), provided
that they don't contain incompatible equations. The restrictions can
also be relaxed for any period by considering their "observation" as
missing. However, in practice, it appears that several kinds of
contemporaneous constraints yield unstable results. This is more
especially true for constraints that contain differences (which is the
case for non-binding constraints). The use of a special square root
initialiser improves in a significant way the stability of the
algorithm.

\hypertarget{tools}{%
\section{Tools}\label{tools}}

\hypertarget{references-6}{%
\section{References}\label{references-6}}

\hypertarget{trend-cycle-estimation-1}{%
\chapter{Trend-cycle estimation}\label{trend-cycle-estimation-1}}

\hypertarget{motivation-5}{%
\section{Motivation}\label{motivation-5}}

\hypertarget{underlying-theory-2}{%
\section{Underlying Theory}\label{underlying-theory-2}}

\hypertarget{tools-1}{%
\section{Tools}\label{tools-1}}

\hypertarget{nowcasting}{%
\chapter{Nowcasting}\label{nowcasting}}

\hypertarget{graphical-user-interface}{%
\chapter{Graphical User Interface}\label{graphical-user-interface}}

\hypertarget{structure-and-assets}{%
\section{Structure and assets}\label{structure-and-assets}}

\hypertarget{main-functions}{%
\section{Main functions}\label{main-functions}}

\hypertarget{seasonal-adjustment-2}{%
\subsection{Seasonal adjustment}\label{seasonal-adjustment-2}}

\hypertarget{benchmarking}{%
\subsection{Benchmarking}\label{benchmarking}}

\hypertarget{r-packages}{%
\chapter{R packages}\label{r-packages}}

\hypertarget{available-functions}{%
\section{Available functions}\label{available-functions}}

\hypertarget{seasonal-adjustment-3}{%
\subsection{Seasonal Adjustment}\label{seasonal-adjustment-3}}

\hypertarget{interaction-with-gui}{%
\section{Interaction with GUI}\label{interaction-with-gui}}

\hypertarget{plug-ins-for-jdemetra}{%
\chapter{Plug-ins for JDemetra+}\label{plug-ins-for-jdemetra}}

\hypertarget{main-functions-1}{%
\section{Main functions}\label{main-functions-1}}

\hypertarget{production}{%
\chapter{Production}\label{production}}

\hypertarget{tool-selection-issues}{%
\chapter{Tool selection issues}\label{tool-selection-issues}}

\hypertarget{spectral-analysis-principles-and-tools}{%
\chapter{Spectral Analysis Principles and Tools}\label{spectral-analysis-principles-and-tools}}

add : R code, rjd3toolkit references

\hypertarget{overview-2}{%
\section{overview}\label{overview-2}}

This scenario is designed for advanced users interested in an in-depth analysis of
time series in the frequency domain using three spectral graphs. Those
graphs can also be used as a complementary analysis for a better
understanding of the results obtained with some of the tests described
above.

Economic time series are usually presented in a time domain (X-axis).
However, for analytical purposes it is convenient to convert the
series to a frequency domain due to the fact that any stationary time
series can be expressed as a combination of cosine (or sine) functions.
These functions are characterized with different periods (amount of time
to complete a full cycle) and amplitudes (maximum/minimum value during
the cycle).

The tool used for the analysis of a time series in a frequency domain is
called a spectrum. The peaks in the spectrum indicate the presence of
cyclical movements with periodicity between two months and one year. A seasonal
series should have peaks at the seasonal
frequencies. Calendar adjusted data are not expected to have peak at with a
calendar frequency.

The periodicity of the phenomenon at frequency \emph{f} is \(\frac{2\pi}{f}\). It
means that for a monthly time series the seasonal frequencies
\(\frac{\pi}{6},\ \frac{\pi}{3},\ \frac{\pi}{2},\ \frac{2\pi}{3},\ \frac{5\pi}{6}\ \)
and \(\pi\) correspond to 1, 2, 3, 4, 5 and 6 cycles per year. For
example, the frequency \(\frac{\pi}{3}\) corresponds to a periodicity of 6
months (2 cycles per year are completed). For the quarterly series there
are two seasonal frequencies: \(\frac{\pi}{2}\) (one cycle per year) and
\(\pi\) (two cycles per year). A peak at the zero frequency always
corresponds to the trend component of the series. Seasonal frequencies
are marked as grey vertical lines, while violet vertical lines represent the
trading-days frequencies. The trading day frequency is 0.348 and derives
from the fact that a daily component which repeats every seven days goes
through 4.348 cycles in a month of average length 30.4375 days. It is
therefore seen to advance 0.348 cycles per month when the data are
obtained at twelve equally spaced times in 365.25 days (the average
length of a year).

The interpretation of the spectral graph is rather straightforward. When
the values of a spectral graph for low frequencies (i.e.~one year and
more) are large in relation to its other values it means that the
long-term movements dominate in the series. When the values of a
spectral graph for high frequencies (i.e.~below one year) are large in
relation to its other values it means that the series are rather
trendless and contains a lot of noise. When the values of a spectral
graph are distributed randomly around a constant without any visible
peaks, then it is highly probable that the series is a random process.
The presence of seasonality in a time series is manifested in a spectral
graph by the peaks on the seasonal frequencies.

\hypertarget{reg-arima-models}{%
\chapter{Reg-Arima models}\label{reg-arima-models}}

\hypertarget{overview-3}{%
\section{Overview}\label{overview-3}}

\hypertarget{autocorrelation-function}{%
\section{Autocorrelation function}\label{autocorrelation-function}}

The correlation is a measure of the strength and the direction of a
linear relationship between two variables. For time series the
correlation can refer to the relation between its observations, e.g.
between the current observation and the observation lagged by a given
number of units. In this case all observations come from one variable,
so similarity between a given time series and a \(k\)-lagged version of
itself over successive time intervals is called an autocorrelation.

The autocorrelation coefficient at lag \(k\) is defined as:

\[\rho\left( k \right) = \frac{\sum_{t = k + 1}^{n}\left( x_{t} - \overline{x} \right)}{\sum_{t = 1}^{n}\left( x_{t} - \overline{x} \right)^{2}}
\], \[1\]

where:

\(x_{t}\) -- time series;

\(n\) -- total number of observations;

\(\overline{x}\) -- mean of the time series.

The set of autocorrelation coefficients \((k)\) arranged as a function of
\(k\) is the autocorrelation function (ACF). The graphical or numerical
representation of the ACF is called an autocorrelogram.

\begin{figure}
\centering
\includegraphics{All_images/UG_A_image20.png}
\caption{Text}
\end{figure}

\textbf{Autocorrelation function}

The autocorrelation function is a valuable tool for investigating
properties of an empirical time series.\footnote{MAKRIDAKIS, S., WHEELWRIGHT, S.C., and HYNDMAN, R.J. (1998).} The assessment of the order
of an AR process simply from the sample ACF is not straightforward.
While for a first-order process the theoretical ACF decreases
exponentially and the sample function is expected to have the similar
shape, for the higher-order processes the ACF maybe a mixture of damper
exponential or sinusoidal functions, which makes the order of the AR
process difficult to identify.\footnote{CHATFIELD, C. (2004).} JDemetra+ displays the values of
autocorrelation function for the residuals from the ARIMA model (see
section \href{../reference-manual/residuals.html}{Residuals}). The ACF graph (figure above), presents autocorrelation
coefficients and the confidence intervals. If the autocorrelation
coefficient is in the confidence interval, it is regarded as not
statistically significant. Therefore, the user should focus on the
values where the value of the ACF is outside the confidence interval. In
JDemetra+ the confidence interval is indicated by two grey, horizontal,
dotted lines.

\textbf{Partial autocorrelation function}

The partial autocorrelation is a tool for the identification and
estimation of the ARIMA model. It is defined as the amount of
correlation between two variables that is not explained by their mutual
correlations with a given set of other variables.

Partial autocorrelation at lag \(k\) is defined as the autocorrelation
between \(x_{t}\) and \(x_{t - k}\) that is not accounted for by lags 1
through to \(k\)-1, which means that correlations with all the elements up
to lag \(k\) are removed. Following this definition, a partial
autocorrelation for lag 1 is equivalent to an autocorrelation.

The partial autocorrelation function (PACF) is the set of partial
autocorrelation coefficients \((k)\) arranged as a function
of \(k\). This function can be used to detect the presence of an
autoregressive process in time series and identify the order of this
process. Theoretically, the number of significant lags determines the
order of the autoregressive process.

\begin{figure}
\centering
\includegraphics{All_images/UG_A_image21.png}
\caption{Text}
\end{figure}

\textbf{Partial autocorrelation function}

The PACF graph above, which
is available from the \emph{Tools}\(\  \rightarrow \ \)\emph{Differencing} menu
presents partial autocorrelation coefficients and the confidence
intervals (two grey, horizontal, dotted lines). If the partial autocorrelation coefficient is in the
confidence interval, it is regarded as statistically insignificant.
Therefore, the user should focus on the values, for which the absolute value
of the PACF is outside the confidence interval.

\hypertarget{estimation-of-arma-models}{%
\section{Estimation Of Arma models}\label{estimation-of-arma-models}}

The computation of \href{../../stats/likelihood/ll.md}{exact likelihood} requires the evaluation of two main quantities: the determinant of the covariance matrix
and the sum of the squared residuals.
The different algorithms for the computation of the likelihood of ARMA models provides efficient solutions for those two problems.
The quantity
\[ y' \Omega^-1 y \]
is computed by defining a linear transformation of the observations such that
\[ y' \Omega^-1 y = \left(y' T' \right) \left(T y \right) \]
An obvious solution will be the use of the inverse of the Cholesky factor of the covariance matrix. However, any transformation
\[ T\sim m \times n \]
such that
\[ \Omega^-1 =  T' T \]
might be considered. Note that m can be larger than n (which means that the transformed observations will not be independent).

JD+ provides several routines for estimating the exact likelihood of ARMA models

\begin{longtable}[]{@{}ll@{}}
\toprule
Algorithm & Use \\
\midrule
\endhead
Kalman filter & Default \\
\href{./ansley.md}{Ansley} & Large regression models \\
\href{./x12.md}{X12} & Legacy \\
Ljung-Box & Deprecated \\
\bottomrule
\end{longtable}

\hypertarget{maximum-likelihood-estimation}{%
\section{Maximum likelihood estimation}\label{maximum-likelihood-estimation}}

\hypertarget{likelihood-of-a-multivariate-normal-distribution}{%
\subsection{Likelihood of a multivariate normal distribution}\label{likelihood-of-a-multivariate-normal-distribution}}

The pdf of a multivariate normal distribution is:

\[p\left( y \right) = \left( 2 \pi \right)^{-\frac{n}{2}} \vert \Sigma \vert ^{-\frac{1}{2}}e^{ {-\frac{1}{2}y' \Sigma ^{-1} y} } \]

If we set

\[ y' \Sigma ^{-1} y=u'u \:\: or \:\:  L^{-1}y = u \]

the log-likelihood is:

\[ l \left( \theta | y \right) =- \frac{1}{2} \left(n \log{2 \pi}+ \log{|\Sigma |} +u'u\right) \]

In most cases, we will use a covariance matrix with a (unknown) scaling factor:

\[ \Sigma = \sigma^2 \Omega \]

If we set

\[ L^{-1}y = e , \quad LL' = \Omega\]

the log-likelihood can then be written:

\[ l \left(\theta, \sigma | y \right ) = - \frac{1}{2} \left(n \log{2 \pi}+ n \log {\sigma^2} + \log{|\Omega |} + \frac{1}{\sigma ^2} e'e \right) \]

The scaling factor can be concentrated out of the likelihood. Its estimator is

\[ \hat{\sigma} ^2 = \frac{e' e}{n} \]

so that :

\[ l_c \left( \theta | y \right ) = - \frac{1}{2} \left(n \log{2 \pi} + n\log{\frac{e'e}{n}} + \log{|\Omega |} + n \right) \]

or

\[ l_c \left( \theta | y \right ) = - \frac{n}{2} \left(\log{2 \pi}+ 1 - \log {n} + \log{e'e} + \log{|\Omega |^\frac{1}{n}}\right) \]

Maximizing \$ l\_c \$ is equivalent to minimizing the deviance

\[ d \left( y | \theta\right ) = e'e |\Omega |^\frac{1}{n} = v'v, \quad where \quad v = \sqrt{ |\Omega |^\frac{1}{n} }\: e\]

This last formulation will be used in optimization procedures based on sums of squares (Levenberg-Marquardt and similar algorithms).

\hypertarget{linear-model-with-gaussian-noises}{%
\subsection{Linear model with gaussian noises}\label{linear-model-with-gaussian-noises}}

The likelihood is often computed on a linear model
\[ y=X\beta + \mu \quad \mu \sim N\left(0, \sigma^2\Omega\right) \]

The log-likelihood is then
\[ l \left(\theta,\beta , \sigma | y \right ) = - \frac{1}{2} \left(n \log{2 \pi}+ n \log {\sigma^2} + \log{|\Omega |} + \frac{1}{\sigma ^2} \left(y-X\beta \right)'\Omega^{-1}\left(y-X\beta \right) \right) \]

The maximum likelihood estimator of \(\beta\) is

\[ \hat{\beta} = \left( X'\Omega^{-1}X\right)^{-1}X'\Omega^{-1}y \]

which is normally distributed with variance
\[ \sigma^2 \left( X'\Omega^{-1}X\right)^{-1} \]

The formulae of the likelihood are still valid, using
\[ e=L^{-1} \left(y-X\hat\beta \right) \]

\hypertarget{implementation-3}{%
\subsection{Implementation}\label{implementation-3}}

Those representations of the concentrated likelihood are defined in the interfaces \textbf{\emph{demetra.likelihood.ILikelihood}} and \textbf{\emph{demetra.likelihood.IConcentratedLikelihood}}

\hypertarget{correspondance-between-the-elements-of-the-likelihood-see-formulae-and-the-methods-of-the-classes}{%
\subsubsection{Correspondance between the elements of the likelihood (see formulae) and the methods of the classes}\label{correspondance-between-the-elements-of-the-likelihood-see-formulae-and-the-methods-of-the-classes}}

\begin{itemize}
\tightlist
\item
  \(n\) : dim()
\item
  \(e'e\) : ssq()
\item
  \(e\) : e()
\item
  \(\log \|\Omega\|\) : logDeterminant()
\item
  \(v\) : v()
\item
  \(\|\Omega\|^{\frac{1}{n}}\) : factor()
\end{itemize}

\hypertarget{missing-values}{%
\subsection{Missing values}\label{missing-values}}

Missing values are not taken into account in the likelihood. More especially, when they are estimated by means of additive outliers, all the different elements of the likelihood (dimension, determinantal term, coefficients\ldots) should be adjusted to remove their effect.

\hypertarget{perfect-collinearity-in-x}{%
\subsection{Perfect collinearity in X}\label{perfect-collinearity-in-x}}

In the case of perfect collinearity in the linear model, the dimensions of the coefficients and of the related matrices are not modified. However, information related to the redundant variables is set to 0.

\hypertarget{references-7}{%
\subsection{References}\label{references-7}}

\emph{Gomez V. and Maravall A.} (1994): ``Estimation, Prediction, and Interpolation for Nonstationary Series With the Kalman Filter'', Journal of the American Statistical Association, vol.~89, n° 426, 611-624.

\hypertarget{x-13-implementation}{%
\subsection{X-13 implementation}\label{x-13-implementation}}

Estimation of the exact likelihood of an ARMA model

We suppose that
\[ y_t, \quad 1 \le t \le n \]
follows an ARMA model.

The X12 implementation computes the exact likelihood in two main steps

\hypertarget{overview-4}{%
\subsubsection{Overview}\label{overview-4}}

We consider the transformation

\[ z_t = \begin{pmatrix} z_{1t} \\ z_{2t} \end{pmatrix} = \begin{cases} y_t, & 1 \le t \le p \\ \Phi\left(B\right) y_t, & p \lt t \le n\end{cases}\]

It is obvious that
\[ p\left(y_t\right) = p\left(z_t\right) \]

We will estimate
\[ p\left(z_t\right) = p\left(z_{2t}\right)  p\left(z_{1t}\right | z_{2t} )\]

\hypertarget{step-1-likelihood-of-a-pure-moving-average-process-1}{%
\subsubsection{Step 1: likelihood of a pure moving average process}\label{step-1-likelihood-of-a-pure-moving-average-process-1}}

\[z_{2t}\]
is a pure moving average process. Its exact likelihood is estimated as follows (see {[}1{]} for details).
We list below the different steps of the algorithm.

\begin{itemize}
\item
  Compute the conditional least squares residuals by the recursion:
  \[ a_{0t} = \begin{cases} 0,& -q \lt t \leq 0 \\ z_t-\theta_1 a_{0t-1}- \cdots --\theta_q a_{0t-q},&  0 \lt t \leq n  \end{cases} \]
\item
  Compute the Pi-Weights of the model. They defines the (n+q x q) matrix
\end{itemize}

\[ G = \begin{pmatrix} 1 & 0 & \cdots & 0 \\ \pi_1 & 1 & \cdots & 0 \\ \pi_2 & \pi_1 & \cdots & \vdots\\ \vdots & \vdots & \vdots & \vdots \\ \pi_{n+q-1} & \pi_{n+q-2} & \cdots & \pi_{n} \end{pmatrix} \]

\begin{itemize}
\item
  Compute by recursion
  \[ G'a \quad and \quad G'G \]
\item
  Compute by Cholesky decomposition
  \[ G'G \hat z_* = G'a \quad and \quad |G'G| \]
\item
  Obtain by recursion the exact likelihood residuals
  \[ \Theta\left(B\right)\begin{pmatrix}\hat a_* \\ \hat a_t \end{pmatrix}  = \begin{pmatrix}z_* \\ z_t \end{pmatrix} \]
\end{itemize}

The processing defines the linear transformation
\[ T z_t = \begin{pmatrix}\hat a_* \\ \hat a_t \end{pmatrix} \] and the searched determinant.

\hypertarget{step-2-conditional-distribution-of-the-initial-observations-1}{%
\subsubsection{Step 2: conditional distribution of the initial observations}\label{step-2-conditional-distribution-of-the-initial-observations-1}}

\[ p\left(z_{1t}\right | z_{2t} ) \]
is easily obtained by considering the join distribution of
\[ \left( z_{1t}, z_{2t} \right) \sim N \left( 0, \begin{pmatrix} \Sigma_{11} && \Sigma{12} \\ \Sigma_{21} && \Sigma{22} \end{pmatrix}\right) \ \]

It is distributed as
\[ N\left( \Sigma_{12} \Sigma_{22}^{-1}v, \Sigma_{11}-\Sigma_{12} \Sigma_{22}^{-1} \Sigma_{21} \right)\]

\[= N\left( \Sigma_{12} T'Tv, \Sigma_{11}-\Sigma_{12} T'T\Sigma_{21} \right)\]

\[  = N\left( U'Tv, \Sigma_{11}-U'U \right)\]

where
* v is obtained by applying the auto-regressive polynomial on the observations
* T is the linear transformation defined in step 1
*
\[ U = T \Sigma_{21} \]

\hypertarget{references-8}{%
\paragraph{References}\label{references-8}}

{[}1{]} \textbf{\emph{Otto M. C., Bell W.R., Burman J.P.}} (1987), ``An Iterative GLS Approach to Maximum Likelihood Estimation of Regression Models with Arima Errors'', Bureau of The Census, SRD Research Report CENSUS/SRD/RR\_87/34.

\hypertarget{handling-of-missing-observations-in-regarima-models}{%
\section{Handling of missing observations in (Reg)ARIMA models}\label{handling-of-missing-observations-in-regarima-models}}

\hypertarget{skipping-approach}{%
\subsection{Skipping approach}\label{skipping-approach}}

\hypertarget{additive-outlier-approach}{%
\subsection{Additive outlier approach}\label{additive-outlier-approach}}

\hypertarget{references-9}{%
\subsection{References}\label{references-9}}

\textbf{GOMEZ V. , MARAVALL A. AND PEÑA D.} (1999): ``\emph{Missing observations in ARIMA models: Skipping approach versus additive outlier approach}'', Journal of econometrics 88, 341-363.

\hypertarget{tests-on-residuals}{%
\section{Tests on residuals}\label{tests-on-residuals}}

\hypertarget{autocorrelation}{%
\subsection{Autocorrelation}\label{autocorrelation}}

\hypertarget{the-durbin-watson-statistic-is-defined-by81}{%
\subsubsection[The Durbin-Watson statistic is defined by:]{\texorpdfstring{The Durbin-Watson statistic is defined by\footnote{CHATFIELD, C. (2004).}:}{The Durbin-Watson statistic is defined by:}}\label{the-durbin-watson-statistic-is-defined-by81}}

\[
 d = \frac{\sum_{t = 2}^{N}\left( {\widehat{a}}_{t} - {\widehat{a}}_{t - 1} \right)^{2}}{\sum_{t = 1}^{N}{\widehat{a}}_{t}^{2}}
 \]

where:

\({\widehat{a}}_{t}\) : residual from the model.

Since
\[\sum_{t = 2}^{N}\left( {\widehat{a}}_{t} - {\widehat{a}}_{t - 1} \right)^{2} \cong \ \]2\[\sum_{t = 1}^{N}{\widehat{a}}_{t}^{2} - 2\sum_{t = 2}^{N}{ {\widehat{a}}_{t}{\widehat{a}}_{t - 1}}\],
then the approximation \(d \cong 2(1 - r_{z,1})\), where
\[r_{z,1} = \frac{\sum_{t = 1}^{N}{ {\widehat{a}}_{t}{\widehat{a}}_{t - 1}}}{\sum_{t = 1}^{N}{\widehat{a}}_{t}^{2}}\]
is the autocorrelation coefficient of the residuals at lag 1, is true.

The Durbin-Watson statistics is between 0 and 4. When the model provides
an adequate description of the data, then \(r_{z,1}\) should be close to 0
and therefore the Durbin-Watson statistics is close to 2. When the
Durbin--Watson statistic is substantially less than 2, there is evidence
of positive serial correlation, while when it is substantially greater
than 2 it indicates that the successive error terms are, on average,
much different in value from one another, i.e., negatively correlated.

More formally, to test for a positive autocorrelation at
significance level \(\alpha\), the Durbin-Watson statistics is compared to
the lower (\(d_{L,\alpha}\ )\ \)and upper (\(d_{U,\alpha})\) critical
values:

\begin{itemize}
\item
  If \(d < d_{L,\alpha}\) there is statistical evidence that the error terms are positively autocorrelated.
\item
  If \(d > d_{U,\alpha}\) there is no statistical evidence that the error terms are positively autocorrelated.
\item
  If \(d_{L,\alpha}\) \(< d < d_{U,\alpha}\) the test is inconclusive.
\end{itemize}

Positive serial correlation is serial correlation in which a positive
error for one observation increases the chances of a positive error for
another observation.

To test for negative autocorrelation at significance\(\ \alpha\), the test
statistic \((4 - d)\) is compared to the lower (\(d_{L,\alpha}\ )\ \)and
upper (\(d_{U,\alpha})\) critical values:

\begin{itemize}
\item
  If \(\left( 4 - d \right) < d_{L,\alpha}\) there is statistical evidence that the error terms are negatively autocorrelated.
\item
  If \(\left( 4 - d \right) > d_{U,\alpha}\) there is no statistical evidence that the error terms are negatively autocorrelated.
\item
  If \(d_{U,\alpha} < \left( 4 - d \right) < d_{U,\alpha}\) the test is inconclusive.
\end{itemize}

\hypertarget{ljung-box-test}{%
\subsubsection{Ljung-Box test}\label{ljung-box-test}}

(described twice: merge)

The Ljung-Box Q-statistics are given by:

\[
  \text{LB}\left( k \right) = n \times (n + 2) \times \sum_{k = 1}^{K}\frac{\rho_{a,k}^{2}}{n - k}
  \], \[1\]

where:

\[\rho_{a,k}^{2}\] is the autocorrelation coefficient at lag \(k\) of the
residuals \[{\widehat{a}}_{t}\].

\(n\) is the number of terms in differenced series;

\[K\] is the maximum lag being considered, set in JDemetra+ to \(24\)
(monthly series) or \(8\) (quarterly series).

If the residuals are random (which is the case for residuals from a well
specified model), they will be distributed as \(\chi_{(K - m)}^{2}\),
where \(m\) is the number of parameters in the model which has been fitted
to the data.

The Ljung-Box and Box-Pierce tests sometimes fail to reject a poorly
fitting model. Therefore, care should be taken not to accept a model on
a basis of their results. For the description of autocorrelation concept
see section \href{../theory/ACF_and_PACF.html}{Autocorrelation function and partial autocorrelation function}.

The Ljung-Box test checks the ``overall'' randomnes of a time series using a given number of \href{../../descriptive.md}{autocorrelations}.\\
It tests wether any of a group of autocorrelations of a time series are significantly different from 0.

\hypertarget{algorithm}{%
\paragraph{Algorithm}\label{algorithm}}

We consider the autocorrelations \(\hat\gamma_l, \cdots, \hat\gamma_{l\cdot k}\). Typically, \(l=1\) when testing the independence of the series or \(l=freq\) when testing seasonality.

The value of the test is defined by

\[ lb=n \left(n+2\right)\sum_{i=1}^k\frac{\hat\gamma_{i \cdot l}^2}{n-i \cdot l}\]

It is asymptotically distributed as a \(\chi \left(k\right)\)

\hypertarget{impementation-in-gui}{%
\paragraph{Impementation in GUI}\label{impementation-in-gui}}

\hypertarget{impementation-in-r}{%
\paragraph{Impementation in R}\label{impementation-in-r}}

\hypertarget{java-library-1}{%
\paragraph{Java Library}\label{java-library-1}}

This test is implemented in the class \texttt{demetra.stats.tests.LjungBoxTest}

\begin{Shaded}
\begin{Highlighting}[]
    \DataTypeTok{int}\NormalTok{ N}\OperatorTok{=}\DecValTok{100}\OperatorTok{;}
\NormalTok{    DataBlock sample}\OperatorTok{=}\NormalTok{DataBlock}\OperatorTok{.}\FunctionTok{make}\OperatorTok{(}\NormalTok{N}\OperatorTok{);}
    \BuiltInTok{Random}\NormalTok{ rnd}\OperatorTok{=}\KeywordTok{new} \BuiltInTok{Random}\OperatorTok{();}
\NormalTok{    LjungBoxTest lb}\OperatorTok{=}\KeywordTok{new} \FunctionTok{LjungBoxTest}\OperatorTok{(}\NormalTok{sample}\OperatorTok{);}
\NormalTok{    StatisticalTest test }\OperatorTok{=}\NormalTok{ lb}
             \OperatorTok{.}\FunctionTok{lag}\OperatorTok{(}\DecValTok{3}\OperatorTok{)}
             \OperatorTok{.}\FunctionTok{autoCorrelationsCount}\OperatorTok{(}\DecValTok{10}\OperatorTok{)}
             \OperatorTok{.}\FunctionTok{build}\OperatorTok{();}
\end{Highlighting}
\end{Shaded}

\hypertarget{box-pierce-test}{%
\subsubsection{Box-Pierce Test}\label{box-pierce-test}}

(described twice: merge)

The Box-Pierce Q-statistics are given by:

\[\text{BP}\left( k \right) = n\sum_{k = 1}^{K}\rho_{a,k}^{2}
  \], \[1\]

where:

\(\rho_{a,k}^{2}\) is the autocorrelation coefficient at lag \(k\) of the
residuals \({\widehat{a}}_{t}\).

\(n\) is the number of terms in differenced series;

\(K\) is the maximum lag being considered, set in JDemetra+ to \(24\)
(monthly series) or \(8\) (quarterly series).

If the residuals are random (which is the case for residuals from a well
specified model), they will be distributed as \(\chi_{(K - m)}^{2}\)
degrees of freedom, where \(m\) is the number of parameters in the model
which has been fitted to the data.

The Ljung-Box and Box-Pierce tests sometimes fail to reject a poorly
fitting model. Therefore, care should be taken not to accept a model on
a basis of their results. For the description of autocorrelation concept
see section \href{../theory/ACF_and_PACF.html}{Autocorrelation function and partial autocorrelation function}.

Explain difference with Ljung-Box test

The Box-Pierce test checks the ``overall'' randomnes of a time series using a given number of \href{../../descriptive.md}{autocorrelations}.\\
It tests wether any of a group of autocorrelations of a time series are significantly different from 0.

\hypertarget{statistic}{%
\paragraph{Statistic}\label{statistic}}

We consider the autocorrelations \(\hat\gamma_l, \cdots, \hat\gamma_{l\cdot k}\). Typically, \(l=1\) when testing the independence of the series or \(l=freq\) when testing seasonality.

The value of the test is defined by

\[ bp=n \sum_{i=1}^k\hat\gamma_{i \cdot l}^2\]

It is asymptotically distributed as a \(\chi \left(k\right)\)

\hypertarget{impementation-in-gui-1}{%
\paragraph{Impementation in GUI}\label{impementation-in-gui-1}}

\hypertarget{impementation-in-r-1}{%
\paragraph{Impementation in R}\label{impementation-in-r-1}}

\hypertarget{java-library-2}{%
\paragraph{Java Library}\label{java-library-2}}

This test is implemented in the class \texttt{demetra.stats.tests.BoxPierceTest}

\begin{Shaded}
\begin{Highlighting}[]
    \DataTypeTok{int}\NormalTok{ N}\OperatorTok{=}\DecValTok{100}\OperatorTok{;}
\NormalTok{    DataBlock sample}\OperatorTok{=}\NormalTok{DataBlock}\OperatorTok{.}\FunctionTok{make}\OperatorTok{(}\NormalTok{N}\OperatorTok{);}
    \BuiltInTok{Random}\NormalTok{ rnd}\OperatorTok{=}\KeywordTok{new} \BuiltInTok{Random}\OperatorTok{();}
\NormalTok{    sample}\OperatorTok{.}\FunctionTok{set}\OperatorTok{(}\NormalTok{rnd}\OperatorTok{::}\NormalTok{nextDouble}\OperatorTok{);}
\NormalTok{    BoxPierceTest bp}\OperatorTok{=}\KeywordTok{new} \FunctionTok{BoxPierceTest}\OperatorTok{(}\NormalTok{sample}\OperatorTok{);}
\NormalTok{    StatisticalTest test }\OperatorTok{=}\NormalTok{ bp}
              \OperatorTok{.}\FunctionTok{lag}\OperatorTok{(}\DecValTok{3}\OperatorTok{)}
              \OperatorTok{.}\FunctionTok{autoCorrelationsCount}\OperatorTok{(}\DecValTok{10}\OperatorTok{)}
              \OperatorTok{.}\FunctionTok{build}\OperatorTok{();}
\end{Highlighting}
\end{Shaded}

\hypertarget{normality}{%
\subsection{Normality}\label{normality}}

\hypertarget{doornik-hansen-test}{%
\subsubsection{Doornik-Hansen test}\label{doornik-hansen-test}}

The Doornik-Hansen test for multivariate normality (DOORNIK, J.A., and
HANSEN, H. (2008)) is based on the skewness and kurtosis of multivariate
data that is transformed to ensure independence. It is more powerful
than the Shapiro-Wilk test for most tested multivariate
distributions\footnote{The description of the test derives from DOORNIK, J.A., and
  HANSEN, H. (2008).}.

The skewness and kurtosis are defined, respectively, as:
\[s = \frac{m_{3}}{\sqrt{m_{2}}^{3}}\] and
\(k = \frac{m_{4}}{m_{2}^{2}},\ \)where:
\(m_{i} = \frac{1}{n}\sum_{i = 1}^{n}{(x_{i}}{- \overline{x})}^{i}\)
\(\overline{x} = \frac{1}{n}\sum_{i = 1}^{n}x_{i}\) and \(n\) is a number of
(non-missing) residuals.

The Doornik-Hansen test statistic derives from SHENTON, L.R., and
BOWMAN, K.O. (1977) and uses transformed versions of skewness and
kurtosis.

The transformation for the skewness \(s\) into\(\text{z}_{1}\) is as in
D'AGOSTINO, R.B. (1970):

\[
  \beta = \frac{3(n^{2} + 27n - 70)(n + 1)(n + 3)}{(n - 2)(n + 5)(n + 7)(n + 9)}
  \]

\[
  \omega^{2} = - 1 + \sqrt{2(\beta - 1)}
  \]

\[
  \delta = \frac{1}{\sqrt{\log{(\omega}^{2})}}
  \]

\[
  y = s\sqrt{\frac{(\omega^{2} - 1)(n + 1)(n + 3)}{12(n - 2)}}
  \]

\[
  z_{1} = \delta log(y + \sqrt{y^{2} - 1})
  \]

The kurtosis \(k\) is transformed from a gamma distribution to \(\chi^{2}\),
which is then transformed into standard normal \(z_{2}\) using the
Wilson-Hilferty cubed root transformation:

\[
  \delta = (n - 3)(n + 1)(n^{2} + 15n - 4)
  \]

\[
  a = \frac{(n - 2)(n + 5)(n + 7)(n^{2} + 27n - 70)}{6\delta}
  \]

\[
  c = \frac{(n - 7)(n + 5)(n + 7)(n^{2} + 2n - 5)}{6\delta}
  \]

\[
  l= \frac{(n + 5)(n + 7)({n^{3} + 37n}^{2} + 11n - 313)}{12\delta}
  \]

\[
  \alpha = a + c \times s^{2}
  \]

\[
  \chi = 2l(k - 1 - s^{2})
  \]

\[
  z_{2} = \sqrt{9\alpha}\left( \frac{1}{9\alpha} - 1 + \sqrt[3]{\frac{\chi}{2\alpha}} \right)
  \]

Finally, the Doornik-Hansen test statistic is defined as the sum of
squared transformations of the skewness and kurtosis. Approximately, the
test statistic follows a \(\chi^{2}\)distribution, i.e.:

\[
  DH = z_{1}^{2} + z_{2}^{2}\sim\chi^{2}(2)
  \]

\hypertarget{dagostino}{%
\subsubsection{D'agostino ?}\label{dagostino}}

\hypertarget{jarque-bera}{%
\subsubsection{Jarque-Bera}\label{jarque-bera}}

\hypertarget{moving-average-based-decomposition}{%
\chapter{Moving average based decomposition}\label{moving-average-based-decomposition}}

\hypertarget{arima-model-based-decomposition}{%
\chapter{Arima Model based decomposition}\label{arima-model-based-decomposition}}

\hypertarget{state-space-framework}{%
\chapter{State Space Framework}\label{state-space-framework}}

  \bibliography{book.bib,packages.bib}

\end{document}
